{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube · 2024 Ucrania · Search mensual con **reanudación** (v7-fix)\n",
    "\n",
    "Este notebook fuerza el uso de la 4ª key por defecto y ofrece resume por canal/mes/pageToken.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Cargar `.env` y priorizar la ultima key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENV cargado desde: c:\\Users\\User\\Desktop\\Facu\\Master_Espana\\Master_UEMC\\TFM\\codigo\\analisis_guerra_ucrania_youtube\\.env\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Dict\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import socket\n",
    "import time\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from httplib2 import HttpLib2Error\n",
    "\n",
    "def load_env_from_repo_root(max_up: int = 5):\n",
    "    here = os.path.abspath(os.getcwd())\n",
    "    for _ in range(max_up):\n",
    "        candidate = os.path.join(here, \".env\")\n",
    "        if os.path.exists(candidate):\n",
    "            load_dotenv(candidate)\n",
    "            return candidate\n",
    "        here = os.path.dirname(here)\n",
    "    load_dotenv()\n",
    "    return None\n",
    "\n",
    "env_path = load_env_from_repo_root()\n",
    "print(\"ENV cargado desde:\", env_path)\n",
    "\n",
    "k1 = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "k2 = os.getenv(\"YOUTUBE_API_KEY_SECONDARY\")\n",
    "k3 = os.getenv(\"YOUTUBE_API_KEY_TERTIARY\")\n",
    "k4 = os.getenv(\"YOUTUBE_API_KEY_QUATERNARY\")\n",
    "k5 = os.getenv(\"YOUTUBE_API_KEY_QUINARY\")\n",
    "\n",
    "# === MODO: usar sólo la más nueva (p.ej., la 6ª) ===\n",
    "USE_ONLY_NEWEST = False         # ← pon True si quieres usar exclusivamente la última (k5)\n",
    "\n",
    "if USE_ONLY_NEWEST and k5:\n",
    "    API_KEYS = [k5]\n",
    "else:\n",
    "    # Orden recomendado: más nuevas primero (k5 → k4 → k1 → k2 → k3)\n",
    "    API_KEYS = [k for k in [k5, k4, k1, k2, k3] if k]\n",
    "\n",
    "if not API_KEYS:\n",
    "    raise RuntimeError(\"No hay API keys disponibles en .env\")\n",
    "\n",
    "\n",
    "canales_interes: Dict[str, str] = {\n",
    "    'UCPH3Oz99Y_jrVBCQMjQZNSg': 'pro-ucraniano',\n",
    "    'UCJQQVLyM6wtPleV4wFBK06g': 'pro-ucraniano',\n",
    "    'UCnsvJeZO4RigQ898WdDNoBw': 'noticiero',\n",
    "    'UC7QZIf0dta-XPXsp9Hv4dTw': 'noticiero',\n",
    "    'UClLLRs_mFTsNT5U-DqTYAGg': 'noticiero',\n",
    "    'UCGXbLrVe8vnkiFv7q2vYv3w': 'noticiero',\n",
    "    'UCBQnW5_C-6Ns6bob5ozacZg': 'pro-ruso'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Ventana 2024 + keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "DATE_MIN = datetime(2024,1,1,0,0,0,tzinfo=timezone.utc)\n",
    "DATE_MAX = datetime(2024,12,31,23,59,59,tzinfo=timezone.utc)\n",
    "UKR_KEYWORDS = {\n",
    "    'ucrania','ucraniana','ucranianos','zelenski','zelensky','zelenskyy','kiev','kyiv','donbas','donbás','crimea',\n",
    "    'dnipro','odessa','odesa','kharkiv','jersón','kherson','mariúpol','mariupol','zelenskiy','guerra de ucrania',\n",
    "    'guerra en ucrania','rusia ataca','invasión a ucrania','invasion a ucrania','guerra rusia ucrania',\n",
    "    'guerra ruso ucraniana','rusia','putin'\n",
    "}\n",
    "def is_ukraine_war_related(title: str, description: str, tags):\n",
    "    tags = tags or []\n",
    "    text = f\"{title or ''} {description or ''} {' '.join(tags)}\".lower()\n",
    "    return any(k in text for k in UKR_KEYWORDS)\n",
    "def safe_int(x, default=0):\n",
    "    try: return int(x)\n",
    "    except: return default\n",
    "def iso8601_duration_to_seconds(d: str) -> float:\n",
    "    if not d or not d.startswith('P'): return float('nan')\n",
    "    h=m=s=0.0\n",
    "    t = d.split('T',1)[1] if 'T' in d else ''\n",
    "    num = ''\n",
    "    for ch in t:\n",
    "        if ch.isdigit() or ch=='.': num+=ch\n",
    "        else:\n",
    "            if ch=='H': h=float(num or 0); num=''\n",
    "            elif ch=='M': m=float(num or 0); num=''\n",
    "            elif ch=='S': s=float(num or 0); num=''\n",
    "    return h*3600 + m*60 + s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Cliente YouTube (exec verboso + sin `None` en kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def _parse_reason_from_http_error(e: HttpError):\n",
    "    status = getattr(e.resp, 'status', None)\n",
    "    reason = ''\n",
    "    try:\n",
    "        payload = json.loads(e.content.decode('utf-8', errors='ignore'))\n",
    "        if isinstance(payload, dict):\n",
    "            err = payload.get('error') or {}\n",
    "            errs = err.get('errors') or []\n",
    "            if errs and isinstance(errs, list) and isinstance(errs[0], dict):\n",
    "                reason = str(errs[0].get('reason') or '')\n",
    "            elif 'message' in err:\n",
    "                reason = str(err.get('message'))\n",
    "    except Exception:\n",
    "        reason = (getattr(e, 'content', b'') or b'').decode('utf-8', 'ignore')\n",
    "    return status, (reason or '').lower()\n",
    "\n",
    "class YouTubeClient:\n",
    "    def __init__(self, keys):\n",
    "        self.keys = [k for k in (keys or []) if k]\n",
    "        if not self.keys:\n",
    "            raise RuntimeError('No hay API keys disponibles.')\n",
    "        self.idx = 0\n",
    "        self.service = build('youtube', 'v3', developerKey=self.keys[self.idx])\n",
    "    def _switch_key(self) -> bool:\n",
    "        if self.idx + 1 < len(self.keys):\n",
    "            self.idx += 1\n",
    "            self.service = build('youtube', 'v3', developerKey=self.keys[self.idx])\n",
    "            print(f'[KEY] Cambiando a key #{self.idx+1}')\n",
    "            time.sleep(0.2)\n",
    "            return True\n",
    "        return False\n",
    "    def exec(self, req_fn, retries: int = 1, backoff: float = 0.6):\n",
    "        last_exc = None\n",
    "        for attempt in range(retries + 1):\n",
    "            try:\n",
    "                req = req_fn()\n",
    "            except Exception as e:\n",
    "                last_exc = e\n",
    "                print(f'[exec] key_idx={self.idx} fallo construyendo request: {type(e).__name__}: {e}')\n",
    "                time.sleep(backoff * (attempt + 1)); continue\n",
    "            if not hasattr(req, 'execute'):\n",
    "                last_exc = RuntimeError(f'Request inesperada: {type(req)}')\n",
    "                print(f'[exec] key_idx={self.idx} request sin execute(): {type(req)}')\n",
    "                time.sleep(backoff * (attempt + 1)); continue\n",
    "            try:\n",
    "                resp = req.execute()\n",
    "                if not isinstance(resp, dict):\n",
    "                    last_exc = RuntimeError(f'Respuesta no-JSON (type={type(resp)}): {resp!r}')\n",
    "                    print('[exec] respuesta no-JSON; reintento…')\n",
    "                    time.sleep(backoff * (attempt + 1)); continue\n",
    "                return resp\n",
    "            except HttpError as e:\n",
    "                status, reason = _parse_reason_from_http_error(e)\n",
    "                print(f\"[exec] HttpError key_idx={self.idx} status={status} reason='{reason}' (attempt={attempt})\")\n",
    "                quota_like = reason in {'quotaexceeded','dailylimitexceeded','ratelimitexceeded'} or (status==403 and ('quota' in reason or 'forbidden' in reason))\n",
    "                key_invalid = reason in {'keyinvalid','forbidden'} and status in (400,403)\n",
    "                if (quota_like or key_invalid) and self._switch_key():\n",
    "                    time.sleep(backoff * (attempt + 1)); continue\n",
    "                last_exc = e\n",
    "                time.sleep(backoff * (attempt + 1))\n",
    "            except (HttpLib2Error, socket.timeout, ConnectionError) as e:\n",
    "                last_exc = e\n",
    "                print(f'[exec] Transporte key_idx={self.idx}: {type(e).__name__}: {e} (attempt={attempt})')\n",
    "                time.sleep(backoff * (attempt + 1))\n",
    "            except Exception as e:\n",
    "                last_exc = e\n",
    "                print(f'[exec] Excepción inesperada key_idx={self.idx}: {type(e).__name__}: {e} (attempt={attempt})')\n",
    "                time.sleep(backoff * (attempt + 1))\n",
    "        if isinstance(last_exc, HttpError):\n",
    "            status, reason = _parse_reason_from_http_error(last_exc)\n",
    "            raise RuntimeError(f\"API agotada o error persistente (HTTP {status}, reason='{reason}', key_idx={self.idx}).\") from last_exc\n",
    "        if last_exc is not None:\n",
    "            raise RuntimeError(f\"Fallo persistente con key_idx={self.idx}: {type(last_exc).__name__}: {last_exc}\") from last_exc\n",
    "        raise RuntimeError(f\"Fallo desconocido ejecutando la request de YouTube API (sin excepción previa). key_idx={self.idx}\")\n",
    "\n",
    "def yt_search_list(yc: YouTubeClient, **kwargs):\n",
    "    params = {k: v for k, v in kwargs.items() if v is not None}\n",
    "    dbg = {k: params.get(k) for k in ('channelId','publishedAfter','publishedBefore','pageToken','maxResults','order','type','q')}\n",
    "    print('[search] key_idx=', yc.idx, 'params=', dbg)\n",
    "    return yc.exec(lambda: yc.service.search().list(**params))\n",
    "def yt_videos_list(yc: YouTubeClient, **kwargs):\n",
    "    params = {k: v for k, v in kwargs.items() if v is not None}\n",
    "    return yc.exec(lambda: yc.service.videos().list(**params))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Estado (resume) + rangos mensuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = '../data/raw/data_rebuild'\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "STATE_PATH = os.path.join(OUT_DIR, 'discovery_state.json')\n",
    "F_VIDEOS_MASTER = os.path.join(OUT_DIR, '0_videos_por_canal_2024_ukraine.csv')\n",
    "\n",
    "def month_ranges_2024_indexed():\n",
    "    ranges = []\n",
    "    for m in range(1, 13):\n",
    "        start = datetime(2024, m, 1, 0, 0, 0, tzinfo=timezone.utc)\n",
    "        end = datetime(2025, 1, 1, 0, 0, 0, tzinfo=timezone.utc) if m == 12 else datetime(2024, m+1, 1, 0, 0, 0, tzinfo=timezone.utc)\n",
    "        ranges.append((m-1, start.strftime('%Y-%m-%dT%H:%M:%SZ'), end.strftime('%Y-%m-%dT%H:%M:%SZ')))\n",
    "    return ranges\n",
    "def load_state(path=STATE_PATH):\n",
    "    if os.path.exists(path):\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "def save_state(state, path=STATE_PATH):\n",
    "    tmp = path + '.tmp'\n",
    "    with open(tmp, 'w', encoding='utf-8') as f:\n",
    "        json.dump(state, f, ensure_ascii=False, indent=2)\n",
    "    os.replace(tmp, path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Search mensual con reanudación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_channel_2024_monthly_resume(ytc: YouTubeClient, channel_id: str, q: str, state: dict, verbose=True):\n",
    "    ch_state = state.get(channel_id, {'month_index': 0, 'page_token': None, 'done': False})\n",
    "    if ch_state.get('done'):\n",
    "        if verbose: print('   · resume: channel already done')\n",
    "        return [], state, False\n",
    "    vids, seen = [], set()\n",
    "    per_ch_csv = os.path.join(OUT_DIR, f'0_videos_por_canal_2024_ukraine_{channel_id}.csv')\n",
    "    if os.path.exists(per_ch_csv):\n",
    "        try:\n",
    "            prev_ids = pd.read_csv(per_ch_csv, usecols=['video_id']).get('video_id', pd.Series([], dtype=str)).astype(str).tolist()\n",
    "            seen.update(prev_ids)\n",
    "        except Exception:\n",
    "            pass\n",
    "    month_ranges = month_ranges_2024_indexed()\n",
    "    start_month = int(ch_state.get('month_index', 0))\n",
    "    page = ch_state.get('page_token') or None\n",
    "    for idx, start_iso, end_iso in month_ranges[start_month:]:\n",
    "        month_count = 0\n",
    "        while True:\n",
    "            try:\n",
    "                r = yt_search_list(\n",
    "                    ytc,\n",
    "                    part='id',\n",
    "                    channelId=channel_id,\n",
    "                    type='video',\n",
    "                    order='date',\n",
    "                    maxResults=50,\n",
    "                    publishedAfter=start_iso,\n",
    "                    publishedBefore=end_iso,\n",
    "                    q=(q.strip() if isinstance(q, str) and q.strip() else None),\n",
    "                    pageToken=page\n",
    "                )\n",
    "            except RuntimeError as e:\n",
    "                msg = str(e).lower()\n",
    "                if any(t in msg for t in ['quota', 'ratelimit', 'daily', 'api agotada']):\n",
    "                    state[channel_id] = {'month_index': idx, 'page_token': page, 'done': False}\n",
    "                    save_state(state)\n",
    "                    if verbose: print('   · cuota/key → estado guardado; deteniendo.')\n",
    "                    return vids, state, True\n",
    "                raise\n",
    "            items = r.get('items', [])\n",
    "            for it in items:\n",
    "                vid = (it.get('id') or {}).get('videoId')\n",
    "                if vid and vid not in seen:\n",
    "                    seen.add(vid); vids.append(vid); month_count += 1\n",
    "            page = r.get('nextPageToken') or None\n",
    "            if not page:\n",
    "                if verbose: print(f'   · {start_iso[:7]} +{month_count} (acum canal={len(seen)})')\n",
    "                state[channel_id] = {'month_index': idx+1, 'page_token': None, 'done': False}\n",
    "                save_state(state)\n",
    "                break\n",
    "    state[channel_id] = {'month_index': 12, 'page_token': None, 'done': True}\n",
    "    save_state(state)\n",
    "    if verbose: print('   · canal finalizado (12/12 meses).')\n",
    "    return vids, state, False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Enriquecer + guardar CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_and_filter_videos_to_df(ytc: YouTubeClient, video_ids):\n",
    "    rows = []\n",
    "    for i in range(0, len(video_ids), 50):\n",
    "        chunk = video_ids[i:i+50]\n",
    "        r = yt_videos_list(ytc, part='snippet,statistics,contentDetails', id=','.join(chunk))\n",
    "        for it in r.get('items', []):\n",
    "            sn = it.get('snippet', {}); st = it.get('statistics', {}); ct = it.get('contentDetails', {})\n",
    "            title = sn.get('title', ''); desc = sn.get('description', ''); tags = sn.get('tags', [])\n",
    "            if not is_ukraine_war_related(title, desc, tags):\n",
    "                continue\n",
    "            rows.append({\n",
    "                'video_id': it.get('id'),\n",
    "                'video_title': title,\n",
    "                'channel_title': sn.get('channelTitle'),\n",
    "                'video_published_at': sn.get('publishedAt'),\n",
    "                'video_views': safe_int(st.get('viewCount')),\n",
    "                'video_likes': safe_int(st.get('likeCount')),\n",
    "                'video_duration': iso8601_duration_to_seconds(ct.get('duration', '')),\n",
    "                'video_tags': '|'.join(tags) if tags else '',\n",
    "                'video_category_id': int(sn.get('categoryId')) if sn.get('categoryId') else 0\n",
    "            })\n",
    "        time.sleep(0.05)\n",
    "    return pd.DataFrame(rows)\n",
    "def append_dedup_csv(path: str, df: pd.DataFrame, subset=('video_id',)):\n",
    "    if df is None or df.empty: return\n",
    "    if os.path.exists(path):\n",
    "        prev = pd.read_csv(path)\n",
    "        out = pd.concat([prev, df], ignore_index=True).drop_duplicates(subset=list(subset))\n",
    "    else:\n",
    "        out = df.drop_duplicates(subset=list(subset))\n",
    "    out.to_csv(path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Orquestador reanudable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_search_only_resume(canales: Dict[str,str], q_keywords: str=None):\n",
    "    state = load_state()\n",
    "    ytc_local = YouTubeClient(API_KEYS)\n",
    "    for ch_i, (ch_id, bloque) in enumerate(canales.items(), start=1):\n",
    "        print(f\"\\n== Canal {ch_i}/{len(canales)} [{bloque}] {ch_id}\")\n",
    "        vids, state, stopped = search_channel_2024_monthly_resume(\n",
    "            ytc_local, ch_id, q=q_keywords, state=state, verbose=True\n",
    "        )\n",
    "        if not vids and not state.get(ch_id, {}).get('done'):\n",
    "            print('   · sin nuevos ids (posible cuota); paso al siguiente canal.')\n",
    "            if stopped: break\n",
    "            continue\n",
    "        per_ch_csv = os.path.join(OUT_DIR, f'videos_2024_ukraine_{ch_id}.csv')\n",
    "        if vids:\n",
    "            df = enrich_and_filter_videos_to_df(ytc_local, sorted(set(vids)))\n",
    "            print(f'   · relevantes añadidos (Ucrania): {len(df)}')\n",
    "            if not df.empty:\n",
    "                df.insert(0, 'channel_id', ch_id)\n",
    "                df.insert(1, 'bloque', bloque)\n",
    "                append_dedup_csv(per_ch_csv, df, subset=('video_id',))\n",
    "                print(f'   · guardado parcial → {per_ch_csv}')\n",
    "        else:\n",
    "            print('   · no hubo ids nuevos este ciclo.')\n",
    "        if stopped:\n",
    "            print('⚠️  Detenido por cuota/key. Estado guardado. Podrás reanudar desde aquí.')\n",
    "            break\n",
    "    per_files = [os.path.join(OUT_DIR, f) for f in os.listdir(OUT_DIR) if f.startswith('videos_2024_ukraine_') and f.endswith('.csv')]\n",
    "    if per_files:\n",
    "        dfs = []\n",
    "        for f in per_files:\n",
    "            try: dfs.append(pd.read_csv(f))\n",
    "            except Exception: pass\n",
    "        if dfs:\n",
    "            master = pd.concat(dfs, ignore_index=True).drop_duplicates(subset=['video_id'])\n",
    "            master.to_csv(F_VIDEOS_MASTER, index=False)\n",
    "            print(f'✅ Master actualizado: {F_VIDEOS_MASTER}  | vídeos={len(master)}')\n",
    "    else:\n",
    "        print('No hay CSVs parciales por canal aún.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Ejecutar discovery (reanudable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Canal 1/11 [pro-ucraniano] UCPH3Oz99Y_jrVBCQMjQZNSg\n",
      "   · resume: channel already done\n",
      "   · no hubo ids nuevos este ciclo.\n",
      "\n",
      "== Canal 2/11 [pro-ucraniano] UCJQQVLyM6wtPleV4wFBK06g\n",
      "   · resume: channel already done\n",
      "   · no hubo ids nuevos este ciclo.\n",
      "\n",
      "== Canal 3/11 [noticiero] UCnsvJeZO4RigQ898WdDNoBw\n",
      "   · resume: channel already done\n",
      "   · no hubo ids nuevos este ciclo.\n",
      "\n",
      "== Canal 4/11 [noticiero] UC7QZIf0dta-XPXsp9Hv4dTw\n",
      "   · resume: channel already done\n",
      "   · no hubo ids nuevos este ciclo.\n",
      "\n",
      "== Canal 5/11 [noticiero] UClLLRs_mFTsNT5U-DqTYAGg\n",
      "   · resume: channel already done\n",
      "   · no hubo ids nuevos este ciclo.\n",
      "\n",
      "== Canal 6/11 [noticiero] UCGXbLrVe8vnkiFv7q2vYv3w\n",
      "   · resume: channel already done\n",
      "   · no hubo ids nuevos este ciclo.\n",
      "\n",
      "== Canal 7/11 [noticiero] UCCJs5mITIqxqJGeFjt9N1Mg\n",
      "   · resume: channel already done\n",
      "   · no hubo ids nuevos este ciclo.\n",
      "\n",
      "== Canal 8/11 [pro-ruso] UCwd8Byi93KbnsYmCcKLExvQ\n",
      "   · resume: channel already done\n",
      "   · no hubo ids nuevos este ciclo.\n",
      "\n",
      "== Canal 9/11 [pro-ruso] UCgms7r9SaeYhuIBaPGOjnhw\n",
      "   · resume: channel already done\n",
      "   · no hubo ids nuevos este ciclo.\n",
      "\n",
      "== Canal 10/11 [pro-ruso] UCNKomgId0-uTA-vVLM9v1pw\n",
      "[search] key_idx= 0 params= {'channelId': 'UCNKomgId0-uTA-vVLM9v1pw', 'publishedAfter': '2024-02-01T00:00:00Z', 'publishedBefore': '2024-03-01T00:00:00Z', 'pageToken': 'CMgBEAA', 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "[search] key_idx= 0 params= {'channelId': 'UCNKomgId0-uTA-vVLM9v1pw', 'publishedAfter': '2024-02-01T00:00:00Z', 'publishedBefore': '2024-03-01T00:00:00Z', 'pageToken': 'CPoBEAA', 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "   · 2024-02 +88 (acum canal=88)\n",
      "[search] key_idx= 0 params= {'channelId': 'UCNKomgId0-uTA-vVLM9v1pw', 'publishedAfter': '2024-03-01T00:00:00Z', 'publishedBefore': '2024-04-01T00:00:00Z', 'pageToken': None, 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "[search] key_idx= 0 params= {'channelId': 'UCNKomgId0-uTA-vVLM9v1pw', 'publishedAfter': '2024-03-01T00:00:00Z', 'publishedBefore': '2024-04-01T00:00:00Z', 'pageToken': 'CDIQAA', 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "[search] key_idx= 0 params= {'channelId': 'UCNKomgId0-uTA-vVLM9v1pw', 'publishedAfter': '2024-03-01T00:00:00Z', 'publishedBefore': '2024-04-01T00:00:00Z', 'pageToken': 'CGQQAA', 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "   · 2024-03 +146 (acum canal=234)\n",
      "[search] key_idx= 0 params= {'channelId': 'UCNKomgId0-uTA-vVLM9v1pw', 'publishedAfter': '2024-04-01T00:00:00Z', 'publishedBefore': '2024-05-01T00:00:00Z', 'pageToken': None, 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "[search] key_idx= 0 params= {'channelId': 'UCNKomgId0-uTA-vVLM9v1pw', 'publishedAfter': '2024-04-01T00:00:00Z', 'publishedBefore': '2024-05-01T00:00:00Z', 'pageToken': 'CDIQAA', 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "[search] key_idx= 0 params= {'channelId': 'UCNKomgId0-uTA-vVLM9v1pw', 'publishedAfter': '2024-04-01T00:00:00Z', 'publishedBefore': '2024-05-01T00:00:00Z', 'pageToken': 'CGQQAA', 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "   · 2024-04 +137 (acum canal=371)\n",
      "[search] key_idx= 0 params= {'channelId': 'UCNKomgId0-uTA-vVLM9v1pw', 'publishedAfter': '2024-05-01T00:00:00Z', 'publishedBefore': '2024-06-01T00:00:00Z', 'pageToken': None, 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "[search] key_idx= 0 params= {'channelId': 'UCNKomgId0-uTA-vVLM9v1pw', 'publishedAfter': '2024-05-01T00:00:00Z', 'publishedBefore': '2024-06-01T00:00:00Z', 'pageToken': 'CDIQAA', 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "   · 2024-05 +51 (acum canal=422)\n",
      "[search] key_idx= 0 params= {'channelId': 'UCNKomgId0-uTA-vVLM9v1pw', 'publishedAfter': '2024-06-01T00:00:00Z', 'publishedBefore': '2024-07-01T00:00:00Z', 'pageToken': None, 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "[search] key_idx= 0 params= {'channelId': 'UCNKomgId0-uTA-vVLM9v1pw', 'publishedAfter': '2024-06-01T00:00:00Z', 'publishedBefore': '2024-07-01T00:00:00Z', 'pageToken': 'CDIQAA', 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "[search] key_idx= 0 params= {'channelId': 'UCNKomgId0-uTA-vVLM9v1pw', 'publishedAfter': '2024-06-01T00:00:00Z', 'publishedBefore': '2024-07-01T00:00:00Z', 'pageToken': 'CGQQAA', 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "[search] key_idx= 0 params= {'channelId': 'UCNKomgId0-uTA-vVLM9v1pw', 'publishedAfter': '2024-06-01T00:00:00Z', 'publishedBefore': '2024-07-01T00:00:00Z', 'pageToken': 'CJYBEAA', 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "[search] key_idx= 0 params= {'channelId': 'UCNKomgId0-uTA-vVLM9v1pw', 'publishedAfter': '2024-06-01T00:00:00Z', 'publishedBefore': '2024-07-01T00:00:00Z', 'pageToken': 'CMgBEAA', 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "[search] key_idx= 0 params= {'channelId': 'UCNKomgId0-uTA-vVLM9v1pw', 'publishedAfter': '2024-06-01T00:00:00Z', 'publishedBefore': '2024-07-01T00:00:00Z', 'pageToken': 'CPoBEAA', 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "[search] key_idx= 0 params= {'channelId': 'UCNKomgId0-uTA-vVLM9v1pw', 'publishedAfter': '2024-06-01T00:00:00Z', 'publishedBefore': '2024-07-01T00:00:00Z', 'pageToken': 'CKwCEAA', 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "[search] key_idx= 0 params= {'channelId': 'UCNKomgId0-uTA-vVLM9v1pw', 'publishedAfter': '2024-06-01T00:00:00Z', 'publishedBefore': '2024-07-01T00:00:00Z', 'pageToken': 'CN4CEAA', 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "   · 2024-06 +383 (acum canal=805)\n",
      "[search] key_idx= 0 params= {'channelId': 'UCNKomgId0-uTA-vVLM9v1pw', 'publishedAfter': '2024-07-01T00:00:00Z', 'publishedBefore': '2024-08-01T00:00:00Z', 'pageToken': None, 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "[search] key_idx= 0 params= {'channelId': 'UCNKomgId0-uTA-vVLM9v1pw', 'publishedAfter': '2024-07-01T00:00:00Z', 'publishedBefore': '2024-08-01T00:00:00Z', 'pageToken': 'CDIQAA', 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "   · 2024-07 +92 (acum canal=897)\n",
      "[search] key_idx= 0 params= {'channelId': 'UCNKomgId0-uTA-vVLM9v1pw', 'publishedAfter': '2024-08-01T00:00:00Z', 'publishedBefore': '2024-09-01T00:00:00Z', 'pageToken': None, 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "   · 2024-08 +15 (acum canal=912)\n",
      "[search] key_idx= 0 params= {'channelId': 'UCNKomgId0-uTA-vVLM9v1pw', 'publishedAfter': '2024-09-01T00:00:00Z', 'publishedBefore': '2024-10-01T00:00:00Z', 'pageToken': None, 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "[search] key_idx= 0 params= {'channelId': 'UCNKomgId0-uTA-vVLM9v1pw', 'publishedAfter': '2024-09-01T00:00:00Z', 'publishedBefore': '2024-10-01T00:00:00Z', 'pageToken': 'CDIQAA', 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "[search] key_idx= 0 params= {'channelId': 'UCNKomgId0-uTA-vVLM9v1pw', 'publishedAfter': '2024-09-01T00:00:00Z', 'publishedBefore': '2024-10-01T00:00:00Z', 'pageToken': 'CGQQAA', 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "[search] key_idx= 0 params= {'channelId': 'UCNKomgId0-uTA-vVLM9v1pw', 'publishedAfter': '2024-09-01T00:00:00Z', 'publishedBefore': '2024-10-01T00:00:00Z', 'pageToken': 'CJYBEAA', 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "[search] key_idx= 0 params= {'channelId': 'UCNKomgId0-uTA-vVLM9v1pw', 'publishedAfter': '2024-09-01T00:00:00Z', 'publishedBefore': '2024-10-01T00:00:00Z', 'pageToken': 'CMgBEAA', 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "   · 2024-09 +249 (acum canal=1161)\n",
      "[search] key_idx= 0 params= {'channelId': 'UCNKomgId0-uTA-vVLM9v1pw', 'publishedAfter': '2024-10-01T00:00:00Z', 'publishedBefore': '2024-11-01T00:00:00Z', 'pageToken': None, 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "   · 2024-10 +0 (acum canal=1161)\n",
      "[search] key_idx= 0 params= {'channelId': 'UCNKomgId0-uTA-vVLM9v1pw', 'publishedAfter': '2024-11-01T00:00:00Z', 'publishedBefore': '2024-12-01T00:00:00Z', 'pageToken': None, 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "   · 2024-11 +0 (acum canal=1161)\n",
      "[search] key_idx= 0 params= {'channelId': 'UCNKomgId0-uTA-vVLM9v1pw', 'publishedAfter': '2024-12-01T00:00:00Z', 'publishedBefore': '2025-01-01T00:00:00Z', 'pageToken': None, 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "   · 2024-12 +2 (acum canal=1163)\n",
      "   · canal finalizado (12/12 meses).\n",
      "   · relevantes añadidos (Ucrania): 198\n",
      "   · guardado parcial → ./data_rebuild\\videos_2024_ukraine_UCNKomgId0-uTA-vVLM9v1pw.csv\n",
      "\n",
      "== Canal 11/11 [pro-ruso] UCBQnW5_C-6Ns6bob5ozacZg\n",
      "[search] key_idx= 0 params= {'channelId': 'UCBQnW5_C-6Ns6bob5ozacZg', 'publishedAfter': '2024-01-01T00:00:00Z', 'publishedBefore': '2024-02-01T00:00:00Z', 'pageToken': None, 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "   · 2024-01 +49 (acum canal=49)\n",
      "[search] key_idx= 0 params= {'channelId': 'UCBQnW5_C-6Ns6bob5ozacZg', 'publishedAfter': '2024-02-01T00:00:00Z', 'publishedBefore': '2024-03-01T00:00:00Z', 'pageToken': None, 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "[search] key_idx= 0 params= {'channelId': 'UCBQnW5_C-6Ns6bob5ozacZg', 'publishedAfter': '2024-02-01T00:00:00Z', 'publishedBefore': '2024-03-01T00:00:00Z', 'pageToken': 'CDIQAA', 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "   · 2024-02 +62 (acum canal=111)\n",
      "[search] key_idx= 0 params= {'channelId': 'UCBQnW5_C-6Ns6bob5ozacZg', 'publishedAfter': '2024-03-01T00:00:00Z', 'publishedBefore': '2024-04-01T00:00:00Z', 'pageToken': None, 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "[search] key_idx= 0 params= {'channelId': 'UCBQnW5_C-6Ns6bob5ozacZg', 'publishedAfter': '2024-03-01T00:00:00Z', 'publishedBefore': '2024-04-01T00:00:00Z', 'pageToken': 'CDIQAA', 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "   · 2024-03 +65 (acum canal=176)\n",
      "[search] key_idx= 0 params= {'channelId': 'UCBQnW5_C-6Ns6bob5ozacZg', 'publishedAfter': '2024-04-01T00:00:00Z', 'publishedBefore': '2024-05-01T00:00:00Z', 'pageToken': None, 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "[search] key_idx= 0 params= {'channelId': 'UCBQnW5_C-6Ns6bob5ozacZg', 'publishedAfter': '2024-04-01T00:00:00Z', 'publishedBefore': '2024-05-01T00:00:00Z', 'pageToken': 'CDIQAA', 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "   · 2024-04 +58 (acum canal=234)\n",
      "[search] key_idx= 0 params= {'channelId': 'UCBQnW5_C-6Ns6bob5ozacZg', 'publishedAfter': '2024-05-01T00:00:00Z', 'publishedBefore': '2024-06-01T00:00:00Z', 'pageToken': None, 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "[search] key_idx= 0 params= {'channelId': 'UCBQnW5_C-6Ns6bob5ozacZg', 'publishedAfter': '2024-05-01T00:00:00Z', 'publishedBefore': '2024-06-01T00:00:00Z', 'pageToken': 'CDIQAA', 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "   · 2024-05 +62 (acum canal=296)\n",
      "[search] key_idx= 0 params= {'channelId': 'UCBQnW5_C-6Ns6bob5ozacZg', 'publishedAfter': '2024-06-01T00:00:00Z', 'publishedBefore': '2024-07-01T00:00:00Z', 'pageToken': None, 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "[search] key_idx= 0 params= {'channelId': 'UCBQnW5_C-6Ns6bob5ozacZg', 'publishedAfter': '2024-06-01T00:00:00Z', 'publishedBefore': '2024-07-01T00:00:00Z', 'pageToken': 'CDIQAA', 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "   · 2024-06 +60 (acum canal=356)\n",
      "[search] key_idx= 0 params= {'channelId': 'UCBQnW5_C-6Ns6bob5ozacZg', 'publishedAfter': '2024-07-01T00:00:00Z', 'publishedBefore': '2024-08-01T00:00:00Z', 'pageToken': None, 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "[search] key_idx= 0 params= {'channelId': 'UCBQnW5_C-6Ns6bob5ozacZg', 'publishedAfter': '2024-07-01T00:00:00Z', 'publishedBefore': '2024-08-01T00:00:00Z', 'pageToken': 'CDIQAA', 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "   · 2024-07 +61 (acum canal=417)\n",
      "[search] key_idx= 0 params= {'channelId': 'UCBQnW5_C-6Ns6bob5ozacZg', 'publishedAfter': '2024-08-01T00:00:00Z', 'publishedBefore': '2024-09-01T00:00:00Z', 'pageToken': None, 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "   · 2024-08 +42 (acum canal=459)\n",
      "[search] key_idx= 0 params= {'channelId': 'UCBQnW5_C-6Ns6bob5ozacZg', 'publishedAfter': '2024-09-01T00:00:00Z', 'publishedBefore': '2024-10-01T00:00:00Z', 'pageToken': None, 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "   · 2024-09 +30 (acum canal=489)\n",
      "[search] key_idx= 0 params= {'channelId': 'UCBQnW5_C-6Ns6bob5ozacZg', 'publishedAfter': '2024-10-01T00:00:00Z', 'publishedBefore': '2024-11-01T00:00:00Z', 'pageToken': None, 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "   · 2024-10 +38 (acum canal=527)\n",
      "[search] key_idx= 0 params= {'channelId': 'UCBQnW5_C-6Ns6bob5ozacZg', 'publishedAfter': '2024-11-01T00:00:00Z', 'publishedBefore': '2024-12-01T00:00:00Z', 'pageToken': None, 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "   · 2024-11 +32 (acum canal=559)\n",
      "[search] key_idx= 0 params= {'channelId': 'UCBQnW5_C-6Ns6bob5ozacZg', 'publishedAfter': '2024-12-01T00:00:00Z', 'publishedBefore': '2025-01-01T00:00:00Z', 'pageToken': None, 'maxResults': 50, 'order': 'date', 'type': 'video', 'q': None}\n",
      "   · 2024-12 +25 (acum canal=584)\n",
      "   · canal finalizado (12/12 meses).\n",
      "   · relevantes añadidos (Ucrania): 580\n",
      "   · guardado parcial → ./data_rebuild\\videos_2024_ukraine_UCBQnW5_C-6Ns6bob5ozacZg.csv\n",
      "✅ Master actualizado: ./data_rebuild\\videos_por_canal_2024_ukraine.csv  | vídeos=2099\n",
      "Listo para siguiente ciclo si salta cuota. Estado en: ./data_rebuild\\discovery_state.json\n"
     ]
    }
   ],
   "source": [
    "df_run = discover_search_only_resume(canales_interes, q_keywords=None)\n",
    "print('Listo para siguiente ciclo si salta cuota. Estado en:', STATE_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c02cabd",
   "metadata": {},
   "source": [
    "## Parte 2: Extracción completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3323c7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "videos en master: 864\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "864"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "OUT_DIR = './data_rebuild'\n",
    "F_MASTER = os.path.join(OUT_DIR, \"videos_master_refinado_ukraine_rules.csv\")\n",
    "df_master = pd.read_csv(F_MASTER)\n",
    "print('videos en master:', len(df_master))\n",
    "\n",
    "cond_por_canal = dict(df_master[['channel_id','bloque']].drop_duplicates().itertuples(index=False, name=None))\n",
    "video_ids = sorted(set(df_master['video_id'].astype(str)))\n",
    "len(video_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b18299c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time, socket, pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from httplib2 import HttpLib2Error\n",
    "\n",
    "def safe_int(x, default=0):\n",
    "    try: return int(x)\n",
    "    except: return default\n",
    "\n",
    "def iso8601_duration_to_seconds(d: str) -> float:\n",
    "    if not d or not d.startswith('P'): return float('nan')\n",
    "    h=m=s=0.0\n",
    "    t = d.split('T',1)[1] if 'T' in d else ''\n",
    "    num = ''\n",
    "    for ch in t:\n",
    "        if ch.isdigit() or ch=='.': num+=ch\n",
    "        else:\n",
    "            if ch=='H': h=float(num or 0); num=''\n",
    "            elif ch=='M': m=float(num or 0); num=''\n",
    "            elif ch=='S': s=float(num or 0); num=''\n",
    "    return h*3600 + m*60 + s\n",
    "\n",
    "def _parse_reason_from_http_error(e: HttpError):\n",
    "    status = getattr(e.resp, 'status', None)\n",
    "    reason = ''\n",
    "    try:\n",
    "        payload = json.loads(e.content.decode('utf-8', errors='ignore'))\n",
    "        if isinstance(payload, dict):\n",
    "            err = payload.get('error') or {}\n",
    "            errs = err.get('errors') or []\n",
    "            if errs and isinstance(errs, list) and isinstance(errs[0], dict):\n",
    "                reason = str(errs[0].get('reason') or '')\n",
    "            elif 'message' in err:\n",
    "                reason = str(err.get('message'))\n",
    "    except Exception:\n",
    "        reason = (getattr(e, 'content', b'') or b'').decode('utf-8', 'ignore')\n",
    "    return status, (reason or '').lower()\n",
    "\n",
    "class YouTubeClient:\n",
    "    def __init__(self, keys):\n",
    "        self.keys = [k for k in (keys or []) if k]\n",
    "        if not self.keys:\n",
    "            raise RuntimeError('No hay API keys disponibles.')\n",
    "        self.idx = 0\n",
    "        self.service = build('youtube','v3',developerKey=self.keys[self.idx])\n",
    "    def _switch_key(self) -> bool:\n",
    "        if self.idx + 1 < len(self.keys):\n",
    "            self.idx += 1\n",
    "            self.service = build('youtube','v3',developerKey=self.keys[self.idx])\n",
    "            print(f'[KEY] Cambiando a key #{self.idx+1}')\n",
    "            time.sleep(0.2)\n",
    "            return True\n",
    "        return False\n",
    "    def exec(self, req_fn, retries: int = 1, backoff: float = 0.6):\n",
    "        last_exc = None\n",
    "        for attempt in range(retries + 1):\n",
    "            try:\n",
    "                req = req_fn()\n",
    "            except Exception as e:\n",
    "                last_exc = e\n",
    "                print(f'[exec] key_idx={self.idx} fallo construyendo request: {type(e).__name__}: {e}')\n",
    "                time.sleep(backoff * (attempt + 1)); continue\n",
    "            try:\n",
    "                resp = req.execute()\n",
    "                if not isinstance(resp, dict):\n",
    "                    last_exc = RuntimeError(f'Respuesta no-JSON (type={type(resp)}): {resp!r}')\n",
    "                    print('[exec] respuesta no-JSON; reintento…')\n",
    "                    time.sleep(backoff * (attempt + 1)); continue\n",
    "                return resp\n",
    "            except HttpError as e:\n",
    "                status, reason = _parse_reason_from_http_error(e)\n",
    "                print(f\"[exec] HttpError key_idx={self.idx} status={status} reason='{reason}' (attempt={attempt})\")\n",
    "                quota_like = reason in {'quotaexceeded','dailylimitexceeded','ratelimitexceeded'} or (status==403 and ('quota' in reason or 'forbidden' in reason))\n",
    "                key_invalid = reason in {'keyinvalid','forbidden'} and status in (400,403)\n",
    "                if (quota_like or key_invalid) and self._switch_key():\n",
    "                    time.sleep(backoff * (attempt + 1)); continue\n",
    "                last_exc = e\n",
    "                time.sleep(backoff * (attempt + 1))\n",
    "            except (HttpLib2Error, socket.timeout, ConnectionError) as e:\n",
    "                last_exc = e\n",
    "                print(f'[exec] Transporte key_idx={self.idx}: {type(e).__name__}: {e} (attempt={attempt})')\n",
    "                time.sleep(backoff * (attempt + 1))\n",
    "            except Exception as e:\n",
    "                last_exc = e\n",
    "                print(f'[exec] Excepción inesperada key_idx={self.idx}: {type(e).__name__}: {e} (attempt={attempt})')\n",
    "                time.sleep(backoff * (attempt + 1))\n",
    "        if isinstance(last_exc, HttpError):\n",
    "            status, reason = _parse_reason_from_http_error(last_exc)\n",
    "            raise RuntimeError(f\"API agotada o error persistente (HTTP {status}, reason='{reason}', key_idx={self.idx}).\") from last_exc\n",
    "        if last_exc is not None:\n",
    "            raise RuntimeError(f\"Fallo persistente con key_idx={self.idx}: {type(last_exc).__name__}: {last_exc}\") from last_exc\n",
    "        raise RuntimeError(f\"Fallo desconocido ejecutando la request de YouTube API (sin excepción previa). key_idx={self.idx}\")\n",
    "\n",
    "def yt_videos_list(yc: YouTubeClient, **kwargs):\n",
    "    params = {k:v for k,v in kwargs.items() if v is not None}\n",
    "    return yc.exec(lambda: yc.service.videos().list(**params))\n",
    "def yt_comment_threads(yc: YouTubeClient, **kwargs):\n",
    "    params = {k:v for k,v in kwargs.items() if v is not None}\n",
    "    return yc.exec(lambda: yc.service.commentThreads().list(**params))\n",
    "def yt_channels_list(yc: YouTubeClient, **kwargs):\n",
    "    params = {k:v for k,v in kwargs.items() if v is not None}\n",
    "    return yc.exec(lambda: yc.service.channels().list(**params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e209c050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_meta cache rows: 864\n"
     ]
    }
   ],
   "source": [
    "META_CACHE = os.path.join(OUT_DIR, \"video_meta_cache_refinado.csv\")\n",
    "\n",
    "def build_video_meta(ytc: YouTubeClient, ids, cache_path=META_CACHE, batch=50):\n",
    "    exist = pd.read_csv(cache_path) if os.path.exists(cache_path) else pd.DataFrame()\n",
    "    have = set(exist['video_id']) if not exist.empty else set()\n",
    "    todo = [vid for vid in ids if vid not in have]\n",
    "    rows = []\n",
    "    for i in range(0, len(todo), batch):\n",
    "        chunk = todo[i:i+50]\n",
    "        r = yt_videos_list(ytc, part='snippet,statistics,contentDetails', id=','.join(chunk))\n",
    "        for it in r.get('items', []):\n",
    "            sn = it.get('snippet',{}); st=it.get('statistics',{}); ct=it.get('contentDetails',{})\n",
    "            rows.append({\n",
    "                'video_id': it.get('id'),\n",
    "                'video_title': sn.get('title'),\n",
    "                'channel_title': sn.get('channelTitle'),\n",
    "                'video_published_at': sn.get('publishedAt'),\n",
    "                'video_views': safe_int(st.get('viewCount')),\n",
    "                'video_likes': safe_int(st.get('likeCount')),\n",
    "                'video_duration': iso8601_duration_to_seconds(ct.get('duration','')),\n",
    "                'video_tags': '|'.join(sn.get('tags', [])) if sn.get('tags') else '',\n",
    "                'video_category_id': safe_int(sn.get('categoryId'), 0),\n",
    "                'channel_id': sn.get('channelId')\n",
    "            })\n",
    "        time.sleep(0.05)\n",
    "    if rows:\n",
    "        df_new = pd.DataFrame(rows)\n",
    "        df_out = pd.concat([exist, df_new], ignore_index=True).drop_duplicates(subset=['video_id'])\n",
    "        df_out.to_csv(cache_path, index=False)\n",
    "    else:\n",
    "        df_out = exist\n",
    "    return df_out\n",
    "\n",
    "ytc = YouTubeClient(API_KEYS)\n",
    "df_meta = build_video_meta(ytc, video_ids)\n",
    "print('video_meta cache rows:', len(df_meta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b52f482",
   "metadata": {},
   "source": [
    "## 3) Extraer **comentarios top-level** con resume + guardado incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af4aca4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Guardado parcial 5000 rows…\n",
      "✅ Comentarios top-level extraídos (resume completado)\n"
     ]
    }
   ],
   "source": [
    "import tempfile, shutil, time\n",
    "\n",
    "\n",
    "COMMENTS_TMP = os.path.join(OUT_DIR, 'comments_top_level_refinado_tmp.csv')\n",
    "STATE_CMT   = os.path.join(OUT_DIR, 'comments_state_refinado.json')\n",
    "\n",
    "def load_state(path):\n",
    "    if os.path.exists(path):\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "def save_state(state, path=STATE_CMT):\n",
    "    \"\"\"\n",
    "    Guardado atómico con fallback para Windows:\n",
    "    - Escribe en un archivo tmp con fsync\n",
    "    - Intenta os.replace; si hay PermissionError, intenta borrar el destino y mover\n",
    "    - Reintenta algunas veces por si el archivo destino está bloqueado\n",
    "    \"\"\"\n",
    "    tmp_dir = os.path.dirname(path) or \".\"\n",
    "    base = os.path.basename(path)\n",
    "\n",
    "    fd, tmp_path = tempfile.mkstemp(prefix=base + \".\", suffix=\".tmp\", dir=tmp_dir)\n",
    "    try:\n",
    "        with os.fdopen(fd, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(state, f, ensure_ascii=False, indent=2)\n",
    "            f.flush()\n",
    "            os.fsync(f.fileno())\n",
    "\n",
    "        for i in range(6):  # reintentos\n",
    "            try:\n",
    "                # intento principal: reemplazo atómico\n",
    "                os.replace(tmp_path, path)\n",
    "                return\n",
    "            except PermissionError:\n",
    "                # fallback Windows: eliminar destino (si existe) y mover\n",
    "                try:\n",
    "                    if os.path.exists(path):\n",
    "                        os.remove(path)\n",
    "                except PermissionError:\n",
    "                    pass  # puede seguir bloqueado; dormimos y reintentamos\n",
    "                try:\n",
    "                    shutil.move(tmp_path, path)\n",
    "                    return\n",
    "                except PermissionError:\n",
    "                    time.sleep(0.25 * (i + 1))  # backoff progresivo\n",
    "        raise PermissionError(f\"No pude reemplazar {path} tras varios reintentos\")\n",
    "    finally:\n",
    "        # si el tmp sigue existiendo a esta altura, limpiarlo\n",
    "        if os.path.exists(tmp_path):\n",
    "            try:\n",
    "                os.remove(tmp_path)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "def extract_comments_top_level(ytc: YouTubeClient, df_meta: pd.DataFrame, save_every=5000):\n",
    "    \"\"\"\n",
    "    Extrae comentarios top-level con reanudación.\n",
    "    - Lee video_ids desde df_meta (evita depender de variables globales).\n",
    "    - Guarda progreso en STATE_CMT y en COMMENTS_TMP.\n",
    "    \"\"\"\n",
    "    # --- IDs desde el meta (scope local) ---\n",
    "    video_ids = [str(v) for v in df_meta['video_id'].dropna().astype(str).tolist()]\n",
    "\n",
    "    # --- índice rápido de meta por video_id ---\n",
    "    meta_by_id = {str(r['video_id']): r for _, r in df_meta.iterrows()}\n",
    "\n",
    "    # --- estado previo ---\n",
    "    state = load_state(STATE_CMT)\n",
    "    done = set(state.get('done_video_ids', []))\n",
    "    page_tokens = state.get('page_tokens', {})\n",
    "\n",
    "    buf = []\n",
    "\n",
    "    for vid in video_ids:\n",
    "        if vid in done:\n",
    "            continue\n",
    "\n",
    "        meta = meta_by_id.get(vid, {})\n",
    "        page = page_tokens.get(vid)\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                r = yt_comment_threads(\n",
    "                    ytc,\n",
    "                    part='snippet',\n",
    "                    videoId=vid,\n",
    "                    maxResults=100,\n",
    "                    pageToken=page,\n",
    "                    textFormat='plainText'\n",
    "                )\n",
    "            except RuntimeError as e:\n",
    "                # si es cuota/ratelimit ⇒ persistimos y salimos para reanudar luego\n",
    "                msg = str(e).lower()\n",
    "                if any(t in msg for t in ('quota', 'ratelimit', 'daily', 'agotada')):\n",
    "                    if buf:\n",
    "                        pd.DataFrame(buf).to_csv(\n",
    "                            COMMENTS_TMP,\n",
    "                            mode='a',\n",
    "                            index=False,\n",
    "                            header=not os.path.exists(COMMENTS_TMP)\n",
    "                        )\n",
    "                        buf.clear()\n",
    "                    state['done_video_ids'] = list(done)\n",
    "                    state['page_tokens'] = page_tokens\n",
    "                    save_state(state, STATE_CMT)\n",
    "                    print('⚠️ Cuota/Key → progreso guardado. Salgo para reanudar luego.')\n",
    "                    return\n",
    "                # otro error ⇒ relanzar\n",
    "                raise\n",
    "\n",
    "            items = r.get('items', [])\n",
    "            for it in items:\n",
    "                sn = (it.get('snippet') or {})\n",
    "                tlc = (sn.get('topLevelComment') or {})\n",
    "                top = (tlc.get('snippet') or {})\n",
    "\n",
    "                text = top.get('textDisplay') or ''\n",
    "                author_ch = (top.get('authorChannelId') or {}).get('value')\n",
    "\n",
    "                buf.append({\n",
    "                    'comment_id': tlc.get('id'),\n",
    "                    'comment': text,\n",
    "                    'comment_text_length': len(text),\n",
    "                    'user_id': author_ch,\n",
    "                    'user_name': top.get('authorDisplayName'),\n",
    "                    'comment_time': top.get('publishedAt'),\n",
    "                    'comment_likes': safe_int(top.get('likeCount')),\n",
    "                    'total_reply_count': safe_int(sn.get('totalReplyCount')),\n",
    "                    'is_top_level_comment': True,\n",
    "\n",
    "                    'video_title': meta.get('video_title'),\n",
    "                    'channel_title': meta.get('channel_title'),\n",
    "                    'video_published_at': meta.get('video_published_at'),\n",
    "                    'video_views': safe_int(meta.get('video_views')),\n",
    "                    'video_likes': safe_int(meta.get('video_likes')),\n",
    "                    'video_duration': meta.get('video_duration'),\n",
    "                    'video_tags': meta.get('video_tags', ''),\n",
    "                    'video_category_id': safe_int(meta.get('video_category_id'), 0),\n",
    "\n",
    "                    'condiciones_cuenta': cond_por_canal.get(meta.get('channel_id'), 'desconocido'),\n",
    "                    'channel_id': meta.get('channel_id'),\n",
    "                    'subscriber_count': None,  # se completa en la etapa de enriquecimiento\n",
    "                })\n",
    "\n",
    "            # paginación\n",
    "            page = r.get('nextPageToken') or None\n",
    "            if not page:\n",
    "                done.add(vid)\n",
    "                page_tokens.pop(vid, None)\n",
    "                break\n",
    "            else:\n",
    "                page_tokens[vid] = page\n",
    "\n",
    "            # flush por bloques\n",
    "            if len(buf) >= save_every:\n",
    "                pd.DataFrame(buf).to_csv(\n",
    "                    COMMENTS_TMP,\n",
    "                    mode='a',\n",
    "                    index=False,\n",
    "                    header=not os.path.exists(COMMENTS_TMP)\n",
    "                )\n",
    "                print(f'💾 Guardado parcial {save_every} rows…')\n",
    "                buf.clear()\n",
    "\n",
    "        # flush por video\n",
    "        if buf:\n",
    "            pd.DataFrame(buf).to_csv(\n",
    "                COMMENTS_TMP,\n",
    "                mode='a',\n",
    "                index=False,\n",
    "                header=not os.path.exists(COMMENTS_TMP)\n",
    "            )\n",
    "            buf.clear()\n",
    "\n",
    "        # persistir progreso tras cada video\n",
    "        state['done_video_ids'] = list(done)\n",
    "        state['page_tokens'] = page_tokens\n",
    "        save_state(state, STATE_CMT)\n",
    "\n",
    "    print('✅ Comentarios top-level extraídos (resume completado)')\n",
    "\n",
    "# Llamada:\n",
    "extract_comments_top_level(ytc, df_meta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beed2835",
   "metadata": {},
   "source": [
    "## 4) Enriquecer `subscriber_count` (canal del **video**) + `account_created_at` (canal del **usuario**) y exportar dataset final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a398ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comentarios cargados: 371740\n",
      "✅ Dataset final → ./data_rebuild\\comments_2024_final.csv\n"
     ]
    }
   ],
   "source": [
    "FINAL_PATH = os.path.join(OUT_DIR, 'comments_2024_final.csv')\n",
    "\n",
    "def enrich_video_channel_subs(ytc: YouTubeClient, df_comments: pd.DataFrame):\n",
    "    ch_ids = sorted(set(df_comments['channel_id'].dropna().astype(str)))\n",
    "    map_subs = {}\n",
    "    for i in range(0, len(ch_ids), 50):\n",
    "        chunk = ch_ids[i:i+50]\n",
    "        r = yt_channels_list(ytc, part='statistics', id=','.join(chunk))\n",
    "        for it in r.get('items', []):\n",
    "            cid = it.get('id')\n",
    "            subs = safe_int((it.get('statistics') or {}).get('subscriberCount'))\n",
    "            map_subs[cid] = subs\n",
    "        time.sleep(0.05)\n",
    "    df_comments['subscriber_count'] = df_comments['channel_id'].map(map_subs).fillna(df_comments['subscriber_count'])\n",
    "    return df_comments\n",
    "\n",
    "def enrich_user_account_created(ytc: YouTubeClient, df_comments: pd.DataFrame):\n",
    "    user_ids = sorted(set(df_comments['user_id'].dropna().astype(str)))\n",
    "    acc_map = {}\n",
    "    for i in range(0, len(user_ids), 50):\n",
    "        chunk = user_ids[i:i+50]\n",
    "        try:\n",
    "            r = yt_channels_list(ytc, part='snippet', id=','.join(chunk))\n",
    "        except RuntimeError as e:\n",
    "            msg = str(e).lower()\n",
    "            if any(t in msg for t in ['quota','ratelimit','daily','agotada']):\n",
    "                break\n",
    "            raise\n",
    "        for it in r.get('items', []):\n",
    "            cid = it.get('id')\n",
    "            acc_map[cid] = (it.get('snippet') or {}).get('publishedAt')\n",
    "        time.sleep(0.05)\n",
    "    df_comments['account_created_at'] = df_comments['user_id'].map(acc_map)\n",
    "    return df_comments\n",
    "\n",
    "df_cmt = pd.read_csv(COMMENTS_TMP)\n",
    "print('comentarios cargados:', len(df_cmt))\n",
    "\n",
    "df_cmt = enrich_video_channel_subs(YouTubeClient(API_KEYS), df_cmt)\n",
    "df_cmt = enrich_user_account_created(YouTubeClient(API_KEYS), df_cmt)\n",
    "\n",
    "final_cols = [\n",
    " 'comment_id','comment','comment_text_length','user_id','user_name','comment_time','comment_likes',\n",
    " 'total_reply_count','is_top_level_comment','video_title','channel_title','video_published_at','video_views',\n",
    " 'video_likes','video_duration','video_tags','video_category_id','condiciones_cuenta','account_created_at',\n",
    " 'channel_id','subscriber_count'\n",
    "]\n",
    "df_cmt = df_cmt[final_cols]\n",
    "df_cmt.to_csv(FINAL_PATH, index=False)\n",
    "print('✅ Dataset final →', FINAL_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
