{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2abe101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Modelo de spaCy (una vez por entorno)\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import es_core_news_sm  # noqa\n",
    "except Exception:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"es_core_news_sm\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb37ab9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import langid\n",
    "\n",
    "# Bots\n",
    "import hashlib\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from math import log2\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pathlib import Path\n",
    "\n",
    "# Clusterizaci√≥n\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "70e8dd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/processed/7_final_label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "874fdf4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 113583 entries, 0 to 113582\n",
      "Data columns (total 51 columns):\n",
      " #   Column                       Non-Null Count   Dtype              \n",
      "---  ------                       --------------   -----              \n",
      " 0   comment_id                   113583 non-null  object             \n",
      " 1   comment                      113583 non-null  object             \n",
      " 2   comment_text_length          113583 non-null  int64              \n",
      " 3   user_id                      113583 non-null  object             \n",
      " 4   user_name                    113583 non-null  object             \n",
      " 5   comment_time                 113583 non-null  datetime64[ns, UTC]\n",
      " 6   comment_likes                113583 non-null  int64              \n",
      " 7   total_reply_count            113583 non-null  int64              \n",
      " 8   is_top_level_comment         113583 non-null  bool               \n",
      " 9   video_title                  113583 non-null  object             \n",
      " 10  channel_title                113583 non-null  object             \n",
      " 11  video_published_at           113583 non-null  datetime64[ns, UTC]\n",
      " 12  video_views                  113583 non-null  int64              \n",
      " 13  video_likes                  113583 non-null  int64              \n",
      " 14  video_duration               113583 non-null  float64            \n",
      " 15  video_tags                   113583 non-null  object             \n",
      " 16  video_category_id            113583 non-null  int64              \n",
      " 17  relacion_evento              110799 non-null  object             \n",
      " 18  evento                       110799 non-null  object             \n",
      " 19  tipo_evento                  110799 non-null  object             \n",
      " 20  condiciones_cuenta           113583 non-null  object             \n",
      " 21  account_created_at           53843 non-null   object             \n",
      " 22  channel_id                   113583 non-null  object             \n",
      " 23  subscriber_count             113583 non-null  int64              \n",
      " 24  days_since_account_creation  53843 non-null   float64            \n",
      " 25  insulto                      113583 non-null  bool               \n",
      " 26  n_insultos                   113583 non-null  int64              \n",
      " 27  comment_clean                111037 non-null  object             \n",
      " 28  text_with_ctx                113583 non-null  object             \n",
      " 29  label_rule                   14016 non-null   object             \n",
      " 30  regla_aplicada               14016 non-null   object             \n",
      " 31  label_ml                     99567 non-null   object             \n",
      " 32  ml_proba_max                 99567 non-null   float64            \n",
      " 33  ml_margen                    99567 non-null   float64            \n",
      " 34  ml_entropia                  99567 non-null   float64            \n",
      " 35  label_final                  105083 non-null  object             \n",
      " 36  clasificacion_origen         113583 non-null  object             \n",
      " 37  lang                         111119 non-null  object             \n",
      " 38  user_country                 14057 non-null   object             \n",
      " 39  user_region                  14057 non-null   object             \n",
      " 40  bot_flag                     113583 non-null  bool               \n",
      " 41  bot_score                    113583 non-null  float64            \n",
      " 42  user_segment                 113583 non-null  object             \n",
      " 43  user_rank                    113583 non-null  int32              \n",
      " 44  user_n_comments              113583 non-null  int64              \n",
      " 45  user_days_active             113583 non-null  int64              \n",
      " 46  user_freq_diaria             113583 non-null  float64            \n",
      " 47  eje_argumentativo            8403 non-null    object             \n",
      " 48  term_especial                2684 non-null    object             \n",
      " 49  evento_fecha                 110799 non-null  datetime64[ns, UTC]\n",
      " 50  sub_tipo_evento              110799 non-null  object             \n",
      "dtypes: bool(3), datetime64[ns, UTC](3), float64(7), int32(1), int64(10), object(27)\n",
      "memory usage: 41.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86bbb7e",
   "metadata": {},
   "source": [
    "# Detecci√≥n del idioma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "51906162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ df['lang'] creado (ISO-639-1; NaN si indeterminado)\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Detectar idioma ‚Üí df[\"lang\"] (ISO-639-1)\n",
    "#  - NaN si el texto es muy corto o casi no ling√º√≠stico (emojis/URLs)\n",
    "#  - RU/UK por alfabeto (cir√≠lico vs latino)\n",
    "#  - langid obligatorio (sin fallback a 'es')\n",
    "# ===========================================\n",
    "\n",
    "URL_RX   = re.compile(r\"http[s]?://|www\\.\", re.I)\n",
    "EMOJI_RX = re.compile(r\"[^\\w\\s,.\\-¬°!¬ø?\\(\\)\\\"'@#:/]\", re.I)    # aproximado\n",
    "RX_LAT   = re.compile(r\"[a-z√°√©√≠√≥√∫√±√º]\", re.I)\n",
    "RX_CYR   = re.compile(r\"[–∞-—è—ñ—ó—î“ë—ë]\", re.I)\n",
    "\n",
    "# Opcional (mejora precisi√≥n): restringimos a idiomas m√°s probables del corpus\n",
    "langid.set_languages([\"es\",\"ru\",\"uk\",\"en\",\"pt\",\"it\",\"fr\",\"de\"])\n",
    "\n",
    "def _strip_urls(s: str) -> str:\n",
    "    return URL_RX.sub(\"\", s)\n",
    "\n",
    "def _language_of(text: str) -> str:\n",
    "    if text is None: return np.nan\n",
    "    raw = str(text)\n",
    "    if not raw.strip(): return np.nan\n",
    "\n",
    "    lat = len(RX_LAT.findall(raw))\n",
    "    cyr = len(RX_CYR.findall(raw))\n",
    "    non_alpha_frac = len(EMOJI_RX.findall(raw)) / max(1, len(raw))\n",
    "\n",
    "    # NaN si casi no hay letras o es mayormente s√≠mbolos/emojis\n",
    "    if (lat + cyr) < 3 or non_alpha_frac > 0.6:\n",
    "        return np.nan\n",
    "\n",
    "    # Cir√≠lico predominante ‚Üí ru/uk por letras exclusivas\n",
    "    if cyr > lat and cyr > 3:\n",
    "        return \"uk\" if re.search(r\"[—ñ—ó—î“ë]\", raw) else \"ru\"\n",
    "\n",
    "    # Clasificaci√≥n con langid (sobre texto con URLs removidas)\n",
    "    lg = langid.classify(_strip_urls(raw))[0]\n",
    "    lg = str(lg).lower()\n",
    "    return lg if (len(lg) == 2 and lg.isalpha()) else np.nan\n",
    "\n",
    "df[\"lang\"] = df[\"comment\"].apply(_language_of)\n",
    "\n",
    "print(\"df['lang'] creado (ISO-639-1; NaN si indeterminado)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1467639a",
   "metadata": {},
   "source": [
    "# Detecci√≥n del Pais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "26e9e41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A√±adidas columnas en df: user_country (ISO-2), user_region (Europe/LatAm/None)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# Pa√≠s/Regi√≥n AUTO-IDENTIFICADO por USUARIO (LatAm + Europa)\n",
    "#   - Lee se√±ales en comentarios (flags üá¶üá∑, \"saludos desde...\", gentilicios, hashtags)\n",
    "#   - Asigna 2 columnas nuevas en el master:\n",
    "#       user_country (ISO-2) y user_region (Europe/LatAm/Other)\n",
    "#   - Sin exports (solo define OUTPUT_DIR para m√°s adelante)\n",
    "# ==========================================================\n",
    "\n",
    "\n",
    "OUTPUT_DIR = \"../data/processed\"  # definido para el final del proyecto, no se usa ahora\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    s = str(s).lower()\n",
    "    return unicodedata.normalize(\"NFKD\", s).encode(\"ascii\",\"ignore\").decode(\"ascii\")\n",
    "\n",
    "# --- mapas de pa√≠ses (alias + gentilicios) ---\n",
    "ALIAS = {\n",
    "    # Europa\n",
    "    \"espana\":\"ES\",\"espa√±a\":\"ES\",\"francia\":\"FR\",\"alemania\":\"DE\",\"italia\":\"IT\",\"portugal\":\"PT\",\"polonia\":\"PL\",\n",
    "    \"suiza\":\"CH\",\"austria\":\"AT\",\"suecia\":\"SE\",\"noruega\":\"NO\",\"finlandia\":\"FI\",\"dinamarca\":\"DK\",\"estonia\":\"EE\",\"letonia\":\"LV\",\"lituania\":\"LT\",\n",
    "    \"chequia\":\"CZ\",\"republica checa\":\"CZ\",\"eslovaquia\":\"SK\",\"hungria\":\"HU\",\"grecia\":\"GR\",\"bulgaria\":\"BG\",\"rumania\":\"RO\",\"romania\":\"RO\",\n",
    "    \"croacia\":\"HR\",\"serbia\":\"RS\",\"bosnia\":\"BA\",\"eslovenia\":\"SI\",\n",
    "    # LatAm\n",
    "    \"argentina\":\"AR\",\"mexico\":\"MX\",\"m√©xico\":\"MX\",\"chile\":\"CL\",\"uruguay\":\"UY\",\"paraguay\":\"PY\",\"peru\":\"PE\",\"per√∫\":\"PE\",\n",
    "    \"bolivia\":\"BO\",\"colombia\":\"CO\",\"ecuador\":\"EC\",\"venezuela\":\"VE\",\"costa rica\":\"CR\",\"panama\":\"PA\",\"panam√°\":\"PA\",\n",
    "    \"guatemala\":\"GT\",\"honduras\":\"HN\",\"nicaragua\":\"NI\",\"el salvador\":\"SV\",\"republica dominicana\":\"DO\",\"cuba\":\"CU\",\"puerto rico\":\"PR\",\n",
    "}\n",
    "DEMONYMS = {\n",
    "    # Europa\n",
    "    \"espanol\":\"ES\",\"espa√±ol\":\"ES\",\"espanola\":\"ES\",\"espa√±ola\":\"ES\",\"frances\":\"FR\",\"francesa\":\"FR\",\"aleman\":\"DE\",\"alemana\":\"DE\",\n",
    "    \"italiano\":\"IT\",\"italiana\":\"IT\",\"portugues\":\"PT\",\"portuguesa\":\"PT\",\"polaco\":\"PL\",\"polaca\":\"PL\",\n",
    "    \"britanico\":\"GB\",\"brit√°nica\":\"GB\",\"irlandes\":\"IE\",\"irlandesa\":\"IE\",\"holandes\":\"NL\",\"holandesa\":\"NL\",\n",
    "    \"belga\":\"BE\",\"suizo\":\"CH\",\"suiza\":\"CH\",\"austriaco\":\"AT\",\"austriaca\":\"AT\",\"sueco\":\"SE\",\"sueca\":\"SE\",\"noruego\":\"NO\",\"noruega\":\"NO\",\n",
    "    \"finlandes\":\"FI\",\"finlandesa\":\"FI\",\"danes\":\"DK\",\"danesa\":\"DK\",\"estonio\":\"EE\",\"leton\":\"LV\",\"letona\":\"LV\",\"lituano\":\"LT\",\"lituana\":\"LT\",\n",
    "    \"checo\":\"CZ\",\"checa\":\"CZ\",\"eslovaco\":\"SK\",\"eslovaca\":\"SK\",\"hungaro\":\"HU\",\"hungara\":\"HU\",\"griego\":\"GR\",\"griega\":\"GR\",\n",
    "    \"bulgaro\":\"BG\",\"bulgara\":\"BG\",\"rumano\":\"RO\",\"rumana\":\"RO\",\"croata\":\"HR\",\"serbio\":\"RS\",\"bosnio\":\"BA\",\"esloveno\":\"SI\",\n",
    "    # LatAm\n",
    "    \"argentino\":\"AR\",\"argentina\":\"AR\",\"mexicano\":\"MX\",\"mexicana\":\"MX\",\"chileno\":\"CL\",\"chilena\":\"CL\",\"uruguayo\":\"UY\",\"uruguaya\":\"UY\",\n",
    "    \"paraguayo\":\"PY\",\"paraguaya\":\"PY\",\"peruano\":\"PE\",\"peruana\":\"PE\",\"boliviano\":\"BO\",\"boliviana\":\"BO\",\"colombiano\":\"CO\",\"colombiana\":\"CO\",\n",
    "    \"ecuatoriano\":\"EC\",\"ecuatoriana\":\"EC\",\"venezolano\":\"VE\",\"venezolana\":\"VE\",\"costarricense\":\"CR\",\"panameno\":\"PA\",\"paname√±a\":\"PA\",\n",
    "    \"guatemalteco\":\"GT\",\"guatemalteca\":\"GT\",\"hondureno\":\"HN\",\"hondure√±a\":\"HN\",\"nicaraguense\":\"NI\",\"salvadoreno\":\"SV\",\"salvadore√±a\":\"SV\",\n",
    "    \"dominicano\":\"DO\",\"dominicana\":\"DO\",\"cubano\":\"CU\",\"cubana\":\"CU\",\"puertorriqueno\":\"PR\",\"puertorrique√±a\":\"PR\",\n",
    "}\n",
    "EU    = {\"ES\",\"FR\",\"DE\",\"IT\",\"PT\",\"PL\",\"GB\",\"IE\",\"NL\",\"BE\",\"CH\",\"AT\",\"SE\",\"NO\",\"FI\",\"DK\",\"EE\",\"LV\",\"LT\",\"CZ\",\"SK\",\"HU\",\"GR\",\"BG\",\"RO\",\"HR\",\"RS\",\"BA\",\"SI\"}\n",
    "LATAM = {\"AR\",\"MX\",\"CL\",\"UY\",\"PY\",\"PE\",\"BO\",\"CO\",\"EC\",\"VE\",\"CR\",\"PA\",\"GT\",\"HN\",\"NI\",\"SV\",\"DO\",\"CU\",\"PR\"}\n",
    "\n",
    "def _region_of(code: str) -> str:\n",
    "    if code in EU: return \"Europe\"\n",
    "    if code in LATAM: return \"LatAm\"\n",
    "    return np.nan\n",
    "\n",
    "# --- flags üá¶üá∑ ‚Üí ISO2 ---\n",
    "def _iso_from_flags(text: str):\n",
    "    raw = str(text)\n",
    "    vals = []\n",
    "    for ch in raw:\n",
    "        o = ord(ch)\n",
    "        vals.append(o - 0x1F1E6 if 0x1F1E6 <= o <= 0x1F1FF else None)\n",
    "    out, i = [], 0\n",
    "    while i < len(vals)-1:\n",
    "        if vals[i] is not None and vals[i+1] is not None:\n",
    "            out.append(chr(vals[i]+65) + chr(vals[i+1]+65))\n",
    "            i += 2\n",
    "        else:\n",
    "            i += 1\n",
    "    return [c for c in out if c in (EU | LATAM)]\n",
    "\n",
    "# --- patrones de auto-identificaci√≥n ---\n",
    "PAT_SELF = re.compile(r\"\\b(saludos(?:\\s+desde)?|soy\\s+de|somos\\s+de|aqui\\s+en|aqu√≠\\s+en|desde|reportando\\s+desde)\\b\", re.I)\n",
    "ALIAS_RX    = re.compile(r\"\\b(\" + \"|\".join(re.escape(k) for k in sorted(ALIAS, key=len, reverse=True)) + r\")\\b\", re.I)\n",
    "DEMONYMS_RX = re.compile(r\"\\b(\" + \"|\".join(re.escape(k) for k in sorted(DEMONYMS, key=len, reverse=True)) + r\")\\b\", re.I)\n",
    "HASHTAG_RX  = re.compile(r\"#([a-zA-Z√°√©√≠√≥√∫√±√º]{3,})\")\n",
    "\n",
    "# m√©todo ‚Üí prioridad (m√°s alto = mejor)\n",
    "METHOD_PRI = {\"flag\":5,\"self+alias\":4,\"self+demonym\":3,\"hashtag\":2,\"hashtag_demonym\":1,\"demonym\":1,\"none\":0}\n",
    "CONF_PRI   = {\"high\":3,\"med\":2,\"low\":1}\n",
    "\n",
    "def detect_self_country(text: str):\n",
    "    \"\"\"\n",
    "    Devuelve tupla (code, region, conf, method) o ('Unknown','Unknown','low','none')\n",
    "    Prioridad: Flag > (self+alias) > (self+demonym) > hashtag > demonym\n",
    "    \"\"\"\n",
    "    raw = str(text)\n",
    "    norm = _norm(raw)\n",
    "\n",
    "    flags = _iso_from_flags(raw)\n",
    "    if flags:\n",
    "        code = flags[0]\n",
    "        return (code, _region_of(code), \"high\", \"flag\")\n",
    "\n",
    "    if PAT_SELF.search(norm):\n",
    "        m = ALIAS_RX.search(norm)\n",
    "        if m:\n",
    "            code = ALIAS[m.group(1)]\n",
    "            return (code, _region_of(code), \"high\", \"self+alias\")\n",
    "        m = DEMONYMS_RX.search(norm)\n",
    "        if m:\n",
    "            code = DEMONYMS[m.group(1)]\n",
    "            return (code, _region_of(code), \"med\", \"self+demonym\")\n",
    "\n",
    "    hashtags = [_norm(x) for x in HASHTAG_RX.findall(raw)]\n",
    "    for h in hashtags:\n",
    "        if h in ALIAS:\n",
    "            code = ALIAS[h]\n",
    "            return (code, _region_of(code), \"med\", \"hashtag\")\n",
    "        if h in DEMONYMS:\n",
    "            code = DEMONYMS[h]\n",
    "            return (code, _region_of(code), \"low\", \"hashtag_demonym\")\n",
    "\n",
    "    m2 = DEMONYMS_RX.search(norm)\n",
    "    if m2:\n",
    "        code = DEMONYMS[m2.group(1)]\n",
    "        return (code, _region_of(code), \"low\", \"demonym\")\n",
    "\n",
    "    return (\"Unknown\",\"Unknown\",\"low\",\"none\")\n",
    "\n",
    "# --- detecci√≥n por comentario ---\n",
    "tmp = df[[\"user_id\",\"comment_id\",\"comment_clean\"]].copy()\n",
    "vals = tmp[\"comment_clean\"].fillna(\"\").map(detect_self_country).tolist()\n",
    "tmp[[\"code\",\"region\",\"conf\",\"method\"]] = pd.DataFrame(vals, index=tmp.index)\n",
    "\n",
    "# conservamos solo pa√≠ses v√°lidos (EU/LatAm)\n",
    "valid = tmp[tmp[\"code\"].isin(EU | LATAM)].copy()\n",
    "if not valid.empty:\n",
    "    # agregamos por usuario y pa√≠s: conteo + mejores se√±ales\n",
    "    valid[\"conf_score\"]   = valid[\"conf\"].map(CONF_PRI).astype(int)\n",
    "    valid[\"method_score\"] = valid[\"method\"].map(METHOD_PRI).astype(int)\n",
    "\n",
    "    grp = (valid\n",
    "           .groupby([\"user_id\",\"code\",\"region\"], as_index=False)\n",
    "           .agg(\n",
    "               n_mentions = (\"comment_id\",\"count\"),\n",
    "               max_conf   = (\"conf_score\",\"max\"),\n",
    "               max_method = (\"method_score\",\"max\"),\n",
    "           ))\n",
    "\n",
    "    # para cada user elegimos el (code,region) con mayor (max_conf, n_mentions, max_method)\n",
    "    grp.sort_values([\"user_id\",\"max_conf\",\"n_mentions\",\"max_method\"],\n",
    "                    ascending=[True, False, False, False], inplace=True)\n",
    "    best = grp.groupby(\"user_id\", as_index=False).first()[[\"user_id\",\"code\",\"region\"]]\n",
    "    best.rename(columns={\"code\":\"user_country\",\"region\":\"user_region\"}, inplace=True)\n",
    "\n",
    "    # merge al master\n",
    "    df = df.merge(best, on=\"user_id\", how=\"left\")\n",
    "else:\n",
    "    # si no hay ninguna auto-identificaci√≥n v√°lida, asignamos NaN (no exportamos nada)\n",
    "    df[\"user_country\"] = np.nan\n",
    "    df[\"user_region\"]  = np.nan\n",
    "\n",
    "print(\"A√±adidas columnas en df: user_country (ISO-2), user_region (Europe/LatAm/None)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ab41b2",
   "metadata": {},
   "source": [
    "# Time Stats by User --> Bots & Core users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "578747cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 M√©tricas por usuario (tiempo + comportamiento)\n",
    "\n",
    "# ===========================================\n",
    "# Time Stats by User ‚Äî m√©tricas base por usuario\n",
    "#  - Calcula: user_n_comments, user_days_active, user_freq_diaria,\n",
    "#             mean_gap_h, burstiness, hour_var,\n",
    "#             n_videos, n_channels, dup_ratio, url_rate, mention_rate,\n",
    "#             emoji_frac, insulto_rate, n_insultos_mean, channel_entropy\n",
    "#  - Crea 'user_feats' para la celda 2\n",
    "# ===========================================\n",
    "\n",
    "\n",
    "# 1) Parsing √∫nico de tiempo\n",
    "df[\"comment_time\"] = pd.to_datetime(df[\"comment_time\"], errors=\"coerce\", utc=True)\n",
    "g = df.groupby(\"user_id\", dropna=False)\n",
    "\n",
    "# 2) Utilidades\n",
    "URL_RX   = re.compile(r\"http[s]?://|www\\.\", re.I)\n",
    "MENT_RX  = re.compile(r\"@[\\w_]+\")\n",
    "EMOJI_RX = re.compile(r\"[^\\w\\s,.\\-¬°!¬ø?\\(\\)\\\"'@#:/]\")\n",
    "\n",
    "def _frac_emojis(s: str) -> float:\n",
    "    s = str(s)\n",
    "    return len(EMOJI_RX.findall(s)) / max(1, len(s))\n",
    "\n",
    "def _norm_text(s: str) -> str:\n",
    "    s = str(s).lower()\n",
    "    s = re.sub(r\"\\s+\",\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def _hash(s: str) -> str:\n",
    "    return hashlib.md5(_norm_text(s).encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def _entropy(items) -> float:\n",
    "    if not items:\n",
    "        return 0.0\n",
    "    c = Counter(items); tot = sum(c.values()); p = [v/tot for v in c.values()]\n",
    "    return -sum(pi*log2(pi) for pi in p if pi > 0)\n",
    "\n",
    "def _to_ns(series_dt: pd.Series) -> np.ndarray:\n",
    "    arr = series_dt.dropna().sort_values().values\n",
    "    return arr.astype(\"datetime64[ns]\").astype(\"int64\")\n",
    "\n",
    "def _mean_gap_h(ns_arr: np.ndarray) -> float:\n",
    "    n = ns_arr.size\n",
    "    if n < 2:\n",
    "        return np.nan\n",
    "    gaps_h = np.diff(ns_arr) / (1e9 * 3600.0)\n",
    "    return float(np.mean(gaps_h))\n",
    "\n",
    "def _burst(ns_arr: np.ndarray) -> float:\n",
    "    n = ns_arr.size\n",
    "    if n < 3:\n",
    "        return 0.0\n",
    "    diffs = np.diff(ns_arr).astype(\"float64\")\n",
    "    mu = diffs.mean()\n",
    "    if mu <= 0:\n",
    "        return 0.0\n",
    "    return float(diffs.std(ddof=0) / (mu + 1e-9))\n",
    "\n",
    "def _hour_var(series_dt: pd.Series) -> float:\n",
    "    hrs = series_dt.dropna().dt.hour.to_numpy()\n",
    "    return float(np.var(hrs, ddof=0)) if hrs.size else 0.0\n",
    "\n",
    "def user_days_active(series_dt: pd.Series) -> int:\n",
    "    return int(series_dt.dropna().dt.floor(\"D\").nunique())\n",
    "\n",
    "# 3) C√°lculos por usuario (vectorizados)\n",
    "ns_arrays = g[\"comment_time\"].apply(_to_ns)\n",
    "\n",
    "user_feats = pd.DataFrame({\n",
    "    \"user_id\": ns_arrays.index.astype(str),\n",
    "    \"mean_gap_h\": ns_arrays.apply(_mean_gap_h).values,\n",
    "    \"burstiness\": ns_arrays.apply(_burst).values,\n",
    "    \"hour_var\": g[\"comment_time\"].apply(_hour_var).values,\n",
    "    \"user_days_active\": g[\"comment_time\"].apply(user_days_active).values,\n",
    "})\n",
    "\n",
    "user_feats[\"user_n_comments\"] = g.size().values\n",
    "user_feats[\"user_freq_diaria\"] = user_feats[\"user_n_comments\"] / np.maximum(1, user_feats[\"user_days_active\"])\n",
    "\n",
    "# 4) Diversidad y se√±ales de spam/estilo\n",
    "hashes = g[\"comment_clean\"].apply(lambda s: [_hash(x) for x in s])\n",
    "user_feats[\"dup_ratio\"] = hashes.apply(lambda h: 1.0 - (len(set(h)) / len(h) if len(h) else 1.0)).values\n",
    "\n",
    "user_feats[\"n_videos\"]   = g[\"video_title\"].nunique().values\n",
    "user_feats[\"n_channels\"] = g[\"channel_title\"].nunique().values\n",
    "\n",
    "user_feats[\"url_rate\"]     = g[\"comment\"].apply(lambda s: float(np.mean([bool(URL_RX.search(str(x))) for x in s]))).values\n",
    "user_feats[\"mention_rate\"] = g[\"comment\"].apply(lambda s: float(np.mean([bool(MENT_RX.search(str(x))) for x in s]))).values\n",
    "user_feats[\"emoji_frac\"]   = g[\"comment\"].apply(lambda s: float(np.mean([_frac_emojis(x) for x in s]))).values\n",
    "\n",
    "user_feats[\"insulto_rate\"]    = g[\"insulto\"].mean().values\n",
    "user_feats[\"n_insultos_mean\"] = g[\"n_insultos\"].mean().values\n",
    "user_feats[\"channel_entropy\"] = g[\"channel_title\"].apply(lambda s: _entropy(list(s))).values\n",
    "\n",
    "# 5) Tipos num√©ricos consistentes\n",
    "num_cols = user_feats.columns.difference([\"user_id\"])\n",
    "user_feats[num_cols] = user_feats[num_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49d1f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Time Stats by User --> Bots & Segments (CELDA 2 actualizada)\n",
    "#  - Usa 'user_feats' creado en Celda 1\n",
    "#  - Crea: bot_flag, bot_score, user_segment, user_rank\n",
    "#  - Merge al df y limpia intermedias si quedaron en df\n",
    "# ===========================================\n",
    "\n",
    "\n",
    "# ---------- 1) Cuantiles para reglas (sobre user_feats) ----------\n",
    "Q = user_feats[[\"user_n_comments\",\"n_channels\",\"user_freq_diaria\"]].quantile(\n",
    "    [0.35, 0.50, 0.85, 0.90, 0.95, 0.97]\n",
    ")\n",
    "def q(p, col): return float(Q.loc[p, col])\n",
    "\n",
    "# ---------- 2) Bot score (heur√≠stico) + umbral din√°mico (p98, conservador) ----------\n",
    "acct_median = (\n",
    "    df.groupby(\"user_id\")[\"days_since_account_creation\"]\n",
    "      .median()\n",
    "      .reindex(user_feats[\"user_id\"])\n",
    "      .fillna(365)\n",
    "      .to_numpy()\n",
    ")\n",
    "\n",
    "spam_proxy = 0.5*user_feats[\"url_rate\"] + 0.3*user_feats[\"dup_ratio\"] + 0.2*user_feats[\"mention_rate\"]\n",
    "\n",
    "bot_score = (\n",
    "    0.28*user_feats[\"dup_ratio\"] +\n",
    "    0.18*spam_proxy +\n",
    "    0.12*(user_feats[\"user_n_comments\"] >= 50).astype(float) +\n",
    "    0.12*(user_feats[\"mean_gap_h\"] <= 0.25).fillna(0).astype(float) +\n",
    "    0.10*(user_feats[\"emoji_frac\"] > 0.08).astype(float) +\n",
    "    0.12*(user_feats[\"insulto_rate\"] > 0.2).astype(float) +\n",
    "    0.08*(acct_median < 30).astype(float)\n",
    ").clip(0, 1)\n",
    "user_feats[\"bot_score\"] = bot_score\n",
    "\n",
    "TH = max(0.55, float(user_feats[\"bot_score\"].quantile(0.98)))\n",
    "likely_bot_heur = (user_feats[\"bot_score\"] >= TH)\n",
    "\n",
    "# ---------- 3) IsolationForest (complementario; m√°s estricto) ----------\n",
    "iso_cols = [\n",
    "    \"dup_ratio\",\"user_n_comments\",\"n_videos\",\"n_channels\",\n",
    "    \"mean_gap_h\",\"emoji_frac\",\"insulto_rate\",\"user_freq_diaria\",\n",
    "    \"hour_var\",\"channel_entropy\"\n",
    "]\n",
    "X  = user_feats[iso_cols].replace([np.inf, -np.inf], np.nan).fillna(0).values\n",
    "Xz = StandardScaler().fit_transform(X)\n",
    "\n",
    "iso = IsolationForest(n_estimators=400, contamination=0.02, random_state=42)\n",
    "likely_bot_iso = (iso.fit_predict(Xz) == -1)\n",
    "\n",
    "user_feats[\"bot_flag\"] = likely_bot_heur | likely_bot_iso\n",
    "\n",
    "# ---------- 4) Segmentaci√≥n (ACTIVO; ESPOR√ÅDICO = 1 comentario) ----------\n",
    "seg = np.full(len(user_feats), \"activo\", dtype=object)  # base m√°s amplia\n",
    "\n",
    "# 4.1 sospecha_bot (prioridad m√°xima)\n",
    "seg = np.where(user_feats[\"bot_flag\"], \"sospecha_bot\", seg)\n",
    "\n",
    "# 4.2 n√∫cleo_duro\n",
    "mask_core = (\n",
    "    (user_feats[\"user_n_comments\"] >= q(0.97, \"user_n_comments\")) &\n",
    "    ((user_feats[\"n_channels\"] >= q(0.85, \"n_channels\")) | (user_feats[\"user_freq_diaria\"] >= q(0.95, \"user_freq_diaria\")))\n",
    ")\n",
    "seg = np.where((seg != \"sospecha_bot\") & mask_core, \"nucleo_duro\", seg)\n",
    "\n",
    "# 4.3 fiel\n",
    "mask_fiel = (\n",
    "    (user_feats[\"user_n_comments\"] >= q(0.85, \"user_n_comments\")) &\n",
    "    (user_feats[\"n_channels\"] <= q(0.35, \"n_channels\"))\n",
    ")\n",
    "seg = np.where((seg == \"activo\") & mask_fiel, \"fiel\", seg)\n",
    "\n",
    "# 4.4 espor√°dico = 1 comentario (y que no sea bot/core/fiel)\n",
    "mask_espo = (user_feats[\"user_n_comments\"] == 1)\n",
    "seg = np.where((seg == \"activo\") & mask_espo, \"esporadico\", seg)\n",
    "\n",
    "user_feats[\"user_segment\"] = seg\n",
    "\n",
    "# ---------- 5) Ranking por actividad ----------\n",
    "user_feats[\"user_rank\"] = user_feats[\"user_n_comments\"].rank(method=\"dense\", ascending=False).astype(int)\n",
    "\n",
    "# ---------- 6) Merge m√≠nimo al df (sin sufijos) ----------\n",
    "final_cols = [\"user_id\",\"bot_flag\",\"bot_score\",\"user_segment\",\"user_rank\",\n",
    "              \"user_n_comments\",\"user_days_active\",\"user_freq_diaria\"]\n",
    "\n",
    "df = df.drop(columns=[c for c in final_cols if c in df.columns and c != \"user_id\"], errors=\"ignore\")\n",
    "df = df.merge(user_feats[final_cols], on=\"user_id\", how=\"left\", validate=\"m:1\")\n",
    "\n",
    "# ---------- 7) Limpieza de posibles intermedias en df ----------\n",
    "drop_if_present = [\n",
    "    \"mean_gap_h\",\"burstiness\",\"hour_var\",\"n_videos\",\"n_channels\",\n",
    "    \"dup_ratio\",\"url_rate\",\"mention_rate\",\"emoji_frac\",\"insulto_rate\",\n",
    "    \"n_insultos_mean\",\"channel_entropy\"\n",
    "]\n",
    "df.drop(columns=[c for c in drop_if_present if c in df.columns], inplace=True, errors=\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "38998fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conteo de valores para 'bot_flag':\n",
      "\n",
      "0               97203 (85.58%)\n",
      "1               16380 (14.42%)\n"
     ]
    }
   ],
   "source": [
    "col = \"bot_flag\"\n",
    "\n",
    "conteos = df[col].value_counts()\n",
    "total = conteos.sum()\n",
    "\n",
    "print(f\"Conteo de valores para '{col}':\\n\")\n",
    "for valor, count in conteos.items():\n",
    "    porcentaje = count / total * 100\n",
    "    print(f\"{valor:<15} {count:>5} ({porcentaje:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a1ee79",
   "metadata": {},
   "source": [
    "# Classificaci√≥n -1 a 1 de sentimientos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c4de9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Funci√≥n que mapea a -1, 0, 1 seg√∫n el 'compound'\n",
    "def sentiment_label_vader(text, low= -0.05, high=0.05):\n",
    "    s = sia.polarity_scores(text)['compound']\n",
    "    if s >= high: return  1\n",
    "    if s <= low:  return -1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f925aa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['comment'].astype(str).apply(sentiment_label_vader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "831b3bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts:\n",
      " sentiment\n",
      "-1    34969\n",
      " 0    72563\n",
      " 1     6244\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentages:\n",
      " sentiment\n",
      "-1    30.73\n",
      " 0    63.78\n",
      " 1     5.49\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Conteos absolutos y relativos\n",
    "counts = df['sentiment'].value_counts().sort_index()\n",
    "props  = df['sentiment'].value_counts(normalize=True).sort_index() * 100\n",
    "\n",
    "print(\"Counts:\\n\", counts)\n",
    "print(\"\\nPercentages:\\n\", props.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd2cac21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Clase -1 ===\n",
      "- Miguel üëèüëè estoy impactada üò¢üò¢üò¢ üòûüòî no puedo ni hablar.\n",
      "- No entiendo nada por el canal prensa alternativa de Juan Jos√© del castillo publican que Rusia üá∑üá∫ est√° acabando con los rebeldes los mig 31 y sucoy 35 est√°n bombardeado sin piedad a los extremistas kurdos y por aqu√≠ dicen lo contrario as√≠ que no entiendo nada\n",
      "- TU ERES PROCOMUNISMO JAJAJAJAJA YA VI VARIOS DE TUS VIDEOS Y TODOS USTEDES HACEN POSTURAS DE CONDOLENCIAS A GOBERNANTES COMUNISTAS, POBRE Y LAMENTABLE, PERO ESTA ES LA GRAN PRUEBA PARA LA HUMANIDAD EL COMUNISMO!!!!!, AL FINAL ESTO ES UNA BATALLA ESPIRITUAL Y DE UNA VES DE AVISO EL COMUNISMO YA PERDIO Y TU PARECE SER QUE TE INCLINAS POR UN BANDO PERDEDOR!\n",
      "- No todo van a ser malas noticias para Rusia dice...y si no fuera por la OTAN Ucrania se mea encima y aun as√≠ solo esta consiguiendo una mierda bien gorda.\n",
      "- Estos cretinos no son concientes que est√°n abriendo las puertas del INFIERNO para la humanidad, los ciudadanos europeos deben rechazar este tipo de actitudes de sus Gobernantes\n",
      "\n",
      "=== Clase 0 ===\n",
      "- Pero es territorio ruso, de qu√© lloran esos ucranianos?\n",
      "- ¬øQu√© nadie est√° ganando?\n",
      "Putin est√° ganando, de momento territorio ucraniano sigue ocupado y la OTAN cada vez quiere invertir menos recursos en esta guerra‚Ä¶\n",
      "- Bueno yo esperaba una sola bomba at√≥mica directa a kiev .. Putin tiene demasiada paciencia...\n",
      "- Comentarista russo\n",
      "- QUIEREN UNA PAUSA PARA REARMARSE Y CONTINUAR ATACANDO A RUSIA, NADA DE ESO AHORA SE AGUANTAN PORQUERIAS..\n",
      "\n",
      "=== Clase 1 ===\n",
      "- Si, es comprensible el cabreo de Alemania si solo tenemos en cuenta el saboteo ucraniano del nordstream, pero la cosa es muy distinta si miramos mas atras. \n",
      "\n",
      "Hay que tener en cuenta que fue el mismisimo entonces canciller de Alemania Gerhard Schroder quien fue evidentemente comprado por Rusia para hacer a Europa dependiente energeticamente de esta, a cambio de darle al excanciller el puesto de presidente de accionistas de Nord Stream AG, controlada por Gazprom, y luego presidente de la junta directiva de Rosneft, principal petrolera de Rusia, en un caso escandaloso de puertas giratorias con conflicto de intereses. \n",
      "\n",
      "Por eso los demas paises de europa y estados unidos cerraron sus investigaciones sobre el sabotaje, porque sabian que fueron los ucranianos (probablemente con ayuda de estados unidos) y estaban todos a favor de la operacion. Alemania mas que enfadarse deberia aprender la leccion, meter en la carcel a Schroder y redoblar su apoyo a Ucrania, pero por desgracia seguramente los rusos aun tienen comprados a algunos alemanes.\n",
      "- Que onda tu videos. Son puramente propaganda progre. Solo losnlanzas cuando tienes algo progre que informar.\n",
      "\n",
      "La OTAN se hunde en Ucrania. Rusia a demostrado ser superior a la goepolitica de  3 potencias mundiales. \n",
      "\n",
      "Ademas se te perdieron los 5 abrahams estrenados por occidente y enterrados por rusia.\n",
      "- Retirada exitosa de tropas ucranianas. A'mos te que te ha faltado decir exitosas y felices. Yo sinceramente tengo que entender esto q aqu√≠  nos cuentas c√≥mo algo c√≥mico para re√≠rnos todos un poquito. No se me alcanza que todo esto lo digas en serio, y lo peor, intentes q nos lo creamos.\n",
      "- severo loot se ha metido ucrania\n",
      "- Cuando Rusia se retire del territorio ucraniano va a ser gracioso ver a estos ruso lovers meterse con Estados Unidos y Europa despu√©s de predicar que rusa iba ganando \"Avanzan por el mapa con lupa a cambio de 300 blindados\"\n"
     ]
    }
   ],
   "source": [
    "for s in [-1, 0, 1]:\n",
    "    print(f\"\\n=== Clase {s} ===\")\n",
    "    ejemplos = df[df['sentiment']==s]['comment'].sample(5, random_state=42).tolist()\n",
    "    for c in ejemplos:\n",
    "        print(\"-\", c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80272f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment              -1      0     1\n",
      "condiciones_cuenta                    \n",
      "noticiero            7862  17333  1214\n",
      "pro-ruso            16776  35988  2869\n",
      "pro-ucraniano       10331  19242  2161\n"
     ]
    }
   ],
   "source": [
    "tabla = (\n",
    "    df\n",
    "    .groupby(['condiciones_cuenta', 'sentiment'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "print(tabla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aefeb7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment             -1     0    1\n",
      "condiciones_cuenta                 \n",
      "noticiero           29.8  65.6  4.6\n",
      "pro-ruso            30.2  64.7  5.2\n",
      "pro-ucraniano       32.6  60.6  6.8\n"
     ]
    }
   ],
   "source": [
    "tabla_pct = tabla.div(tabla.sum(axis=1), axis=0).round(3)*100\n",
    "print(tabla_pct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecbe2b0",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172be9c6",
   "metadata": {},
   "source": [
    "# Arguments classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3cb307c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df['eje_argumentativo'] recalculado (insultos_al_creador ampliado con cr√≠tica/sarcasmo cerca del creador/video y desacople de bando).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TXT = \"comment_clean\" if \"comment_clean\" in df.columns else \"comment\"\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    s = str(s).lower()\n",
    "    s = re.sub(r\"\\brussia\\b\", \"rusia\", s)\n",
    "    s = re.sub(r\"\\botar\\b\", \"otan\", s)\n",
    "    s = re.sub(r\"\\bee\\s*\\.?\\s*uu\\s*\\.?\\b\", \"eeuu\", s)\n",
    "    s = re.sub(r\"\\bu\\.?\\s*e\\.?\\b\", \"ue\", s)\n",
    "    s = re.sub(r\"\\bnato\\b\", \"otan\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "text = df[TXT].astype(str).map(_norm)\n",
    "lbl  = df[\"label_final\"].astype(str).str.lower().fillna(\"\")\n",
    "chan = df[\"condiciones_cuenta\"].astype(str).str.lower().fillna(\"\")\n",
    "\n",
    "def channel_side(x: str) -> str:\n",
    "    if \"pro-ruso\" in x: return \"ruso\"\n",
    "    if \"pro-ucraniano\" in x: return \"ucraniano\"\n",
    "    return \"neutro\"\n",
    "chan_side = chan.map(channel_side)\n",
    "\n",
    "# -------- utilidades de proximidad --------\n",
    "TOKEN_SPLIT_RX = re.compile(r\"[^\\w√°√©√≠√≥√∫√º√±]+\", re.I)\n",
    "def split_sentences(s: str): return re.split(r\"[.!?¬°¬ø\\n\\r]+\", s)\n",
    "\n",
    "def has_proximity(sent: str, terms_a, terms_b, win=6) -> bool:\n",
    "    toks = [t for t in TOKEN_SPLIT_RX.split(sent) if t]\n",
    "    if not toks: return False\n",
    "    pos_a = [i for i, t in enumerate(toks) if t in terms_a]\n",
    "    pos_b = [i for i, t in enumerate(toks) if t in terms_b]\n",
    "    if not pos_a or not pos_b: return False\n",
    "    for i in pos_a:\n",
    "        for j in pos_b:\n",
    "            if abs(i - j) <= win:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def any_sentence(sentences, predicate):\n",
    "    for s in sentences:\n",
    "        if predicate(s): return True\n",
    "    return False\n",
    "\n",
    "# -------- l√©xicos --------\n",
    "CREATOR_TERMS = {\n",
    "    \"miguel\",\"borja\",\"memorias\",\"pez\",\"memoriasdepez\",\"canal\",\"video\",\"v√≠deo\",\"creador\",\"youtuber\",\n",
    "    \"tu\",\"tus\",\"te\",\"usted\",\"ustedes\",\"vos\",\"vosotros\"  # para proximidad con 2¬™ persona\n",
    "}\n",
    "CREATOR_STRONG = {\"miguel\",\"borja\",\"memorias\",\"memoriasdepez\",\"canal\"}\n",
    "\n",
    "THANK_TERMS = {\"gracias\",\"agradezco\",\"agradecemos\",\"agradecida\",\"agradecido\",\"milgracias\",\"muchasgracias\"}\n",
    "VIDEO_TERMS  = {\"video\",\"v√≠deo\",\"analisis\",\"an√°lisis\",\"directo\",\"contenido\",\"trabajo\"}\n",
    "\n",
    "# insulto expl√≠cito (como antes)\n",
    "INSULT_TERMS = {\n",
    "    \"basura\",\"vendido\",\"vendida\",\"vendidos\",\"vendidas\",\"payaso\",\"payasa\",\"payasos\",\"payasas\",\n",
    "    \"mentiroso\",\"mentirosa\",\"mentirosos\",\"mentirosas\",\"propagandista\",\"charlatan\",\"charlat√°n\",\n",
    "    \"estafa\",\"manipulador\",\"manipuladora\",\"manipuladores\",\"desinformador\",\"desinformadora\",\n",
    "    \"relatero\",\"relatera\",\"relateros\",\"relato\"\n",
    "}\n",
    "\n",
    "# NUEVO: l√©xico negativo/cr√≠tico no necesariamente insultante\n",
    "NEG_CREATOR_TERMS = {\n",
    "    \"relato\",\"relatos\",\"sesgado\",\"sesgada\",\"sesgados\",\"sesgadas\",\"tendencioso\",\"tendenciosa\",\n",
    "    \"clickbait\",\"click\",\"bait\",\"falso\",\"falsa\",\"falsedad\",\"fake\",\"enga√±oso\",\"enga√±osa\",\"enga√±o\",\n",
    "    \"mientes\",\"miente\",\"mentis\",\"ment√≠s\",\"mentir\",\"no sabes\",\"no sab√©s\",\"no tienes idea\",\n",
    "    \"p√©simo\",\"pesimo\",\"malo\",\"malisimo\",\"mal√≠simo\",\"pobre an√°lisis\",\"mal an√°lisis\",\n",
    "    \"desinformar\",\"desinformas\",\"desinform√°s\",\"desinformacion\",\"desinformaci√≥n\",\"manipulas\",\"manipul√°s\",\n",
    "    \"panfleto\",\"propaganda\",\"trola\",\"verso\"\n",
    "}\n",
    "NEG_CREATOR_RX = [\n",
    "    re.compile(r\"\\b(p[e√©]simo|mal[i√≠]simo|malo)\\s+(video|an[a√°]lisis)\\b\"),\n",
    "    re.compile(r\"\\b(gracias por el relato)\\b\"),\n",
    "    re.compile(r\"\\b(ment[i√≠]s|mientes|mentiroso)\\b\"),\n",
    "    re.compile(r\"\\b(desinform(a|√°s|as|ar))\\b\"),\n",
    "    re.compile(r\"\\b(sesgad[oa]s?|tendencios[oa]s?)\\b\"),\n",
    "    re.compile(r\"\\b(click\\s?bait|clickbait)\\b\"),\n",
    "]\n",
    "\n",
    "# sarcasmos para filtrar \"agradecimientos\"\n",
    "THANK_SARCASM_RX = [\n",
    "    re.compile(r\"\\bgracias a (usa|eeuu|otan|occidente|ue)\\b\"),\n",
    "    re.compile(r\"\\bgracias por el relato\\b\"),\n",
    "    re.compile(r\"\\b(me )?hacen.*risa.*gracias\\b\"),\n",
    "]\n",
    "\n",
    "# --- otros ejes (igual que versi√≥n previa estricta) ---\n",
    "def any_rx(s: str, rxs) -> bool:\n",
    "    return any(rx.search(s) for rx in rxs)\n",
    "\n",
    "RX_HIST = [\n",
    "    re.compile(r\"\\bhistori[ac]a?\\b\", re.I),\n",
    "    re.compile(r\"\\bcontexto hist[o√≥]rico\\b\", re.I),\n",
    "    re.compile(r\"\\bdesde (el|la) (a√±o|siglo)\\b\", re.I),\n",
    "    re.compile(r\"\\bprecedente\\b|\\bcomparaci[o√≥]n\\b\", re.I),\n",
    "    re.compile(r\"\\b(2014|maidan|donb[a√°]s)\\b\", re.I),\n",
    "]\n",
    "RX_MEMES = [\n",
    "    re.compile(r\"[üòÇüòÖü§£]+\"),\n",
    "    re.compile(r\"\\bjaja(ja)+\\b|\\bjeje(je)+\\b\", re.I),\n",
    "    re.compile(r\"\\bmem(e|es)\\b|\\bchiste(s)?\\b|\\bc[o√≥]mico\\b\", re.I),\n",
    "    re.compile(r\"\\bsevero loot\\b|\\bcope\\b|\\bcringe\\b\", re.I),\n",
    "]\n",
    "RX_PODER_RU = [\n",
    "    re.compile(r\"\\bpotencia (militar|industrial)\\b\", re.I),\n",
    "    re.compile(r\"\\bsuperioridad rusa\\b|\\barmas rusas\\b|\\bindustria rusa\\b\", re.I),\n",
    "    re.compile(r\"\\bcapacidad (b√©lica|rusa)\\b\", re.I),\n",
    "    re.compile(r\"\\bavanza (rusia|ej[e√©]rcito ruso)\\b|\\brusia (es|est[a√°]) (fuerte|superior)\\b\", re.I),\n",
    "]\n",
    "RX_CULPA_OTAN = [\n",
    "    re.compile(r\"\\bculpa (de )?(la )?(otan|occidente|ue|eeuu|usa)\\b\", re.I),\n",
    "    re.compile(r\"\\boccidente (provoc[o√≥]|provoca|empuj[o√≥])\\b\", re.I),\n",
    "    re.compile(r\"\\bexpansi[o√≥]n de la otan\\b|\\bsanci[o√≥]n(es)?\\b\", re.I),\n",
    "]\n",
    "RX_CULPA_RU = [\n",
    "    re.compile(r\"\\bculpa (de )?rusia\\b|\\brusia es (el|la) (agresor|culpable)\\b\", re.I),\n",
    "    re.compile(r\"\\binvasi[o√≥]n rusa\\b|\\bcr[i√≠]menes? de guerra (ruso|rusos)\\b\", re.I),\n",
    "]\n",
    "RX_DEF_RU = [\n",
    "    re.compile(r\"\\b(rusia|putin) (tiene|ten[i√≠]a) raz[o√≥]n\\b\", re.I),\n",
    "    re.compile(r\"\\bdefend(er|iendo) a rusia\\b\", re.I),\n",
    "    re.compile(r\"\\boperaci[o√≥]n especial (justa|leg[i√≠]tima)\\b|\\bdesnazificaci[o√≥]n\\b|\\bdesnazificar\\b\", re.I),\n",
    "]\n",
    "RX_DEF_UA = [\n",
    "    re.compile(r\"\\bucrania (se defiende|resiste|tiene raz[o√≥]n)\\b\", re.I),\n",
    "    re.compile(r\"\\bdefender a ucrania\\b|\\bresistencia ucraniana\\b|\\bderecho a defenderse\\b\", re.I),\n",
    "]\n",
    "RX_NEUTRAL = [\n",
    "    re.compile(r\"\\bneutral(es)?\\b|\\bimparcial(es)?\\b|\\bobjetiv[oa]s?\\b\", re.I),\n",
    "    re.compile(r\"\\b(no se sabe|no (se )?puede saber|ambos lados|ning[u√∫]n lado|ni rusia ni ucrania)\\b\", re.I),\n",
    "]\n",
    "\n",
    "def insults_at_creator_extended(s: str, insult_flag: bool, lab: str, chs: str) -> bool:\n",
    "    \"\"\"Marca insultos_al_creador si:\n",
    "       (A) insulto expl√≠cito + proximidad a creador/video (ventana corta), o\n",
    "       (B) bando comentario ‚â† bando canal y hay cr√≠tica/agresi√≥n cerca de creador/video\n",
    "           (l√©xico negativo o patrones de cr√≠tica), incluso sin insulto.\n",
    "    \"\"\"\n",
    "    sents = split_sentences(s)\n",
    "\n",
    "    # A) insulto expl√≠cito cerca del creador/video\n",
    "    def _direct_insult(sent):\n",
    "        return has_proximity(sent, CREATOR_STRONG, INSULT_TERMS, win=3) or \\\n",
    "               has_proximity(sent, CREATOR_TERMS, INSULT_TERMS, win=3)\n",
    "    if insult_flag and any_sentence(sents, _direct_insult):\n",
    "        # si adem√°s hay \"gracias\" + creador en la MISMA oraci√≥n, lo descartamos (posible iron√≠a ambigua)\n",
    "        def _grat_near_creator(sent):\n",
    "            return (\"gracias\" in sent) and has_proximity(sent, CREATOR_TERMS, THANK_TERMS | VIDEO_TERMS, win=4)\n",
    "        if not any_sentence(sents, _grat_near_creator):\n",
    "            return True\n",
    "\n",
    "    # B) cr√≠tica/ataque no insultante hacia creador/video, pero con desacople de bando\n",
    "    if lab != chs:\n",
    "        def _negative_creator(sent):\n",
    "            near_neg_lex = has_proximity(sent, CREATOR_TERMS | VIDEO_TERMS, NEG_CREATOR_TERMS, win=5)\n",
    "            neg_rx_hit   = any(rx.search(sent) for rx in NEG_CREATOR_RX)\n",
    "            return near_neg_lex or neg_rx_hit\n",
    "        if any_sentence(sents, _negative_creator):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def thanks_to_creator(s: str) -> bool:\n",
    "    if \"gracias\" not in s: return False\n",
    "    if any(rx.search(s) for rx in THANK_SARCASM_RX): return False\n",
    "    sents = split_sentences(s)\n",
    "    def _grat(sent):\n",
    "        has_thanks = has_proximity(sent, THANK_TERMS, CREATOR_TERMS | VIDEO_TERMS, win=5)\n",
    "        return has_thanks or re.search(r\"\\bgracias (miguel|borja|por (el )?(video|an[a√°]lisis))\\b\", sent)\n",
    "    return any_sentence(sents, _grat)\n",
    "\n",
    "def any_rx(s: str, rxs) -> bool: return any(rx.search(s) for rx in rxs)\n",
    "\n",
    "def pick_axis_row(s: str, label: str, chs: str, insult_flag: bool) -> str|None:\n",
    "    # 1) insulto/ataque al creador (ampliado)\n",
    "    if insults_at_creator_extended(s, insult_flag, label, chs):\n",
    "        return \"insultos_al_creador\"\n",
    "    # 2) agradecimiento real al creador\n",
    "    if thanks_to_creator(s):\n",
    "        return \"agradecimientos_al_creador\"\n",
    "    # 3) neutralidad condicionada\n",
    "    if label == \"neutro\" and any_rx(s, RX_NEUTRAL):\n",
    "        return \"neutralidad\"\n",
    "    # 4) tem√°ticos\n",
    "    if any_rx(s, RX_HIST):       return \"justificacion_historica\"\n",
    "    if any_rx(s, RX_MEMES):      return \"festejos_y_memes\"\n",
    "    if any_rx(s, RX_PODER_RU):   return \"poderio_ruso\"\n",
    "    if any_rx(s, RX_CULPA_OTAN): return \"culpa_de_la_otan_occidente\"\n",
    "    if any_rx(s, RX_CULPA_RU):   return \"culpa_de_rusia\"\n",
    "    if any_rx(s, RX_DEF_RU):     return \"defensa_acciones_rusas\"\n",
    "    if any_rx(s, RX_DEF_UA):     return \"defensa_acciones_ucranianas\"\n",
    "    return None\n",
    "\n",
    "df[\"eje_argumentativo\"] = [\n",
    "    pick_axis_row(s, l, c, bool(ins))\n",
    "    for s, l, c, ins in zip(text.values, lbl.values, chan_side.values, df[\"insulto\"].values)\n",
    "]\n",
    "\n",
    "print(\"df['eje_argumentativo'] recalculado (insultos_al_creador ampliado con cr√≠tica/sarcasmo cerca del creador/video y desacople de bando).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82aa35b",
   "metadata": {},
   "source": [
    "# Unigrams & Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab2cc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado:\n",
      " - ..\\data\\processed\\pbi_unigrams.csv\n",
      " - ..\\data\\processed\\pbi_bigrams.csv\n",
      "Ejemplo UNIGRAMS: scope_type scope_value    term   n    pct  rank  n_docs\n",
      "   channel     EL PA√çS   rusia 773 2.5083     1    3493\n",
      "   channel     EL PA√çS ucrania 597 1.9372     2    3493\n",
      "   channel     EL PA√çS  guerra 466 1.5121     3    3493\n",
      "   channel     EL PA√çS    ruso 417 1.3531     4    3493\n",
      "   channel     EL PA√çS    otan 350 1.1357     5    3493\n",
      "Ejemplo BIGRAMS : scope_type scope_value           term  n    pct  rank  n_docs\n",
      "   channel     EL PA√çS estados unidos 59 0.2156     1    3493\n",
      "   channel     EL PA√çS     viva rusia 37 0.1352     2    3493\n",
      "   channel     EL PA√çS guerra mundial 25 0.0913     3    3493\n",
      "   channel     EL PA√çS  rusia ucrania 23 0.0840     4    3493\n",
      "   channel     EL PA√çS  propio pueblo 23 0.0840     5    3493\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Unigramas / Bigrams para Power BI (r√°pido + filtro de ruido)\n",
    "#  - Scopes: label_final y channel_title\n",
    "#  - Quita risas/muletillas (\"jajaja\", \"bla bla\", \"xd\", etc.)\n",
    "#  - Mantiene dominio (rusia, ucrania, otan, eeuu) y normaliza alias\n",
    "#  - Bigramas exportados con ESPACIO (no guion bajo)\n",
    "#  - Salida: ../data/processed/pbi_unigrams.csv / pbi_bigrams.csv\n",
    "# ===========================================\n",
    "import pandas as pd, numpy as np, re\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "OUT_DIR = Path(\"../data/processed\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "UNI_PATH = OUT_DIR / \"pbi_unigrams.csv\"\n",
    "BI_PATH  = OUT_DIR / \"pbi_bigrams.csv\"\n",
    "\n",
    "TXT_COL = \"comment_clean\" if \"comment_clean\" in df.columns else \"comment\"\n",
    "TXT     = df[TXT_COL].astype(str).values\n",
    "LANGS   = (df[\"lang\"].astype(str).values if \"lang\" in df.columns else np.array([\"es\"]*len(df)))\n",
    "\n",
    "LABELS  = df[\"label_final\"].fillna(\"desconocido\").astype(str).values\n",
    "CHANNEL = df[\"channel_title\"].astype(str).values\n",
    "\n",
    "# --- Stopwords funcionales (sin dominio)\n",
    "stop_es = {\n",
    " \"a\",\"al\",\"algo\",\"algunas\",\"algunos\",\"ante\",\"antes\",\"como\",\"con\",\"contra\",\"cual\",\"cuando\",\"de\",\"del\",\"desde\",\"donde\",\n",
    " \"durante\",\"e\",\"el\",\"ella\",\"ellas\",\"ellos\",\"en\",\"entre\",\"era\",\"erais\",\"eran\",\"eras\",\"eres\",\"es\",\"esa\",\"esas\",\"ese\",\"eso\",\n",
    " \"esos\",\"esta\",\"estaba\",\"estaban\",\"estado\",\"estais\",\"estamos\",\"estan\",\"estar\",\"este\",\"esto\",\"estos\",\"estoy\",\"fin\",\"fue\",\n",
    " \"fueron\",\"fui\",\"fuimos\",\"ha\",\"habeis\",\"haber\",\"habia\",\"habla\",\"hablan\",\"hace\",\"hacia\",\"han\",\"hasta\",\"hay\",\"la\",\"las\",\n",
    " \"le\",\"les\",\"lo\",\"los\",\"mas\",\"me\",\"mi\",\"mis\",\"mucho\",\"muy\",\"nada\",\"ni\",\"no\",\"nos\",\"nosotros\",\"o\",\"os\",\"otra\",\"otras\",\n",
    " \"otro\",\"otros\",\"para\",\"pero\",\"poco\",\"por\",\"porque\",\"que\",\"quien\",\"quienes\",\"se\",\"sea\",\"segun\",\"ser\",\"si\",\"sin\",\"sobre\",\n",
    " \"sois\",\"solamente\",\"solo\",\"somos\",\"son\",\"soy\",\"su\",\"sus\",\"tal\",\"tambien\",\"tanto\",\"te\",\"tenia\",\"tendra\",\"teneis\",\"tenemos\",\n",
    " \"tienen\",\"toda\",\"todas\",\"todo\",\"todos\",\"tu\",\"tus\",\"un\",\"una\",\"uno\",\"unos\",\"vosotros\",\"y\",\"ya\",\"q\",\"xq\",\"pq\"\n",
    "}\n",
    "stop_en = {\n",
    " \"a\",\"an\",\"the\",\"and\",\"or\",\"but\",\"to\",\"of\",\"in\",\"on\",\"for\",\"with\",\"as\",\"by\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\n",
    " \"at\",\"from\",\"that\",\"this\",\"these\",\"those\",\"it\",\"its\",\"i\",\"you\",\"he\",\"she\",\"they\",\"we\",\"me\",\"him\",\"her\",\"them\",\"us\",\n",
    " \"my\",\"your\",\"his\",\"her\",\"their\",\"our\",\"not\",\"no\",\"do\",\"does\",\"did\",\"have\",\"has\",\"had\",\"so\",\"if\",\"just\",\"about\",\"into\",\n",
    " \"over\",\"than\",\"then\",\"there\",\"here\",\"out\",\"up\",\"down\",\"yes\",\"very\",\"also\"\n",
    "}\n",
    "stop_pt = {\"de\",\"da\",\"do\",\"das\",\"dos\",\"em\",\"no\",\"na\",\"nos\",\"nas\",\"e\",\"ou\",\"mas\",\"para\",\"por\",\"com\",\"sem\",\n",
    "           \"um\",\"uma\",\"uns\",\"umas\",\"se\",\"que\",\"como\",\"muito\",\"muita\",\"muitos\",\"muitas\",\"j√°\",\"tamb√©m\"}\n",
    "STOP_ADD = {\n",
    " \"m√°s\",\"esta\",\"est√°\",\"est√°n\",\"estar\",\"estoy\",\"estaba\",\"estaban\",\n",
    " \"va\",\"van\",\"ver\",\"vez\",\"as√≠\",\"tan\",\"cada\",\"mejor\",\"dice\",\"decir\",\n",
    " \"hacer\",\"puede\",\"tiene\",\"siempre\",\"ahora\",\"bien\",\"yo\",\"tu\",\"usted\",\n",
    " \"a√±os\",\"gente\",\"si\",\"qu√©\"\n",
    "}\n",
    "def _pick_stop(lang: str):\n",
    "    lang = (lang or \"es\").lower()\n",
    "    if   lang.startswith(\"en\"): base = stop_en\n",
    "    elif lang.startswith(\"pt\"): base = stop_pt\n",
    "    else:                       base = stop_es\n",
    "    return base | STOP_ADD\n",
    "\n",
    "# --- Normalizaci√≥n de alias\n",
    "ALIAS = {\n",
    "    \"usa\":\"eeuu\",\"estados_unidos\":\"eeuu\",\n",
    "    \"nato\":\"otan\",\n",
    "    \"union_europea\":\"ue\",\n",
    "    \"zelenski\":\"zelensky\",\"zelenskyy\":\"zelensky\",\"zelenskyi\":\"zelensky\",\n",
    "    \"ucranianos\":\"ucraniano\",\"ucranianas\":\"ucraniano\",\n",
    "    \"rusos\":\"ruso\",\"rusa\":\"ruso\",\"rusas\":\"ruso\",\n",
    "    \"israel√≠es\":\"israel\",\"estadounidenses\":\"eeuu\"\n",
    "}\n",
    "def normalize_term(t: str) -> str:\n",
    "    t = str(t).strip().lower().replace(\" \", \"_\")\n",
    "    return ALIAS.get(t, t)\n",
    "\n",
    "# --- Filtros de ruido (risas/muletillas)\n",
    "LAUGH_RX   = re.compile(r'^(?:j[aeiou]|aj|ha|he|hi|ho){3,}$', re.I)   # ej: jajaja, jejeje, ajajaja, hahaha\n",
    "XD_RX      = re.compile(r'^(?:x+d+|d+x+)$', re.I)                     # xd, xdd, dxx\n",
    "NOISE_BASE = {\"jaja\",\"jajaja\",\"jajajaja\",\"jeje\",\"jejeje\",\"ajaj\",\"ajajaja\",\"xd\",\"lol\",\"lmao\",\"bla\",\"blabla\",\"bla_bla\"}\n",
    "\n",
    "def is_noise_token(t: str) -> bool:\n",
    "    t = t.lower()\n",
    "    return (t in NOISE_BASE) or bool(LAUGH_RX.match(t)) or bool(XD_RX.match(t))\n",
    "\n",
    "# --- Tokenizaci√≥n\n",
    "tok_rx = re.compile(r\"[^\\w√°√©√≠√≥√∫√º√±]+\", re.I)\n",
    "\n",
    "# cache stopwords\n",
    "_stop_cache = {}\n",
    "def get_stop(lang):\n",
    "    if lang not in _stop_cache:\n",
    "        _stop_cache[lang] = _pick_stop(lang)\n",
    "    return _stop_cache[lang]\n",
    "\n",
    "def tokenize(text: str, lang: str):\n",
    "    st = get_stop(lang)\n",
    "    toks = []\n",
    "    for t in tok_rx.split(text.lower()):\n",
    "        if not t or len(t) < 3 or t.isdigit(): \n",
    "            continue\n",
    "        if t in st: \n",
    "            continue\n",
    "        if is_noise_token(t):\n",
    "            continue\n",
    "        toks.append(normalize_term(t))\n",
    "    return toks\n",
    "\n",
    "def to_bigrams(tokens):\n",
    "    # bigrama con ESPACIO, y filtramos bigrams ruidosos (ambos lados ruido o frase tipo \"bla bla\")\n",
    "    bigs = []\n",
    "    for a, b in zip(tokens, tokens[1:]):\n",
    "        if is_noise_token(a) and is_noise_token(b):\n",
    "            continue\n",
    "        term = f\"{a.replace('_',' ')} {b.replace('_',' ')}\"\n",
    "        if term.strip() in {\"bla bla\"}:\n",
    "            continue\n",
    "        bigs.append(term)\n",
    "    return bigs\n",
    "\n",
    "# ===========================\n",
    "# STREAMING COUNT (una pasada)\n",
    "# ===========================\n",
    "uni_label_cnt = defaultdict(Counter)\n",
    "bi_label_cnt  = defaultdict(Counter)\n",
    "uni_chan_cnt  = defaultdict(Counter)\n",
    "bi_chan_cnt   = defaultdict(Counter)\n",
    "docs_label = Counter()\n",
    "docs_chan  = Counter()\n",
    "\n",
    "for text, lang, lbl, ch in zip(TXT, LANGS, LABELS, CHANNEL):\n",
    "    toks = tokenize(text, lang)\n",
    "    bigs = to_bigrams(toks) if toks else []\n",
    "    uni_label_cnt[lbl].update(toks)\n",
    "    bi_label_cnt[lbl].update(bigs)\n",
    "    docs_label[lbl] += 1\n",
    "    uni_chan_cnt[ch].update(toks)\n",
    "    bi_chan_cnt[ch].update(bigs)\n",
    "    docs_chan[ch]  += 1\n",
    "\n",
    "def counters_to_df(cmap, scope_type: str, docs_counter: Counter):\n",
    "    rows = []\n",
    "    for scope_val, cnt in cmap.items():\n",
    "        total = sum(cnt.values())\n",
    "        if total == 0:\n",
    "            continue\n",
    "        k = 0\n",
    "        n_docs = int(docs_counter[scope_val])\n",
    "        for term, n in cnt.most_common():\n",
    "            k += 1\n",
    "            rows.append((scope_type, scope_val, term, n,\n",
    "                         round(100.0*n/total, 4), k, n_docs))\n",
    "    return pd.DataFrame(rows, columns=[\"scope_type\",\"scope_value\",\"term\",\"n\",\"pct\",\"rank\",\"n_docs\"])\n",
    "\n",
    "uni_label_df = counters_to_df(uni_label_cnt, \"label\",   docs_label)\n",
    "bi_label_df  = counters_to_df(bi_label_cnt,  \"label\",   docs_label)\n",
    "uni_chan_df  = counters_to_df(uni_chan_cnt,  \"channel\", docs_chan)\n",
    "bi_chan_df   = counters_to_df(bi_chan_cnt,   \"channel\", docs_chan)\n",
    "\n",
    "pbi_unigrams = pd.concat([uni_label_df, uni_chan_df], ignore_index=True).sort_values([\"scope_type\",\"scope_value\",\"rank\"])\n",
    "pbi_bigrams  = pd.concat([bi_label_df,  bi_chan_df],  ignore_index=True).sort_values([\"scope_type\",\"scope_value\",\"rank\"])\n",
    "\n",
    "pbi_unigrams.to_csv(UNI_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "pbi_bigrams.to_csv(BI_PATH,  index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"Guardado:\\n - {UNI_PATH}\\n - {BI_PATH}\")\n",
    "print(\"Ejemplo UNIGRAMS:\", pbi_unigrams.head(10).to_string(index=False))\n",
    "print(\"Ejemplo BIGRAMS :\", pbi_bigrams.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85d5909",
   "metadata": {},
   "source": [
    "# Terminos espec√≠ficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d0ddec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna df['term_especial'] creada/actualizada (un √∫nico t√©rmino por prioridad).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TXT = \"comment_clean\" if \"comment_clean\" in df.columns else \"comment\"\n",
    "\n",
    "# Lista ordenada por prioridad (Wagner, Kursk, Prigozhin arriba)\n",
    "TERMS_ORDERED = [\n",
    "    (\"Wagner\",         [r\"\\bwagner\\b\"]),\n",
    "    (\"Kursk\",          [r\"\\bkursk\\b\"]),\n",
    "    (\"Prigozhin\",      [r\"\\bprigozhin\\b|prigogin|prigojin|prigogine\"]),\n",
    "    (\"Bucha\",          [r\"\\bbucha\\b\"]),\n",
    "    (\"Azovstal\",       [r\"\\bazovstal\\b\"]),\n",
    "    (\"Bakhmut\",        [r\"\\bbakhmut\\b|bah?mut\"]),\n",
    "    (\"Avdiivka\",       [r\"\\bavdi(iv)?ka\\b|avdi(iv)?ka\"]),\n",
    "    (\"Kakhovka\",       [r\"\\bkakhovka\\b|cajovka|kajovka|kak?hovka\"]),\n",
    "    (\"Kerch_Bridge\",   [r\"\\b(kerch|crime[ao]\\s*bridge|puente\\s+de\\s+crimea|puente\\s+de\\s+kerch)\\b\", r\"\\bkerc[hx]\\b\"]),\n",
    "    (\"HIMARS\",         [r\"\\bhimars?\\b\"]),\n",
    "    (\"ATACMS\",         [r\"\\batacms?\\b\"]),\n",
    "    (\"Leopard\",        [r\"\\bleopard\\b\"]),\n",
    "    (\"Abrams\",         [r\"\\babrams\\b\"]),\n",
    "    (\"Patriot\",        [r\"\\bpatriot\\b\"]),\n",
    "    (\"StormShadow\",    [r\"\\bstorm(?:\\s|-)?shadow\\b\"]),\n",
    "    (\"Taurus\",         [r\"\\btaurus\\b\"]),\n",
    "    (\"Shahed_Geran\",   [r\"\\bshaheds?\\b|\\bgeran-?\\d*\\b\"]),\n",
    "    (\"Kinzhal\",        [r\"\\bkinzhal\\b\"]),\n",
    "    (\"Kalibr\",         [r\"\\bkalibr\\b|\\bcalibre\\b\"]),\n",
    "    (\"Iskander\",       [r\"\\biskander\\b\"]),\n",
    "    (\"Zaporizhzhia\",   [r\"\\bzapo?riz(h|j)(zh|z)ia\\b|\\bzapo?ro?ri?a\\b\"]),\n",
    "    (\"Kherson\",        [r\"\\bkherson\\b|jerson|gerson\"]),\n",
    "    (\"Kharkiv\",        [r\"\\bkharkiv\\b|jarkov|kharkov|jarkiv\"]),\n",
    "    (\"Donbas\",         [r\"\\bdonb(a|√°)s\\b|donbass\"]),\n",
    "    (\"Lyman\",          [r\"\\blyman\\b\"]),\n",
    "    (\"Severodonetsk\",  [r\"\\bseverodonets?k\\b\"]),\n",
    "    (\"Lysychansk\",     [r\"\\blysychansk\\b|lisichansk\"]),\n",
    "    (\"Kupiansk\",       [r\"\\bkupians?k\\b\"]),\n",
    "    (\"Pokrovsk\",       [r\"\\bpokrovsk\\b\"]),\n",
    "    (\"Mariupol\",       [r\"\\bmariupol\\b|mari[u√∫]pol\"]),\n",
    "    (\"Crimea\",         [r\"\\bcrimea\\b|crim[e√©]a|krimea\"]),\n",
    "    (\"Belgorod\",       [r\"\\bbelgorod\\b|b[e√©]lgorod\"]),\n",
    "    (\"AZOV\",           [r\"\\bazov\\b\"]),\n",
    "    (\"NAFO\",           [r\"\\bnafo\\b\"]),\n",
    "    (\"ucranazi\",       [r\"\\bucranazis?\\b\"]),\n",
    "]\n",
    "\n",
    "# Compilaci√≥n de patrones\n",
    "TERMS_COMPILED = [(label, re.compile(\"|\".join(pats), re.I)) for label, pats in TERMS_ORDERED]\n",
    "\n",
    "def _first_special_term(text: str):\n",
    "    s = str(text)\n",
    "    for label, rx in TERMS_COMPILED:\n",
    "        if rx.search(s):\n",
    "            return label\n",
    "    return np.nan\n",
    "\n",
    "df[\"term_especial\"] = df[TXT].apply(_first_special_term)\n",
    "\n",
    "print(\"Columna df['term_especial'] creada/actualizada (un √∫nico t√©rmino por prioridad).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c3f2ef",
   "metadata": {},
   "source": [
    "# Relevant events 2024 assignation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebe7f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eventos 2024 asignados por 'merge_asof' (√∫ltimo evento ‚â§ fecha del video) con claves tz-aware.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# FIX: merge_asof con ambas claves tz-aware (datetime64[ns, UTC])\n",
    "# Asigna √∫ltimo evento 2024 <= fecha del video (mismo d√≠a permitido)\n",
    "# Crea/actualiza: evento, evento_fecha, relacion_evento, sub_tipo_evento, tipo_evento\n",
    "# ==========================================================\n",
    "\n",
    "\n",
    "# 1) Cat√°logo 2024 en el orden provisto (SIN .values para no perder tz)\n",
    "events_2024_ordered = [\n",
    "    (\"Bombardeo en mercado de Donetsk (civiles muertos)\",   \"2024-01-21\", \"militar\",     \"simbolico\",   \"pro-ruso\"),\n",
    "    (\"Muerte de Alexei Navalny en prisi√≥n\",                 \"2024-02-16\", \"politico\",    \"simbolico\",   \"pro-ucraniano\"),\n",
    "    (\"Ca√≠da de Avdiivka\",                                   \"2024-02-18\", \"militar\",     \"simbolico\",   \"pro-ruso\"),\n",
    "    (\"Paquete de ayuda de EE. UU. (61 mil millones USD)\",   \"2024-04-20\", \"geopolitico\", \"estrategico\", \"pro-ucraniano\"),\n",
    "    (\"Ofensiva rusa en el √≥blast de J√°rkov\",                \"2024-05-10\", \"militar\",     \"estrategico\", \"pro-ruso\"),\n",
    "    (\"Ataque ruso a hospital infantil en Kiev (Okhmatdyt)\", \"2024-07-08\", \"militar\",     \"simbolico\",   \"pro-ucraniano\"),\n",
    "    (\"Incursi√≥n ucraniana en regi√≥n rusa de Kursk\",         \"2024-08-06\", \"militar\",     \"estrategico\", \"pro-ucraniano\"),\n",
    "    (\"Toma de Vuhledar (Donetsk)\",                          \"2024-10-02\", \"militar\",     \"estrategico\", \"pro-ruso\"),\n",
    "    (\"Mes de mayores avances rusos (~200 km¬≤)\",             \"2024-10-25\", \"militar\",     \"estrategico\", \"pro-ruso\"),\n",
    "    (\"Ataque masivo con drones ucranianos sobre Mosc√∫\",     \"2024-11-10\", \"militar\",     \"simbolico\",   \"pro-ucraniano\"),\n",
    "    (\"Ca√≠da del r√©gimen sirio (Assad huye a Mosc√∫)\",        \"2024-12-08\", \"geopolitico\", \"simbolico\",   \"pro-ucraniano\"),\n",
    "]\n",
    "ev = pd.DataFrame(events_2024_ordered,\n",
    "                  columns=[\"evento\",\"evento_fecha\",\"relacion_evento\",\"sub_tipo_evento\",\"tipo_evento\"])\n",
    "ev[\"evento_fecha\"] = pd.to_datetime(ev[\"evento_fecha\"], utc=True)\n",
    "ev = ev.sort_values(\"evento_fecha\").reset_index(drop=True)  # conserva tz\n",
    "\n",
    "# 2) Fecha de video (dataset ya filtrado a 2024 y no nulo)\n",
    "df[\"_video_dt\"] = pd.to_datetime(df[\"video_published_at\"], errors=\"raise\", utc=True)\n",
    "\n",
    "# 3) Asignaci√≥n vectorizada: √∫ltimo evento <= fecha del video\n",
    "df_sorted = df.sort_values(\"_video_dt\")\n",
    "asof = pd.merge_asof(\n",
    "    df_sorted[[\"_video_dt\"]],\n",
    "    ev, left_on=\"_video_dt\", right_on=\"evento_fecha\",\n",
    "    direction=\"backward\", allow_exact_matches=True\n",
    ")\n",
    "\n",
    "# 4) Volcar columnas al master (alineando por √≠ndice tras ordenar)\n",
    "df.loc[df_sorted.index, \"evento\"]           = asof[\"evento\"].values\n",
    "df.loc[df_sorted.index, \"evento_fecha\"]     = asof[\"evento_fecha\"].values\n",
    "df.loc[df_sorted.index, \"relacion_evento\"]  = asof[\"relacion_evento\"].values\n",
    "df.loc[df_sorted.index, \"sub_tipo_evento\"]  = asof[\"sub_tipo_evento\"].values\n",
    "df.loc[df_sorted.index, \"tipo_evento\"]      = asof[\"tipo_evento\"].values\n",
    "\n",
    "# 5) Limpieza auxiliar\n",
    "df.drop(columns=[\"_video_dt\"], inplace=True)\n",
    "\n",
    "print(\"Eventos 2024 asignados por 'merge_asof' (√∫ltimo evento ‚â§ fecha del video) con claves tz-aware.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5857ea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar dataset limpio en la carpeta del proyecto\n",
    "output_path = \"../data/processed/8_final_master_enriched.csv\"\n",
    "df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d6cdf5",
   "metadata": {},
   "source": [
    "# BI Tool Star Tables Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2018b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GOLD / BI LAYER para Power BI ‚Äî Star Schema desde df actual (fix video_key)\n",
    "#   Salida: ../data/bi_layer/*.csv (UTF-8 BOM)\n",
    "#   Grano del fact: comment_id\n",
    "#   FKs: user_id, channel_id, video_key, evento\n",
    "#   Mejora: dim_date continua (TZ local) + comment_date para relaci√≥n\n",
    "# ============================================================\n",
    "\n",
    "# Config m√≠nima\n",
    "LOCAL_TZ    = \"Europe/Madrid\"  # eje temporal del reporte\n",
    "BUFFER_DAYS = 7                # margen para rolling/edges\n",
    "\n",
    "OUT_DIR = Path(\"../data/bi_layer\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _hash_series(parts_df: pd.DataFrame) -> pd.Series:\n",
    "    # Vectorizado: concatena como string estable y aplica md5\n",
    "    s = parts_df.astype(str).agg(\"|\".join, axis=1)\n",
    "    return s.map(lambda x: hashlib.md5(x.encode(\"utf-8\")).hexdigest())\n",
    "\n",
    "def _to_local_floor_day(utc_ts: pd.Series, tz: str) -> pd.Series:\n",
    "    # Convierte UTC -> tz local, baja a d√≠a y quita tz (naive date)\n",
    "    return (\n",
    "        pd.to_datetime(utc_ts, utc=True, errors=\"coerce\")\n",
    "          .dt.tz_convert(tz)\n",
    "          .dt.floor(\"D\")\n",
    "          .dt.tz_localize(None)\n",
    "    )\n",
    "\n",
    "# --- Normalizaciones m√≠nimas de tiempo/llaves (sin cambiar l√≥gicas previas)\n",
    "df[\"comment_time\"]       = pd.to_datetime(df[\"comment_time\"], errors=\"coerce\", utc=True)\n",
    "df[\"video_published_at\"] = pd.to_datetime(df[\"video_published_at\"], errors=\"coerce\", utc=True)\n",
    "if pd.api.types.is_datetime64_any_dtype(df[\"evento_fecha\"]) and getattr(df[\"evento_fecha\"].dt, \"tz\", None) is None:\n",
    "    df[\"evento_fecha\"] = pd.to_datetime(df[\"evento_fecha\"], errors=\"coerce\").dt.tz_localize(\"UTC\")\n",
    "\n",
    "# --- video_key auxiliar (vectorizado y estable)\n",
    "df[\"_video_key\"] = _hash_series(df[[\"channel_id\",\"video_title\",\"video_published_at\"]])\n",
    "\n",
    "# --- Agregados por usuario que faltaban en el master\n",
    "user_agg = (\n",
    "    df.groupby(\"user_id\", as_index=False)\n",
    "      .agg(n_channels_user=(\"channel_id\",\"nunique\"),\n",
    "           n_videos_user=(\"_video_key\",\"nunique\"))\n",
    ")\n",
    "\n",
    "# ======================\n",
    "# DIM USER\n",
    "# ======================\n",
    "base_user_cols = [\n",
    "    \"user_id\",\"user_name\",\"user_region\",\"user_country\",\n",
    "    \"user_segment\",\"user_rank\",\"bot_flag\",\"bot_score\",\n",
    "    \"user_n_comments\",\"user_days_active\",\"user_freq_diaria\"\n",
    "]\n",
    "dim_user = (\n",
    "    df.sort_values([\"user_id\",\"comment_time\"])\n",
    "      .drop_duplicates(\"user_id\")[base_user_cols]\n",
    "      .merge(user_agg, on=\"user_id\", how=\"left\", validate=\"1:1\")\n",
    "      .rename(columns={\"n_channels_user\":\"n_channels\",\n",
    "                       \"n_videos_user\":\"n_videos\"})\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ======================\n",
    "# DIM CHANNEL\n",
    "# ======================\n",
    "dim_channel = (\n",
    "    df.sort_values([\"channel_id\",\"comment_time\"])\n",
    "      .drop_duplicates(\"channel_id\")[[\"channel_id\",\"channel_title\",\"subscriber_count\",\"condiciones_cuenta\"]]\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ======================\n",
    "# DIM VIDEO\n",
    "# ======================\n",
    "video_base = (\n",
    "    df[[\"channel_id\",\"video_title\",\"video_published_at\",\"video_views\",\"video_likes\",\"video_duration\",\"video_category_id\",\"video_tags\",\"_video_key\"]]\n",
    "      .drop_duplicates(\"_video_key\")\n",
    "      .copy()\n",
    "      .rename(columns={\"_video_key\":\"video_key\"})\n",
    ")\n",
    "dim_video = video_base[\n",
    "    [\"video_key\",\"channel_id\",\"video_title\",\"video_published_at\",\"video_views\",\"video_likes\",\"video_duration\",\"video_category_id\",\"video_tags\"]\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# ======================\n",
    "# DIM EVENT\n",
    "# ======================\n",
    "dim_event = (\n",
    "    df[[\"evento\",\"evento_fecha\",\"relacion_evento\",\"sub_tipo_evento\",\"tipo_evento\"]]\n",
    "      .dropna(subset=[\"evento\"])\n",
    "      .drop_duplicates()\n",
    "      .sort_values(\"evento_fecha\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ======================\n",
    "# DIM DATE (continua, TZ local + buffer)\n",
    "# ======================\n",
    "# Fechas en TZ local para evitar desfasajes por medianoche\n",
    "_comment_local = df[\"comment_time\"].dt.tz_convert(LOCAL_TZ)\n",
    "_video_local   = df[\"video_published_at\"].dt.tz_convert(LOCAL_TZ)\n",
    "_evento_local  = df[\"evento_fecha\"].dt.tz_convert(LOCAL_TZ) if \"evento_fecha\" in df.columns else pd.Series([], dtype=\"datetime64[ns, UTC]\")\n",
    "\n",
    "fechas_all = pd.concat([_comment_local, _video_local, _evento_local], ignore_index=True).dropna()\n",
    "if fechas_all.empty:\n",
    "    min_dt_local = pd.Timestamp(\"2024-01-01\").normalize()\n",
    "    max_dt_local = pd.Timestamp.today(tz=LOCAL_TZ).normalize().tz_localize(None)\n",
    "else:\n",
    "    min_dt_local = fechas_all.min().normalize().tz_localize(None)\n",
    "    max_dt_local = fechas_all.max().normalize().tz_localize(None)\n",
    "\n",
    "# Buffer para rolling\n",
    "min_dt_local = min_dt_local - pd.Timedelta(days=BUFFER_DAYS)\n",
    "max_dt_local = max_dt_local + pd.Timedelta(days=BUFFER_DAYS)\n",
    "\n",
    "# Rango continuo diario (naive date)\n",
    "dates = pd.date_range(\n",
    "    start=min_dt_local.tz_localize(LOCAL_TZ),\n",
    "    end=max_dt_local.tz_localize(LOCAL_TZ),\n",
    "    freq=\"D\",\n",
    "    name=\"date\"\n",
    ").tz_convert(LOCAL_TZ).tz_localize(None)\n",
    "\n",
    "dim_date = pd.DataFrame({\"date\": dates})\n",
    "dim_date[\"year\"]        = dim_date[\"date\"].dt.year\n",
    "dim_date[\"month\"]       = dim_date[\"date\"].dt.month\n",
    "dim_date[\"day\"]         = dim_date[\"date\"].dt.day\n",
    "dim_date[\"quarter\"]     = dim_date[\"date\"].dt.quarter\n",
    "dim_date[\"ym\"]          = dim_date[\"date\"].dt.strftime(\"%Y-%m\")\n",
    "dim_date[\"dow\"]         = dim_date[\"date\"].dt.dayofweek              # 0=Lun\n",
    "iso = dim_date[\"date\"].astype(\"datetime64[ns]\").dt.isocalendar()\n",
    "dim_date[\"week_iso\"]    = iso.week.astype(int)\n",
    "dim_date[\"year_iso\"]    = iso.year.astype(int)\n",
    "dim_date[\"ym_sort\"]     = dim_date[\"date\"].dt.strftime(\"%Y%m\").astype(int)\n",
    "dim_date[\"date_id\"]     = dim_date[\"date\"].dt.strftime(\"%Y%m%d\").astype(int)\n",
    "dim_date[\"month_name\"]  = dim_date[\"date\"].dt.strftime(\"%b\")         # Ene, Feb...\n",
    "dim_date[\"dow_name\"]    = dim_date[\"date\"].dt.strftime(\"%a\")         # Lun, Mar...\n",
    "dim_date[\"is_month_start\"] = dim_date[\"date\"].dt.is_month_start\n",
    "dim_date[\"is_month_end\"]   = dim_date[\"date\"].dt.is_month_end\n",
    "dim_date[\"is_weekend\"]     = dim_date[\"dow\"].isin([5,6])\n",
    "\n",
    "# ======================\n",
    "# FACT COMMENTS (grano = comment_id)\n",
    "# ======================\n",
    "fact_cols = [\n",
    "    \"comment_id\",\n",
    "    # FKs\n",
    "    \"user_id\",\"channel_id\",\"_video_key\",\"evento\",\n",
    "    # time / measures\n",
    "    \"comment_time\",\"comment_likes\",\"total_reply_count\",\"is_top_level_comment\",\n",
    "    # labels / text flags\n",
    "    \"label_final\",\"insulto\",\"n_insultos\",\"lang\",\"term_especial\",\"eje_argumentativo\",\n",
    "    # trazas √∫tiles\n",
    "    \"video_title\",\"video_published_at\",\"channel_title\",\"condiciones_cuenta\",\n",
    "    # user status\n",
    "    \"user_segment\",\"bot_flag\",\"bot_score\"\n",
    "]\n",
    "fact_comments = df[fact_cols].rename(columns={\"_video_key\":\"video_key\"}).reset_index(drop=True)\n",
    "\n",
    "# üîó Clave de relaci√≥n con dim_date: d√≠a local (naive) derivado de comment_time\n",
    "fact_comments[\"comment_date\"] = _to_local_floor_day(fact_comments[\"comment_time\"], LOCAL_TZ)\n",
    "\n",
    "# ======================\n",
    "# Escritura CSVs\n",
    "# ======================\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "dim_user.to_csv(OUT_DIR/\"dim_user.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "dim_channel.to_csv(OUT_DIR/\"dim_channel.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "dim_video.to_csv(OUT_DIR/\"dim_video.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "dim_event.to_csv(OUT_DIR/\"dim_event.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "dim_date.to_csv(OUT_DIR/\"dim_date.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "fact_comments.to_csv(OUT_DIR/\"fact_comments.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Copia opcional de n-gramas si existen\n",
    "proc_dir = Path(\"../data/processed\")\n",
    "for name in [\"pbi_unigrams.csv\",\"pbi_bigrams.csv\"]:\n",
    "    src = proc_dir/name\n",
    "    if src.exists():\n",
    "        pd.read_csv(src).to_csv(OUT_DIR/name, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Limpieza auxiliar del master\n",
    "df.drop(columns=[\"_video_key\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "# Resumen\n",
    "print(\"=== BI layer escrita en:\", OUT_DIR.resolve(), \"===\")\n",
    "for name in [\"dim_user\",\"dim_channel\",\"dim_video\",\"dim_event\",\"dim_date\",\"fact_comments\",\"pbi_unigrams\",\"pbi_bigrams\"]:\n",
    "    p = OUT_DIR/f\"{name}.csv\"\n",
    "    if p.exists():\n",
    "        try:\n",
    "            n = sum(1 for _ in open(p, \"r\", encoding=\"utf-8-sig\")) - 1\n",
    "            print(f\"  - {name}.csv: ~{n} filas\")\n",
    "        except Exception:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1076bffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14848\\3859188844.py:62: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  fact_comments_aux[c] = pd.to_numeric(fact_comments_aux[c], errors=\"ignore\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - fact_comments_aux.csv: 113583 filas\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# FACT COMMENTS AUX ‚Äî solo NUEVAS columnas + comentario\n",
    "# ======================\n",
    "\n",
    "# Columnas del fact (no repetir)\n",
    "fact_cols = [\n",
    "    \"comment_id\",\"user_id\",\"channel_id\",\"_video_key\",\"evento\",\n",
    "    \"comment_time\",\"comment_likes\",\"total_reply_count\",\"is_top_level_comment\",\n",
    "    \"label_final\",\"insulto\",\"n_insultos\",\"lang\",\"term_especial\",\"eje_argumentativo\",\n",
    "    \"video_title\",\"video_published_at\",\"channel_title\",\"condiciones_cuenta\",\n",
    "    \"user_segment\",\"bot_flag\",\"bot_score\",\"comment_date\"\n",
    "]\n",
    "\n",
    "# 1) Lista expl√≠cita \n",
    "#    (comentario y outputs del modelo / features √∫tiles)\n",
    "wanted_aux = [\n",
    "    \"comment\",                 # <-- texto original (s√≠ incluir)\n",
    "    \"label_rule\",\"regla_aplicada\",\n",
    "    \"label_ml\",\"ml_proba_max\",\"ml_margen\",\"ml_entropia\",\n",
    "    \"clasificacion_origen\",\n",
    "    \"user_rank\",\"user_n_comments\",\"user_days_active\",\"user_freq_diaria\",\n",
    "    \"user_country\",\"user_region\",\n",
    "]\n",
    "\n",
    "# 2) Exclusiones estrictas\n",
    "EXCLUDE = {\n",
    "    \"comment_clean\", \"text_with_ctx\",  # no incluir\n",
    "    \"bot_flag\", \"bot_score\",           # ya est√°n en fact\n",
    "}\n",
    "\n",
    "# 3) Armar columnas finales:\n",
    "#    - tomar las wanted que EXISTAN en df y NO est√©n en fact ni en EXCLUDE\n",
    "aux_cols = [c for c in wanted_aux if c in df.columns and c not in fact_cols and c not in EXCLUDE]\n",
    "\n",
    "# (Opcional) auto-incluir cualquier columna NUEVA futura que no choque con fact ni EXCLUDE\n",
    "AUTO_INCLUDE_NEW = True\n",
    "if AUTO_INCLUDE_NEW:\n",
    "    extras = [\n",
    "        c for c in df.columns\n",
    "        if c not in set(fact_cols) | EXCLUDE | {\"_video_key\"}   # evita t√©cnicas\n",
    "           and c not in aux_cols\n",
    "           and c != \"comment_id\"\n",
    "    ]\n",
    "    aux_cols += extras\n",
    "\n",
    "# Asegurar siempre comment_id\n",
    "base_cols = [\"comment_id\"]\n",
    "\n",
    "# Construcci√≥n del AUX\n",
    "fact_comments_aux = (\n",
    "    df[base_cols + [c for c in aux_cols if c in df.columns]]\n",
    "      .drop_duplicates(subset=[\"comment_id\"], keep=\"last\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Tipado: IDs a string; m√©tricas a num√©rico cuando aplique\n",
    "fact_comments_aux[\"comment_id\"] = fact_comments_aux[\"comment_id\"].astype(\"string\")\n",
    "for c in fact_comments_aux.columns:\n",
    "    if c == \"comment_id\":\n",
    "        continue\n",
    "    if c.startswith((\"ml_\", \"proba_\", \"n_\", \"user_\", \"comment_text_length\")):\n",
    "        fact_comments_aux[c] = pd.to_numeric(fact_comments_aux[c], errors=\"ignore\")\n",
    "    elif fact_comments_aux[c].dtype == \"object\":\n",
    "        fact_comments_aux[c] = fact_comments_aux[c].astype(\"string\")\n",
    "\n",
    "# Validaci√≥n de cobertura vs fact\n",
    "try:\n",
    "    fact_ids = set(fact_comments[\"comment_id\"].astype(\"string\").dropna().unique())\n",
    "    aux_ids  = set(fact_comments_aux[\"comment_id\"].astype(\"string\").dropna().unique())\n",
    "    hu√©rfanos = aux_ids - fact_ids\n",
    "    if hu√©rfanos:\n",
    "        print(f\"{len(hu√©rfanos)} comment_id en AUX no existen en fact_comments (no matchear√°n).\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Escritura\n",
    "(OUT_DIR / \"fact_comments_aux.csv\").parent.mkdir(parents=True, exist_ok=True)\n",
    "fact_comments_aux.to_csv(OUT_DIR/\"fact_comments_aux.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"  - fact_comments_aux.csv:\", len(fact_comments_aux), \"filas\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
