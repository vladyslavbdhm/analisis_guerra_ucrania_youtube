{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e707253a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Desktop\\Facu\\Master_Espana\\Master_UEMC\\TFM\\codigo\\analisis_guerra_ucrania_youtube\\.venv\\Lib\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Desktop\\Facu\\Master_Espana\\Master_UEMC\\TFM\\codigo\\analisis_guerra_ucrania_youtube\\.venv\\Lib\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Desktop\\Facu\\Master_Espana\\Master_UEMC\\TFM\\codigo\\analisis_guerra_ucrania_youtube\\.venv\\Lib\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      neutro      0.928     0.873     0.899      5321\n",
      "        ruso      0.968     0.983     0.976     36222\n",
      "   ucraniano      0.911     0.856     0.883      4081\n",
      "\n",
      "    accuracy                          0.959     45624\n",
      "   macro avg      0.936     0.904     0.919     45624\n",
      "weighted avg      0.958     0.959     0.959     45624\n",
      "\n",
      "✅ Guardado baseline en ../models/baselines/stance_tfidf_svc_calibrated.joblib\n"
     ]
    }
   ],
   "source": [
    "# === BASELINE RÁPIDO: TF-IDF + LinearSVC (+ features) ===\n",
    "import pandas as pd, numpy as np, re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1) Cargar 9k etiquetados (usa tu archivo consolidado)\n",
    "df9 = pd.read_excel(\"../data/processed/comentarios_9000_con_insultos.xlsx\")\n",
    "# si ya tienes un consolidado con label_final humano, usa ese mejor\n",
    "if \"label_final\" not in df9.columns:\n",
    "    df9[\"label_final\"] = df9[\"label_comentario\"]  # ajusta si corresponde\n",
    "\n",
    "# (opcional) añadir auto-etiquetados de alta confianza de tu corrida TF\n",
    "try:\n",
    "    auto = pd.read_csv(\"../data/processed/7_predicciones_full_new.csv\", usecols=[\n",
    "        \"comment\",\"condiciones_cuenta\",\"insulto\",\"n_insultos\",\"comment_text_length\",\n",
    "        \"comment_likes\",\"label_final\",\"ml_proba_max\",\"ml_margen\"\n",
    "    ])\n",
    "    auto_hc = auto[(auto[\"label_final\"].isin([\"ruso\",\"ucraniano\",\"neutro\"])) &\n",
    "                   (auto[\"ml_proba_max\"]>=0.80) & (auto[\"ml_margen\"]>=0.30)]\n",
    "    base = pd.concat([\n",
    "        df9[[\"comment\",\"condiciones_cuenta\",\"insulto\",\"n_insultos\",\"comment_text_length\",\"comment_likes\",\"label_final\"]],\n",
    "        auto_hc\n",
    "    ], ignore_index=True)\n",
    "except Exception:\n",
    "    base = df9[[\"comment\",\"condiciones_cuenta\",\"insulto\",\"n_insultos\",\"comment_text_length\",\"comment_likes\",\"label_final\"]]\n",
    "\n",
    "# Limpieza rápida\n",
    "base = base.dropna(subset=[\"comment\",\"condiciones_cuenta\",\"label_final\"]).copy()\n",
    "base[\"comment\"] = base[\"comment\"].astype(str)\n",
    "base[\"condiciones_cuenta\"] = base[\"condiciones_cuenta\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"http\\S+\",\" \",s)\n",
    "    s = re.sub(r\"@[A-Za-z0-9_]+\",\" \",s)\n",
    "    s = re.sub(r\"\\s+\",\" \",s).strip()\n",
    "    return s\n",
    "\n",
    "base[\"comment_clean\"] = base[\"comment\"].apply(clean_text)\n",
    "base[\"text_with_ctx\"] = \"[canal:\" + base[\"condiciones_cuenta\"] + \"] \" + base[\"comment_clean\"]\n",
    "\n",
    "# Asegurar columnas numéricas\n",
    "for col in [\"insulto\",\"n_insultos\",\"comment_text_length\",\"comment_likes\"]:\n",
    "    if col not in base.columns: base[col] = 0\n",
    "    base[col] = pd.to_numeric(base[col], errors=\"coerce\").fillna(0)\n",
    "\n",
    "X_text = base[\"text_with_ctx\"]\n",
    "y      = base[\"label_final\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "# 2) Partición\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    base, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 3) ColumnTransformer: texto + tabulares\n",
    "text_vect = TfidfVectorizer(\n",
    "    analyzer=\"word\", ngram_range=(1,2), min_df=2, max_features=300000,\n",
    "    # añadimos también char-ngrams vía un segundo vectorizador\n",
    ")\n",
    "char_vect = TfidfVectorizer(\n",
    "    analyzer=\"char\", ngram_range=(3,5), min_df=2, max_features=200000\n",
    ")\n",
    "\n",
    "# ColumnTransformer con 2 ramas de texto + ramas tabulares\n",
    "pre = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"w_tfidf\", text_vect, \"text_with_ctx\"),\n",
    "        (\"c_tfidf\", char_vect, \"text_with_ctx\"),\n",
    "        (\"ohe_canal\", OneHotEncoder(handle_unknown=\"ignore\"), [\"condiciones_cuenta\"]),\n",
    "        (\"num\", StandardScaler(with_mean=False), [\"insulto\",\"n_insultos\",\"comment_text_length\",\"comment_likes\"])\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=0.3\n",
    ")\n",
    "\n",
    "# 4) Clasificador: LinearSVC calibrado (para probabilidades y umbral)\n",
    "svc = LinearSVC(C=1.0, class_weight=\"balanced\")\n",
    "clf = CalibratedClassifierCV(estimator=svc, method=\"isotonic\", cv=3)  # <-- usa 'estimator'\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"pre\", pre),\n",
    "    (\"clf\", clf)\n",
    "])\n",
    "\n",
    "# 5) Entrenar\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# 6) Validación + reporte\n",
    "y_pred = pipe.predict(X_val)\n",
    "print(classification_report(y_val, y_pred, digits=3))\n",
    "\n",
    "# 7) Guardar modelo\n",
    "import joblib, os\n",
    "os.makedirs(\"../models/baselines\", exist_ok=True)\n",
    "joblib.dump(pipe, \"../models/baselines/stance_tfidf_svc_calibrated.joblib\")\n",
    "print(\"✅ Guardado baseline en ../models/baselines/stance_tfidf_svc_calibrated.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49fb5496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño base de entrenamiento: 1439\n",
      "Distribución de y: y\n",
      "ruso         899\n",
      "ucraniano    308\n",
      "neutro       232\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== EVAL HOLD-OUT (solo humano + pseudo-HC si aplicó) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      neutro      0.000     0.000     0.000        46\n",
      "        ruso      0.624     0.994     0.767       180\n",
      "   ucraniano      0.000     0.000     0.000        62\n",
      "\n",
      "    accuracy                          0.622       288\n",
      "   macro avg      0.208     0.331     0.256       288\n",
      "weighted avg      0.390     0.622     0.479       288\n",
      "\n",
      "\n",
      "=== EVAL (subset humano puro) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      neutro      0.000     0.000     0.000        46\n",
      "        ruso      0.624     0.994     0.767       180\n",
      "   ucraniano      0.000     0.000     0.000        62\n",
      "\n",
      "    accuracy                          0.622       288\n",
      "   macro avg      0.208     0.331     0.256       288\n",
      "weighted avg      0.390     0.622     0.479       288\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Desktop\\Facu\\Master_Espana\\Master_UEMC\\TFM\\codigo\\analisis_guerra_ucrania_youtube\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\User\\Desktop\\Facu\\Master_Espana\\Master_UEMC\\TFM\\codigo\\analisis_guerra_ucrania_youtube\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\User\\Desktop\\Facu\\Master_Espana\\Master_UEMC\\TFM\\codigo\\analisis_guerra_ucrania_youtube\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\User\\Desktop\\Facu\\Master_Espana\\Master_UEMC\\TFM\\codigo\\analisis_guerra_ucrania_youtube\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\User\\Desktop\\Facu\\Master_Espana\\Master_UEMC\\TFM\\codigo\\analisis_guerra_ucrania_youtube\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\User\\Desktop\\Facu\\Master_Espana\\Master_UEMC\\TFM\\codigo\\analisis_guerra_ucrania_youtube\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Guardado baseline en ../models/baselines/stance_tfidf_svc_calibrated.joblib\n",
      "Orden de clases del modelo: ['neutro', 'ruso', 'ucraniano']\n"
     ]
    }
   ],
   "source": [
    "# === BASELINE RÁPIDO (limpio): TF-IDF + LinearSVC calibrado (+ features) ===\n",
    "import pandas as pd, numpy as np, re, os, joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ---------------------------\n",
    "# CONFIG\n",
    "# ---------------------------\n",
    "PATH_9K = \"../data/processed/comentarios_9000_con_insultos.xlsx\"  # tu set humano\n",
    "# Usa SOLO SI es el archivo \"bueno\" (el de 110k con métricas altas). Si no, déjalo en None.\n",
    "GOOD_AUTO_PATH = \"/predicciones_full.csv\"\n",
    "AUTO_TH_P = 0.80       # umbral proba\n",
    "AUTO_TH_M = 0.30       # umbral margen\n",
    "OUT_MODEL = \"../models/baselines/stance_tfidf_svc_calibrated.joblib\"\n",
    "\n",
    "# ---------------------------\n",
    "# CARGA Y LIMPIEZA\n",
    "# ---------------------------\n",
    "df9 = pd.read_excel(PATH_9K)\n",
    "\n",
    "# Normaliza columna de etiqueta humana\n",
    "if \"label_final\" in df9.columns:\n",
    "    df9[\"y\"] = df9[\"label_final\"]\n",
    "else:\n",
    "    df9[\"y\"] = df9[\"label_comentario\"]\n",
    "\n",
    "# Normaliza nombres\n",
    "MAP_NORM = {\n",
    "    \"pro-ucraniano\": \"ucraniano\",\n",
    "    \"pro-ruso\": \"ruso\",\n",
    "    \"ucraniano\": \"ucraniano\",\n",
    "    \"ruso\": \"ruso\",\n",
    "    \"neutro\": \"neutro\",\n",
    "    \"neutral\": \"neutro\",\n",
    "    \"\": np.nan,\n",
    "    None: np.nan\n",
    "}\n",
    "df9[\"y\"] = df9[\"y\"].astype(str).str.strip().str.lower().map(MAP_NORM)\n",
    "\n",
    "# columnas básicas\n",
    "need_cols = [\"comment\",\"condiciones_cuenta\",\"insulto\",\"n_insultos\",\"comment_text_length\",\"comment_likes\",\"y\",\"comment_id\"]\n",
    "for c in need_cols:\n",
    "    if c not in df9.columns:\n",
    "        df9[c] = np.nan\n",
    "\n",
    "# Limpieza rápida de texto\n",
    "def clean_text(s: str) -> str:\n",
    "    s = str(s).lower()\n",
    "    s = re.sub(r\"http\\S+\",\" \",s)\n",
    "    s = re.sub(r\"@[A-Za-z0-9_]+\",\" \",s)\n",
    "    s = re.sub(r\"\\s+\",\" \",s).strip()\n",
    "    return s\n",
    "\n",
    "df9 = df9.dropna(subset=[\"comment\",\"condiciones_cuenta\",\"y\"]).copy()\n",
    "df9[\"comment\"] = df9[\"comment\"].astype(str)\n",
    "df9[\"condiciones_cuenta\"] = df9[\"condiciones_cuenta\"].astype(str).str.strip().str.lower()\n",
    "df9[\"comment_clean\"] = df9[\"comment\"].apply(clean_text)\n",
    "df9[\"text_with_ctx\"] = \"[canal:\" + df9[\"condiciones_cuenta\"] + \"] \" + df9[\"comment_clean\"]\n",
    "\n",
    "# Asegura numéricos\n",
    "for col in [\"insulto\",\"n_insultos\",\"comment_text_length\",\"comment_likes\"]:\n",
    "    if col not in df9.columns: df9[col] = 0\n",
    "    df9[col] = pd.to_numeric(df9[col], errors=\"coerce\").fillna(0)\n",
    "\n",
    "# Deduplicación para evitar fuga (muy importante)\n",
    "# 1) por comment_id si existe\n",
    "if \"comment_id\" in df9.columns:\n",
    "    df9 = df9.drop_duplicates(subset=[\"comment_id\"])\n",
    "# 2) por texto limpio\n",
    "df9 = df9.drop_duplicates(subset=[\"comment_clean\"])\n",
    "\n",
    "# ---------------------------\n",
    "# (OPCIONAL) PSEUDO-LABELS DE ALTA CONFIANZA DEL ARCHIVO BUENO\n",
    "# ---------------------------\n",
    "dfs = [df9[[\"comment\",\"condiciones_cuenta\",\"insulto\",\"n_insultos\",\"comment_text_length\",\"comment_likes\",\"text_with_ctx\",\"y\",\"comment_id\"]]]\n",
    "\n",
    "if GOOD_AUTO_PATH is not None and os.path.exists(GOOD_AUTO_PATH):\n",
    "    auto = pd.read_csv(GOOD_AUTO_PATH, low_memory=False)\n",
    "    # normaliza\n",
    "    for c in [\"comment\",\"condiciones_cuenta\",\"label_final\",\"ml_proba_max\",\"ml_margen\"]:\n",
    "        if c not in auto.columns:\n",
    "            auto[c] = np.nan\n",
    "    auto = auto.dropna(subset=[\"comment\",\"condiciones_cuenta\",\"label_final\"]).copy()\n",
    "    auto[\"comment\"] = auto[\"comment\"].astype(str)\n",
    "    auto[\"condiciones_cuenta\"] = auto[\"condiciones_cuenta\"].astype(str).str.strip().str.lower()\n",
    "    auto[\"label_final\"] = auto[\"label_final\"].astype(str).str.strip().str.lower().map(MAP_NORM)\n",
    "\n",
    "    # filtra alta confianza\n",
    "    auto_hc = auto[\n",
    "        auto[\"label_final\"].isin([\"ruso\",\"ucraniano\",\"neutro\"]) &\n",
    "        (pd.to_numeric(auto[\"ml_proba_max\"], errors=\"coerce\") >= AUTO_TH_P) &\n",
    "        (pd.to_numeric(auto[\"ml_margen\"], errors=\"coerce\") >= AUTO_TH_M)\n",
    "    ].copy()\n",
    "\n",
    "    # mergea columnas numéricas si están, si no rellena\n",
    "    for col in [\"insulto\",\"n_insultos\",\"comment_text_length\",\"comment_likes\"]:\n",
    "        if col not in auto_hc.columns: auto_hc[col] = 0\n",
    "        auto_hc[col] = pd.to_numeric(auto_hc[col], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    auto_hc[\"comment_clean\"] = auto_hc[\"comment\"].apply(clean_text)\n",
    "    auto_hc[\"text_with_ctx\"] = \"[canal:\" + auto_hc[\"condiciones_cuenta\"] + \"] \" + auto_hc[\"comment_clean\"]\n",
    "    auto_hc = auto_hc.rename(columns={\"label_final\":\"y\"})\n",
    "\n",
    "    # deduplicar y evitar colisión con humanos\n",
    "    if \"comment_id\" in auto_hc.columns and \"comment_id\" in df9.columns:\n",
    "        auto_hc = auto_hc[~auto_hc[\"comment_id\"].isin(df9[\"comment_id\"])]\n",
    "    auto_hc = auto_hc.drop_duplicates(subset=[\"comment_clean\"])\n",
    "\n",
    "    dfs.append(auto_hc[[\"comment\",\"condiciones_cuenta\",\"insulto\",\"n_insultos\",\"comment_text_length\",\"comment_likes\",\"text_with_ctx\",\"y\",\"comment_id\"]])\n",
    "\n",
    "base = pd.concat(dfs, ignore_index=True)\n",
    "base = base.dropna(subset=[\"text_with_ctx\",\"y\"])\n",
    "\n",
    "print(\"Tamaño base de entrenamiento:\", len(base))\n",
    "print(\"Distribución de y:\", base[\"y\"].value_counts())\n",
    "\n",
    "# ---------------------------\n",
    "# SPLIT\n",
    "# ---------------------------\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    base, base[\"y\"], test_size=0.2, random_state=42, stratify=base[\"y\"]\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# PREPROCESADO\n",
    "# ---------------------------\n",
    "text_vect = TfidfVectorizer(analyzer=\"word\", ngram_range=(1,2), min_df=2, max_features=300_000, sublinear_tf=True)\n",
    "char_vect = TfidfVectorizer(analyzer=\"char\", ngram_range=(3,5), min_df=2, max_features=200_000)\n",
    "\n",
    "pre = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"w_tfidf\", text_vect, \"text_with_ctx\"),\n",
    "        (\"c_tfidf\", char_vect, \"text_with_ctx\"),\n",
    "        (\"ohe_canal\", OneHotEncoder(handle_unknown=\"ignore\"), [\"condiciones_cuenta\"]),\n",
    "        (\"num\", StandardScaler(with_mean=False), [\"insulto\",\"n_insultos\",\"comment_text_length\",\"comment_likes\"])\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=0.3\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# CLASIFICADOR\n",
    "# ---------------------------\n",
    "svc = LinearSVC(C=0.5, class_weight=\"balanced\", max_iter=5000)  # C=0.5 + max_iter↑ para evitar warnings\n",
    "clf = CalibratedClassifierCV(estimator=svc, method=\"isotonic\", cv=3)  # probabilidades calibradas\n",
    "\n",
    "pipe = Pipeline([(\"pre\", pre), (\"clf\", clf)])\n",
    "\n",
    "# ---------------------------\n",
    "# ENTRENAR\n",
    "# ---------------------------\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# ---------------------------\n",
    "# VALIDACIÓN\n",
    "# ---------------------------\n",
    "y_pred = pipe.predict(X_val)\n",
    "print(\"\\n=== EVAL HOLD-OUT (solo humano + pseudo-HC si aplicó) ===\")\n",
    "print(classification_report(y_val, y_pred, digits=3))\n",
    "\n",
    "# (Opcional) Eval SOLO en humano puro (para la memoria)\n",
    "mask_hum = X_val[\"comment_id\"].notna()  # si tus humanos tienen comment_id y los pseudo no\n",
    "if mask_hum.any():\n",
    "    print(\"\\n=== EVAL (subset humano puro) ===\")\n",
    "    print(classification_report(y_val[mask_hum], y_pred[mask_hum], digits=3))\n",
    "\n",
    "# ---------------------------\n",
    "# GUARDAR\n",
    "# ---------------------------\n",
    "os.makedirs(os.path.dirname(OUT_MODEL), exist_ok=True)\n",
    "joblib.dump(pipe, OUT_MODEL)\n",
    "print(\"✅ Guardado baseline en\", OUT_MODEL)\n",
    "print(\"Orden de clases del modelo:\", list(pipe.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6342f0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases del modelo: ['neutro' 'ruso' 'ucraniano']\n"
     ]
    }
   ],
   "source": [
    "print(\"Clases del modelo:\", pipe.classes_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87967e7",
   "metadata": {},
   "source": [
    "Inferencia para el total de los casos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a848f4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡️ Progreso: 50,000 filas\n",
      "➡️ Progreso: 100,000 filas\n",
      "➡️ Progreso: 150,000 filas\n",
      "➡️ Progreso: 200,000 filas\n",
      "➡️ Progreso: 250,000 filas\n",
      "➡️ Progreso: 300,000 filas\n",
      "➡️ Progreso: 350,000 filas\n",
      "✅ CSV listo: ../data/processed/7_predicciones_full_sklearn.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, joblib, os, re\n",
    "\n",
    "MODEL_PATH = \"../models/baselines/stance_tfidf_svc_calibrated.joblib\"\n",
    "OUT_CSV    = \"../data/processed/7_predicciones_full_sklearn.csv\"\n",
    "CHUNK      = 50_000\n",
    "UMBRAL_PROBA = 0.55\n",
    "UMBRAL_MARGEN = 0.15  # margen entre top1 y top2 usando probas calibradas\n",
    "LABELS = [\"ruso\",\"ucraniano\",\"neutro\"]\n",
    "\n",
    "pipe = joblib.load(MODEL_PATH)\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"http\\S+\",\" \",s); s = re.sub(r\"@[A-Za-z0-9_]+\",\" \",s)\n",
    "    s = re.sub(r\"\\s+\",\" \",s).strip()\n",
    "    return s\n",
    "\n",
    "def infer_chunk(df):\n",
    "    df = df.copy()\n",
    "    df[\"comment\"] = df[\"comment\"].astype(str)\n",
    "    df[\"condiciones_cuenta\"] = df[\"condiciones_cuenta\"].astype(str).str.strip().str.lower()\n",
    "    for col in [\"insulto\",\"n_insultos\",\"comment_text_length\",\"comment_likes\"]:\n",
    "        if col not in df.columns: df[col]=0\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    df[\"comment_clean\"] = df[\"comment\"].apply(clean_text)\n",
    "    df[\"text_with_ctx\"] = \"[canal:\" + df[\"condiciones_cuenta\"] + \"] \" + df[\"comment_clean\"]\n",
    "\n",
    "    # Probabilidades calibradas\n",
    "    probas = pipe.predict_proba(df)\n",
    "    top1   = probas.argmax(axis=1)\n",
    "    top1_p = probas[np.arange(len(df)), top1]\n",
    "    \n",
    "    model_classes = pipe.classes_           # ✅ orden correcto\n",
    "    df[\"label_ml\"] = model_classes[top1]    # ✅ usa las clases reales\n",
    "    \n",
    "    # margen top1-top2\n",
    "    sorted_p = -np.sort(-probas, axis=1)\n",
    "    margen = sorted_p[:,0] - sorted_p[:,1]\n",
    "\n",
    "    df[\"label_ml\"]     = np.array(LABELS)[top1]\n",
    "    df[\"ml_proba_max\"] = top1_p\n",
    "    df[\"ml_margen\"]    = margen\n",
    "    # abstención\n",
    "    keep = (df[\"ml_proba_max\"]>=UMBRAL_PROBA) & (df[\"ml_margen\"]>=UMBRAL_MARGEN)\n",
    "    df[\"label_final\"] = np.where(keep, df[\"label_ml\"], \"\")\n",
    "    df[\"clasificacion_origen\"] = np.where(keep, \"automatica-ml-sklearn\", \"sin-clasificar\")\n",
    "    return df\n",
    "\n",
    "# Escritura incremental\n",
    "if os.path.exists(OUT_CSV): os.remove(OUT_CSV)\n",
    "processed, milestone = 0, 50_000\n",
    "for part in pd.read_csv(\"../data/processed/3_comments_youtube_with_insults.csv\", chunksize=CHUNK):\n",
    "    out = infer_chunk(part)\n",
    "    out.to_csv(OUT_CSV, mode=\"a\", header=not os.path.exists(OUT_CSV), index=False, encoding=\"utf-8-sig\")\n",
    "    processed += len(part)\n",
    "    if processed >= milestone:\n",
    "        print(f\"➡️ Progreso: {processed:,} filas\") \n",
    "        milestone += 50_000\n",
    "print(\"✅ CSV listo:\", OUT_CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77c732f",
   "metadata": {},
   "source": [
    "### Bots / Trolls: features + score + modelo simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfea42be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    user_id  bot_score_heur  likely_bot_heur  likely_bot_iso  \\\n",
      "0  UC--80__NOagdmk5_yWVvMSg             0.1            False           False   \n",
      "1  UC--IqT9PO6sVdn5gOJi6Vow             0.2            False           False   \n",
      "2  UC--QB2gsVISlAwrJBW3Cj4Q             0.1            False           False   \n",
      "3  UC--SiTzxl0sLBua11tGUf0g             0.1            False           False   \n",
      "4  UC--TT_WNCBUDQSrODZYoSnw             0.0            False           False   \n",
      "\n",
      "   likely_bot  \n",
      "0       False  \n",
      "1       False  \n",
      "2       False  \n",
      "3       False  \n",
      "4       False  \n",
      "✅ bot_scores_users.csv listo\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, re\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# === Carga ===\n",
    "df = pd.read_excel(\"predicciones_full.xlsx\")\n",
    "for col in [\"comment_time\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "\n",
    "# Normalizaciones rápidas\n",
    "df[\"comment\"] = df[\"comment\"].astype(str)\n",
    "if \"insulto\" not in df.columns: df[\"insulto\"] = False\n",
    "if \"n_insultos\" not in df.columns: df[\"n_insultos\"] = 0\n",
    "if \"user_id\" not in df.columns and \"user_name\" in df.columns:\n",
    "    df[\"user_id\"] = df[\"user_name\"].fillna(\"\")\n",
    "\n",
    "# === utilidades para features ===\n",
    "URL_RX = re.compile(r\"http[s]?://|www\\.\", re.I)\n",
    "MENT_RX = re.compile(r\"@[\\w_]+\")\n",
    "EMOJI_RX = re.compile(r\"[^\\w\\s,.\\-¡!¿?\\(\\)\\\"'@#:/]\")  # aproximación\n",
    "\n",
    "def frac_pattern(s, rx):\n",
    "    m = rx.findall(s)\n",
    "    return len(m)\n",
    "\n",
    "def frac_emojis(s):\n",
    "    # proporción aproximada de caracteres \"no alfanuméricos estándar\"\n",
    "    if not s: return 0.0\n",
    "    return len(EMOJI_RX.findall(s)) / max(1,len(s))\n",
    "\n",
    "def lang_switch_ratio(texts):\n",
    "    # proxy simple: cantidad de alfabetos/acentos distintos\n",
    "    # (si ya tenés idioma por comentario, mejor usa % de cambio de idioma por usuario)\n",
    "    return np.mean([len(set(re.findall(r\"[A-Za-zÀ-ÿ]+\", t)))==0 for t in texts])\n",
    "\n",
    "# === features por usuario ===\n",
    "def build_user_features(df):\n",
    "    g = df.groupby(\"user_id\", dropna=False)\n",
    "\n",
    "    feats = pd.DataFrame({\n",
    "        \"user_id\": g.size().index,\n",
    "        \"n_comments\": g.size().values,\n",
    "        \"n_videos\": g[\"video_title\"].nunique().values if \"video_title\" in df.columns else g.size().values,\n",
    "        \"n_channels\": g[\"channel_title\"].nunique().values if \"channel_title\" in df.columns else g.size().values,\n",
    "        \"mean_len\": g[\"comment\"].apply(lambda s: np.mean([len(x) for x in s])).values,\n",
    "        \"dup_ratio\": g[\"comment\"].apply(lambda s: 1 - (len(set(s)) / max(1,len(s)))).values,  # 0=todo único, 1=todo duplicado\n",
    "        \"url_rate\": g[\"comment\"].apply(lambda s: np.mean([bool(URL_RX.search(x)) for x in s])).values,\n",
    "        \"mention_rate\": g[\"comment\"].apply(lambda s: np.mean([bool(MENT_RX.search(x)) for x in s])).values,\n",
    "        \"emoji_frac\": g[\"comment\"].apply(lambda s: np.mean([frac_emojis(x) for x in s])).values,\n",
    "        \"insulto_rate\": g[\"insulto\"].mean().values,\n",
    "        \"n_insultos_mean\": g[\"n_insultos\"].mean().values,\n",
    "    })\n",
    "\n",
    "    # cadencia temporal\n",
    "    if \"comment_time\" in df.columns:\n",
    "        ts = g[\"comment_time\"].apply(lambda s: np.array(sorted(pd.to_datetime(s, errors=\"coerce\").dropna().astype(\"int64\"))))\n",
    "        # intervalo medio (en horas) entre comentarios\n",
    "        def mean_gap_hours(arr):\n",
    "            if len(arr) < 2: return np.nan\n",
    "            gaps = np.diff(arr) / (1e9*3600)  # ns -> horas\n",
    "            return float(np.mean(gaps))\n",
    "        feats[\"mean_gap_h\"] = ts.apply(mean_gap_hours).values\n",
    "        feats[\"burstiness\"] = ts.apply(lambda a: 0 if len(a)<3 else float(np.std(np.diff(a))/(np.mean(np.diff(a))+1e-9))).values\n",
    "    else:\n",
    "        feats[\"mean_gap_h\"] = np.nan\n",
    "        feats[\"burstiness\"] = np.nan\n",
    "\n",
    "    # “juventud” de la cuenta si está disponible\n",
    "    if \"days_since_account_creation\" in df.columns:\n",
    "        feats[\"acct_age_days_median\"] = g[\"days_since_account_creation\"].median().values\n",
    "    else:\n",
    "        feats[\"acct_age_days_median\"] = np.nan\n",
    "\n",
    "    # diversidad temática (si hay tags)\n",
    "    if \"video_tags\" in df.columns:\n",
    "        feats[\"topic_diversity\"] = g[\"video_tags\"].apply(lambda s: len(set(\";\".join([str(x) for x in s]).lower().split(\";\")))).values\n",
    "    else:\n",
    "        feats[\"topic_diversity\"] = np.nan\n",
    "\n",
    "    # “agresividad” lexical simple\n",
    "    feats[\"toxicity_proxy\"] = feats[\"insulto_rate\"]*0.6 + (feats[\"n_insultos_mean\"]>0).astype(float)*0.4\n",
    "\n",
    "    # intensidad de spam (URLs + duplicación + menciones)\n",
    "    feats[\"spam_proxy\"] = 0.5*feats[\"url_rate\"] + 0.3*feats[\"dup_ratio\"] + 0.2*feats[\"mention_rate\"]\n",
    "\n",
    "    # normalizaciones seguras\n",
    "    for c in [\"mean_len\",\"mean_gap_h\",\"emoji_frac\",\"n_comments\",\"n_videos\",\"n_channels\"]:\n",
    "        feats[c] = feats[c].fillna(0)\n",
    "\n",
    "    return feats\n",
    "\n",
    "user_feats = build_user_features(df)\n",
    "\n",
    "# === Scoring heurístico interpretable ===\n",
    "# pesos simples; ajusta si querés\n",
    "score = (\n",
    "    0.30*user_feats[\"dup_ratio\"] +\n",
    "    0.20*user_feats[\"spam_proxy\"] +\n",
    "    0.15*(user_feats[\"n_comments\"]>=50).astype(float) +\n",
    "    0.10*(user_feats[\"mean_gap_h\"]<=0.25).fillna(0).astype(float) +  # comenta cada ~15 min o menos\n",
    "    0.10*(user_feats[\"emoji_frac\"]>0.08).astype(float) +\n",
    "    0.10*(user_feats[\"insulto_rate\"]>0.2).astype(float) +\n",
    "    0.05*(user_feats[\"acct_age_days_median\"].fillna(365)<30).astype(float)\n",
    ")\n",
    "user_feats[\"bot_score_heur\"] = score.clip(0,1)\n",
    "\n",
    "# UMBRAL heurístico (conservador)\n",
    "TH_HEUR = 0.55\n",
    "user_feats[\"likely_bot_heur\"] = user_feats[\"bot_score_heur\"] >= TH_HEUR\n",
    "\n",
    "# === Anomaly detection (complementario) ===\n",
    "use_cols = [\"dup_ratio\",\"spam_proxy\",\"n_comments\",\"n_videos\",\"mean_gap_h\",\"emoji_frac\",\"insulto_rate\"]\n",
    "X = user_feats[use_cols].replace([np.inf,-np.inf], np.nan).fillna(0).values\n",
    "iso = IsolationForest(n_estimators=200, contamination=0.03, random_state=42)\n",
    "pred_iso = iso.fit_predict(X)   # -1 anomalía\n",
    "user_feats[\"likely_bot_iso\"] = (pred_iso==-1)\n",
    "\n",
    "# Fusión conservadora (OR)\n",
    "user_feats[\"likely_bot\"] = user_feats[\"likely_bot_heur\"] | user_feats[\"likely_bot_iso\"]\n",
    "\n",
    "# Guardar\n",
    "user_feats.to_csv(\"../data/processed/bot_scores_users.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(user_feats[[\"user_id\",\"bot_score_heur\",\"likely_bot_heur\",\"likely_bot_iso\",\"likely_bot\"]].head())\n",
    "print(\"✅ bot_scores_users.csv listo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1620458c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ejes_argumentativos_topics.csv (palabras por tema y bando)\n",
      "✅ ejes_argumentativos_representantes.csv (comentarios ejemplares por tema y bando)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Desktop\\Facu\\Master_Espana\\Master_UEMC\\TFM\\codigo\\analisis_guerra_ucrania_youtube\\.venv\\Lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1759: ConvergenceWarning: Maximum number of iterations 400 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "df = pd.read_excel(\"predicciones_full.xlsx\", usecols=[\"comment\",\"condiciones_cuenta\",\"channel_title\"])\n",
    "df[\"comment\"] = df[\"comment\"].astype(str)\n",
    "df[\"condiciones_cuenta\"] = df[\"condiciones_cuenta\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "# (opcional) si ya tenés label_final del stance, úsalo en lugar de 'condiciones_cuenta'\n",
    "USAR_LABEL_FINAL = False\n",
    "if USAR_LABEL_FINAL and \"label_final\" in df.columns:\n",
    "    df[\"bando\"] = df[\"label_final\"].replace({\"pro-ruso\":\"ruso\",\"pro-ucraniano\":\"ucraniano\"})\n",
    "else:\n",
    "    # proxy por bando del canal\n",
    "    df[\"bando\"] = df[\"condiciones_cuenta\"].map({\"pro-ruso\":\"ruso\",\"pro-ucraniano\":\"ucraniano\"}).fillna(\"neutro\")\n",
    "\n",
    "# limpieza ligera (no muy agresiva para no perder señales)\n",
    "def clean_min(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"http\\S+\",\" \",s)\n",
    "    s = re.sub(r\"@[A-Za-z0-9_]+\",\" \",s)\n",
    "    s = re.sub(r\"\\s+\",\" \",s).strip()\n",
    "    return s\n",
    "\n",
    "df[\"text\"] = df[\"comment\"].apply(clean_min)\n",
    "\n",
    "# === función NMF por bando ===\n",
    "def nmf_topics_for_side(df_side, n_topics=10, n_terms=12, min_df=10, max_df=0.5, max_features=200000):\n",
    "    vect = TfidfVectorizer(\n",
    "        analyzer=\"word\", ngram_range=(1,2),\n",
    "        min_df=min_df, max_df=max_df, max_features=max_features,\n",
    "        sublinear_tf=True\n",
    "    )\n",
    "    X = vect.fit_transform(df_side[\"text\"])\n",
    "    nmf = NMF(n_components=n_topics, random_state=42, init=\"nndsvd\", max_iter=400)\n",
    "    W = nmf.fit_transform(X)     # documentos x topics\n",
    "    H = nmf.components_          # topics x vocab\n",
    "\n",
    "    vocab = np.array(vect.get_feature_names_out())\n",
    "    topics = []\n",
    "    for k in range(n_topics):\n",
    "        top_idx = np.argsort(H[k])[::-1][:n_terms]\n",
    "        terms = vocab[top_idx].tolist()\n",
    "        topics.append({\"topic_id\": k, \"top_terms\": terms})\n",
    "\n",
    "    # comentarios representativos por tema\n",
    "    rep = []\n",
    "    for k in range(n_topics):\n",
    "        # top-5 docs por peso del tema k\n",
    "        doc_idx = np.argsort(W[:,k])[::-1][:5]\n",
    "        reps = df_side.iloc[doc_idx][[\"comment\",\"channel_title\"]].assign(topic_id=k, topic_score=W[doc_idx,k])\n",
    "        rep.append(reps)\n",
    "    rep_df = pd.concat(rep, ignore_index=True)\n",
    "\n",
    "    topics_df = pd.DataFrame(topics)\n",
    "    return topics_df, rep_df\n",
    "\n",
    "# === Ejecutar por bando ===\n",
    "out_topics = []\n",
    "out_reps   = []\n",
    "\n",
    "for side in [\"ruso\",\"ucraniano\",\"neutro\"]:\n",
    "    sub = df[df[\"bando\"]==side].copy()\n",
    "    if len(sub) < 1000:\n",
    "        print(f\"⚠️ Muy pocos comentarios en {side} ({len(sub)}); ajustá min_df o une con otro período.\")\n",
    "        continue\n",
    "    topics_df, rep_df = nmf_topics_for_side(sub, n_topics=12, n_terms=12, min_df=20, max_df=0.6)\n",
    "    topics_df[\"bando\"] = side\n",
    "    rep_df[\"bando\"] = side\n",
    "    out_topics.append(topics_df)\n",
    "    out_reps.append(rep_df)\n",
    "\n",
    "topics_all = pd.concat(out_topics, ignore_index=True)\n",
    "reps_all   = pd.concat(out_reps, ignore_index=True)\n",
    "\n",
    "# Guardar para PowerBI\n",
    "topics_all.to_csv(\"../data/processed/ejes_argumentativos_topics.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "reps_all.to_csv(\"../data/processed/ejes_argumentativos_representantes.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"✅ ejes_argumentativos_topics.csv (palabras por tema y bando)\")\n",
    "print(\"✅ ejes_argumentativos_representantes.csv (comentarios ejemplares por tema y bando)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9463690e",
   "metadata": {},
   "source": [
    "1) Explorar resultados (rápido)\n",
    "1.1. Resumen del CSV de predicciones (sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fa54c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas: 113585\n",
      "clasificacion_origen\n",
      "automatica-nn-tf    91552\n",
      "regla               13475\n",
      "sin-clasificar       8558\n",
      "Name: count, dtype: int64\n",
      "label_final\n",
      "ruso         76564\n",
      "neutro       14621\n",
      "ucraniano    13842\n",
      "NaN           8558\n",
      "Name: count, dtype: int64\n",
      "Cobertura: 1.000\n",
      "\n",
      "Distribución por 'condiciones_cuenta' y label_final:\n",
      "label_final         neutro   ruso  ucraniano\n",
      "condiciones_cuenta                          \n",
      "noticiero             2893  17249       4493\n",
      "pro-ruso              8433  36422       6211\n",
      "pro-ucraniano         3295  22893       3138\n",
      "\n",
      "Tasa de insulto por label_final:\n",
      "label_final\n",
      "ucraniano    0.281534\n",
      "ruso         0.035382\n",
      "neutro       0.008207\n",
      "Name: insulto, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "PRED_XLSX = \"predicciones_full.xlsx\"\n",
    "pred = pd.read_excel(PRED_XLSX)\n",
    "\n",
    "print(\"Filas:\", len(pred))\n",
    "print(pred[\"clasificacion_origen\"].value_counts(dropna=False))\n",
    "print(pred[\"label_final\"].value_counts(dropna=False))\n",
    "\n",
    "# Cobertura (no abstención)\n",
    "cobertura = (pred[\"label_final\"].astype(str) != \"\").mean()\n",
    "print(f\"Cobertura: {cobertura:.3f}\")\n",
    "\n",
    "# Distribución por bando de canal\n",
    "print(\"\\nDistribución por 'condiciones_cuenta' y label_final:\")\n",
    "print(pred.pivot_table(index=\"condiciones_cuenta\", columns=\"label_final\", aggfunc=\"size\", fill_value=0))\n",
    "\n",
    "# % insultos por tipo de comentario (si están las columnas)\n",
    "if \"insulto\" in pred.columns:\n",
    "    print(\"\\nTasa de insulto por label_final:\")\n",
    "    print(pred.groupby(\"label_final\")[\"insulto\"].mean().sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2903b63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "% likely_bot por 'condiciones_cuenta':\n",
      "condiciones_cuenta\n",
      "pro-ucraniano    0.189246\n",
      "pro-ruso         0.174937\n",
      "noticiero        0.148150\n",
      "Name: likely_bot, dtype: float64\n",
      "\n",
      "Top 15 usuarios sospechosos (por score heurístico):\n",
      "                        user_id  bot_score_heur  likely_bot  likely_bot_heur  \\\n",
      "36393  UCd3myKELT4Sbn0eiA1Li29Q        0.470000        True            False   \n",
      "26713  UCT-vA-wEnH23u5xoAPZ3AMA        0.470000        True            False   \n",
      "17833  UCIs5UKpQpQohvcej158G0tg        0.442000        True            False   \n",
      "32970  UC_7_py5TIuUk-rGZD_eQM0A        0.440000        True            False   \n",
      "25771  UCRvhxzKkHPR25yloobXiRrA        0.431096        True            False   \n",
      "3117   UC2V85Go8-XoRuiufNDY6zcQ        0.424000        True            False   \n",
      "48902  UCrIuQN7Re-9JzE5G2Sm0n5A        0.420000        True            False   \n",
      "42233  UCjcgiY3CxQN6LsydZGzYg1A        0.417671        True            False   \n",
      "17190  UCI8zD9BZVJ87yb7y-qUYmGg        0.408571        True            False   \n",
      "21952  UCNb3AYmhstUZHTT1V8-l9BA        0.404615        True            False   \n",
      "24892  UCQtzNKUOZEVX9AfOkbCiC3Q        0.400000        True            False   \n",
      "30826  UCXihtgYMbZlsBn_TFYalMwg        0.400000        True            False   \n",
      "38497  UCfPpZkY7QzuO8rIp_jXEE8w        0.388000        True            False   \n",
      "53834  UCwuoXX4UzN-MDrHjqk27-IA        0.388000        True            False   \n",
      "18348  UCJUEyloNo8i_Xvv19RSQBuA        0.380000        True            False   \n",
      "\n",
      "       likely_bot_iso  dup_ratio  spam_proxy  n_comments   mean_gap_h  \n",
      "36393            True   0.750000    0.225000           4     0.002593  \n",
      "26713            True   0.750000    0.225000           4     0.003056  \n",
      "17833            True   0.950000    0.285000          20   428.010848  \n",
      "32970            True   0.666667    0.200000           3     0.001528  \n",
      "25771            True   0.780822    0.234247          73    96.617693  \n",
      "3117             True   0.900000    0.270000          10   517.955710  \n",
      "48902            True   0.888889    0.266667           9   978.611736  \n",
      "42233            True   0.465753    0.139726          73   108.996169  \n",
      "17190            True   0.857143    0.257143           7   251.999815  \n",
      "21952            True   0.846154    0.253846          13   620.325347  \n",
      "24892            True   0.833333    0.250000          12   257.237298  \n",
      "30826            True   0.833333    0.250000           6     0.005333  \n",
      "38497            True   0.800000    0.240000           5  1595.889931  \n",
      "53834            True   0.800000    0.240000          10   243.490216  \n",
      "18348            True   0.500000    0.150000           2     0.020000  \n"
     ]
    }
   ],
   "source": [
    "bots = pd.read_csv(\"../data/processed/bot_scores_users.csv\")\n",
    "pred_b = pred.merge(bots[[\"user_id\",\"likely_bot\",\"bot_score_heur\",\"likely_bot_heur\",\"likely_bot_iso\"]],\n",
    "                    on=\"user_id\", how=\"left\")\n",
    "\n",
    "print(\"\\n% likely_bot por 'condiciones_cuenta':\")\n",
    "print(pred_b.groupby(\"condiciones_cuenta\")[\"likely_bot\"].mean().sort_values(ascending=False))\n",
    "\n",
    "print(\"\\nTop 15 usuarios sospechosos (por score heurístico):\")\n",
    "cols = [\"user_id\",\"bot_score_heur\",\"likely_bot\",\"likely_bot_heur\",\"likely_bot_iso\",\"dup_ratio\",\"spam_proxy\",\"n_comments\",\"mean_gap_h\"]\n",
    "print(bots.sort_values(\"bot_score_heur\", ascending=False)[cols].head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dceee1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1) Tu lista base de stopwords (la que pasaste)\n",
    "spanish_stopwords = [\n",
    "    \"de\",\"la\",\"que\",\"el\",\"en\",\"y\",\"a\",\"los\",\"del\",\"se\",\"las\",\"por\",\"un\",\"para\",\n",
    "    \"con\",\"no\",\"una\",\"su\",\"al\",\"lo\",\"como\",\"más\",\"pero\",\"sus\",\"le\",\"ya\",\"o\",\"este\",\n",
    "    \"sí\",\"porque\",\"esta\",\"entre\",\"cuando\",\"muy\",\"sin\",\"sobre\",\"también\",\"me\",\"hasta\",\n",
    "    \"hay\",\"donde\",\"quien\",\"desde\",\"todo\",\"nos\",\"durante\",\"todos\",\"uno\",\"les\",\"ni\",\n",
    "    \"contra\",\"otros\",\"ese\",\"eso\",\"ante\",\"ellos\",\"e\",\"esto\",\"mí\",\"antes\",\"algunos\",\n",
    "    \"qué\",\"unos\",\"yo\",\"otro\",\"otras\",\"otra\",\"él\",\"tanto\",\"esa\",\"estos\",\"mucho\",\n",
    "    \"quienes\",\"nada\",\"muchos\",\"cual\",\"poco\",\"ella\",\"estar\",\"estas\",\"algunas\",\"algo\",\n",
    "    \"nosotros\",\"mi\",\"mis\",\"tú\",\"te\",\"ti\",\"tu\",\"tus\",\"ellas\",\"nosotras\",\"vosotros\",\n",
    "    \"vosotras\",\"os\",\"mío\",\"mía\",\"míos\",\"mías\",\"tuyo\",\"tuya\",\"tuyos\",\"tuyas\",\"suyo\",\n",
    "    \"suya\",\"suyos\",\"suyas\",\"nuestro\",\"nuestra\",\"nuestros\",\"nuestras\",\"vuestro\",\n",
    "    \"vuestra\",\"vuestros\",\"vuestras\",\"esos\",\"esas\", \"es\", \"ser\", \"fue\", \"son\", \"sido\", \"tiene\", \"tenido\", \"tienen\", \"tenía\",\n",
    "    \"tenían\", \"tendría\", \"tendrían\", \"había\", \"habían\", \"habrá\", \"habrán\", \"habría\", \"son\", \"serán\", \"sería\", \"serían\",\n",
    "    \"está\", \"están\", \"estuve\", \"estuvieron\", \"estaría\", \"estarían\", \"esté\", \"estén\", \"estuviera\", \"estuvieran\", \n",
    "    \"mas\", \"q\", \"solo\", \"tiene\", \"tienen\", \"si\", \"no\", \"estan\", \"va\", \"ha\", \"vivo\", \"viva\", \"ahora\", \"asi\", \"aqui\", \"ahi\", \n",
    "    \"siempre\", \"nunca\", \"ya\", \"sabe\", \"misma\", \"tambien\", \"era\", \"ve\", \"dio\", \"fueron\", \"toda\", \"misma\", \"hacer\", \"entonces\", \n",
    "    \"iran\", \"cosa\", \"dice\", \"querer\", \"poder\",  \"haber\",  \"ver\", \"tener\", \"dar\", \"deber\", \"creer\", \"saber\", \"seguir\", \"mismo\", \"bien\", \n",
    "    \"ir\", \"ganar\", \"perder\", \"dejar\",  \"decir\"\n",
    "]\n",
    "\n",
    "# 2) Stopwords “meta YouTube” y ruidos comunes en comentarios\n",
    "yt_stop = [\n",
    "    \"video\",\"canal\",\"like\",\"suscribete\",\"suscribirse\",\"suscríbete\",\"link\",\n",
    "    \"gente\",\"personas\",\"hola\",\"gracias\",\"gracia\",\"amigo\",\"amigos\",\"comenta\",\n",
    "    \"comentario\",\"comentarios\",\"ver\",\"viendo\",\"visto\",\"nuevo\",\"noticia\",\"noticias\"\n",
    "]\n",
    "\n",
    "# 3) Lista de términos que NO queremos eliminar (claves del conflicto)\n",
    "PROTECT = {\"ucrania\",\"ucraniano\",\"ucranianos\",\"rusia\",\"ruso\",\"rusos\",\n",
    "           \"putin\",\"zelensky\",\"nato\",\"otan\",\"eeuu\",\"estados\",\"unidos\",\"kiev\",\"crimea\",\"donbas\",\"donbass\",\"kherson\",\"kharkiv\"}\n",
    "\n",
    "def build_stopwords_auto(corpus_series: pd.Series, k_top: int = 1000):\n",
    "    \"\"\"\n",
    "    Extrae las k palabras más frecuentes del corpus y las suma a las stopwords,\n",
    "    excluyendo términos protegidos (PROTECT) y palabras con dígitos.\n",
    "    \"\"\"\n",
    "    # tokenización simple\n",
    "    toks = corpus_series.str.lower().str.findall(r\"[a-záéíóúñü]+\")\n",
    "    freq = {}\n",
    "    for ts in toks.dropna():\n",
    "        for t in ts:\n",
    "            if any(ch.isdigit() for ch in t): \n",
    "                continue\n",
    "            freq[t] = freq.get(t, 0) + 1\n",
    "    # top-k menos informativos\n",
    "    common = [w for w, _ in sorted(freq.items(), key=lambda x: x[1], reverse=True)[:k_top]]\n",
    "    # ensamblar stopwords ampliadas\n",
    "    base = set(spanish_stopwords) | set(yt_stop)\n",
    "    extra = [w for w in common if (w not in PROTECT) and (len(w) > 2)]\n",
    "    return sorted(base | set(extra))\n",
    "\n",
    "# 4) Construye la lista final de stopwords a partir de tu DataFrame de textos LIMPIOS\n",
    "#    Usa el mismo df/texto que alimenta al NMF (p.ej. df[\"text\"])\n",
    "#    Ejemplo: stopwords_final = build_stopwords_auto(df[\"text\"], k_top=800)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f50b0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "def nmf_topics_for_side(df_side, n_topics=12, n_terms=12, min_df=20, max_df=0.6, max_features=200000):\n",
    "    # stopwords automáticas basadas en el subcorpus de ese bando\n",
    "    stopwords_final = build_stopwords_auto(df_side[\"text\"], k_top=800)\n",
    "\n",
    "    vect = TfidfVectorizer(\n",
    "        analyzer=\"word\",\n",
    "        ngram_range=(1,2),\n",
    "        min_df=min_df,\n",
    "        max_df=max_df,\n",
    "        max_features=max_features,\n",
    "        sublinear_tf=True,\n",
    "        stop_words=stopwords_final,\n",
    "        token_pattern=r\"(?u)\\b\\w\\w+\\b\"   # evita tokens de 1 carácter\n",
    "    )\n",
    "    X = vect.fit_transform(df_side[\"text\"])\n",
    "    nmf = NMF(n_components=n_topics, random_state=42, init=\"nndsvd\", max_iter=500, alpha_W=0.0, alpha_H=0.0, l1_ratio=0.0)\n",
    "    W = nmf.fit_transform(X)\n",
    "    H = nmf.components_\n",
    "\n",
    "    vocab = np.array(vect.get_feature_names_out())\n",
    "    topics = []\n",
    "    for k in range(n_topics):\n",
    "        top_idx = np.argsort(H[k])[::-1][:n_terms]\n",
    "        topics.append({\"topic_id\": k, \"top_terms\": vocab[top_idx].tolist()})\n",
    "\n",
    "    # Representativos por tema\n",
    "    rep_rows = []\n",
    "    for k in range(n_topics):\n",
    "        doc_idx = np.argsort(W[:, k])[::-1][:5]\n",
    "        reps = df_side.iloc[doc_idx][[\"comment\",\"channel_title\"]].assign(topic_id=k, topic_score=W[doc_idx, k])\n",
    "        rep_rows.append(reps)\n",
    "    rep_df = pd.concat(rep_rows, ignore_index=True)\n",
    "\n",
    "    topics_df = pd.DataFrame(topics)\n",
    "    return topics_df, rep_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8d7b462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "TOPICS_PATH = \"../data/processed/ejes_argumentativos_topics.csv\"\n",
    "REPS_PATH   = \"../data/processed/ejes_argumentativos_representantes.csv\"\n",
    "\n",
    "topics = pd.read_csv(TOPICS_PATH)\n",
    "reps   = pd.read_csv(REPS_PATH)\n",
    "\n",
    "def parse_terms(x):\n",
    "    if isinstance(x, list): return x\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            v = ast.literal_eval(x)\n",
    "            if isinstance(v, list): return [str(t) for t in v]\n",
    "        except Exception:\n",
    "            return [t.strip() for t in x.split(\",\") if t.strip()]\n",
    "    return []\n",
    "\n",
    "topics[\"bando\"] = topics[\"bando\"].astype(str).str.strip().str.lower()\n",
    "reps[\"bando\"]   = reps[\"bando\"].astype(str).str.strip().str.lower()\n",
    "topics[\"top_terms_list\"] = topics[\"top_terms\"].apply(parse_terms)\n",
    "topics[\"topic_title\"]    = topics[\"top_terms_list\"].apply(lambda xs: \", \".join(xs[:3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dad580d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temas por bando:\n",
      "bando\n",
      "neutro       12\n",
      "ruso         12\n",
      "ucraniano    12\n",
      "Name: topic_id, dtype: int64 \n",
      "\n",
      "=== RUSO — 12 temas ===\n",
      "Topic 00: que, no, se, lo, si, lo que, ya, le, no se, que no, hay, que se\n",
      "Topic 01: viva, viva rusia, rusia viva, viva la, viva putin, rusia, yemen, viva yemen, viva el, palestina, que viva, russia\n",
      "Topic 02: gracias, gracias por, por, gracias miguel, información, por la, la información, muchas gracias, muchas, por tu, miguel por, miguel gracias\n",
      "Topic 03: la, otan, la otan, de la, de, guerra, la guerra, ucrania, en la, en, por la, con la\n",
      "Topic 04: saludos, desde, saludos desde, saludos miguel, méxico, venezuela, miguel desde, miguel saludos, desde venezuela, desde méxico, argentina, desde argentina\n",
      "Topic 05: los, son, de, de los, rusos, todos, los rusos, terroristas, todos los, son los, que los, con\n",
      "Topic 06: el, en, de, del, mundo, en el, el mundo, su, al, con, pueblo, todo\n",
      "Topic 07: excelente, análisis, excelente análisis, excelente información, siempre, como siempre, programa, excelente programa, invitado, información, video, como\n",
      "Topic 08: rusia, fuerza, fuerza rusia, de rusia, rusia no, ucrania, que rusia, vamos, viva rusia, con, vamos rusia, con rusia\n",
      "Topic 09: miguel, gracias miguel, te, tu, hola, saludos miguel, hola miguel, miguel por, muy, eres, buenas, bien\n",
      "Topic 10: es, un, es un, de, una, no es, es una, es el, este, es la, eso, que es\n",
      "Topic 11: putin, viva putin, de putin, vladimir, vladimir putin, putin es, putin no, presidente, que putin, presidente putin, le, ucrania\n",
      "=== UCRANIANO — 12 temas ===\n",
      "Topic 00: que, no, es, lo, se, un, si, de, pero, por, una, lo que\n",
      "Topic 01: memorias, memorias de, de, de otan, de memorias, de pez, otan, pez, de ucrania, de zelensky, zelensky, de la\n",
      "Topic 02: otan, la otan, de la, la, de, pro otan, otan no, ucrania la, contra, propaganda, otan es, otan en\n",
      "Topic 03: este, canal, este canal, canal es, es, pro, en este, de este, un, es un, este tipo, pro ucraniano\n",
      "Topic 04: ucrania, ganando, va, va ganando, de ucrania, ucrania va, que ucrania, ganando ucrania, está, gloria, gloria ucrania, sigue\n",
      "Topic 05: gracias, gracias por, por, por el, informarnos, por informarnos, muchas gracias, resumen, por la, muchas, excelente, video\n",
      "Topic 06: rusia, viva, viva rusia, que rusia, de rusia, vamos, viva ucrania, vamos rusia, viva la, viva russia, madre, la madre\n",
      "Topic 07: los, rusos, los rusos, ucranianos, de, de los, los ucranianos, son, en, comentarios, que los, los comentarios\n",
      "Topic 08: pez, de pez, memoria, memoria de, de, buen, video, buen video, mentiras, mentiras de, pez buen, pez es\n",
      "Topic 09: el, en, de, en el, del, video, ruso, con, es el, resumen, mundo, el video\n",
      "Topic 10: guerra, la guerra, la, esta, de, en, guerra de, ganando la, de la, en la, esta guerra, guerra en\n",
      "Topic 11: ya, ya no, no, ucrania ya, ya se, que ya, rusia ya, se, esta, ya que, ganó, años\n",
      "=== NEUTRO — 12 temas ===\n",
      "Topic 00: que, se, lo, no, lo que, le, que se, que no, si, no se, ya, por\n",
      "Topic 01: viva, viva rusia, viva ucrania, rusia, viva putin, rusia viva, viva la, viva russia, viva el, que viva, abajo, ucrania viva\n",
      "Topic 02: otan, la otan, la, de la, otan no, contra, otan que, eeuu, que la, con, son, con la\n",
      "Topic 03: ucrania, fuerza, fuerza ucrania, gloria, viva ucrania, gloria ucrania, de ucrania, en ucrania, ucrania no, dios, que ucrania, bien\n",
      "Topic 04: los, rusos, los rusos, son, ucranianos, de los, los ucranianos, en, todos, que los, son los, no\n",
      "Topic 05: russia, viva russia, slava, vamos, viva, esta, ukraine, todo, mundo libre, con todo, mundo, libre\n",
      "Topic 06: el, en, mundo, del, en el, el mundo, ruso, por, todo, con, ucraniano, es el\n",
      "Topic 07: putin, de putin, putin es, viva putin, putin no, le, que putin, vladimir, putin se, vladimir putin, asesino, ya\n",
      "Topic 08: rusia, vamos, vamos rusia, de rusia, que rusia, con, rusia no, viva rusia, rusia es, contra, fuerza rusia, con rusia\n",
      "Topic 09: es, un, no, es un, no es, una, es una, eso, si, esto, putin es, eso es\n",
      "Topic 10: guerra, la, la guerra, en, esta, paz, en la, mundial, una, guerra mundial, una guerra, ya\n",
      "Topic 11: de, de la, las, de los, la, de ucrania, una, por, al, de rusia, su, de las\n"
     ]
    }
   ],
   "source": [
    "# Resumen: nº de temas por bando\n",
    "print(\"Temas por bando:\")\n",
    "print(topics.groupby(\"bando\")[\"topic_id\"].nunique(), \"\\n\")\n",
    "\n",
    "# Tabla “bonita” de términos por bando\n",
    "def ver_topicos(bando:str, n_terms:int=12):\n",
    "    b = bando.strip().lower()\n",
    "    t = topics[topics[\"bando\"]==b].sort_values(\"topic_id\")\n",
    "    if t.empty:\n",
    "        print(f\"(No hay temas para '{bando}')\"); return\n",
    "    print(f\"=== {b.upper()} — {t['topic_id'].nunique()} temas ===\")\n",
    "    for _, row in t.iterrows():\n",
    "        tid = int(row[\"topic_id\"])\n",
    "        terms = row[\"top_terms_list\"][:n_terms]\n",
    "        print(f\"Topic {tid:02d}: \" + \", \".join(terms))\n",
    "\n",
    "ver_topicos(\"ruso\")\n",
    "ver_topicos(\"ucraniano\")\n",
    "ver_topicos(\"neutro\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
