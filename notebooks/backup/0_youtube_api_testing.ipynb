{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fba5c240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "from typing import Dict, List, Optional\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f93a76fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- CONFIG ----------------\n",
    "API_KEY = os.getenv(\"YOUTUBE_API_KEY\")  # o reemplaza por string literal si prefieres\n",
    "OUT_DIR = \"./data_rebuild\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Salidas\n",
    "F_VIDEOS = os.path.join(OUT_DIR, \"videos_por_canal.csv\")\n",
    "F_COMMENTS = os.path.join(OUT_DIR, \"0_comments_raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d05d779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tus canales (ID -> bloque)\n",
    "canales_interes: Dict[str, str] = {\n",
    "    'UCPH3Oz99Y_jrVBCQMjQZNSg': 'pro-ucraniano',       # Memorias de Pez\n",
    "    'UC3tNpTOHsTnkmbX1M2sS4xg': 'pro-ucraniano',       # VisualPolitik\n",
    "    'UCnsvJeZO4RigQ898WdDNoBw': 'noticiero',           # El PaÃ­s\n",
    "    'UC7QZIf0dta-XPXsp9Hv4dTw': 'noticiero',           # RTVE Noticias\n",
    "    'UClLLRs_mFTsNT5U-DqTYAGg': 'noticiero',           # La Vanguardia\n",
    "    'UCwd8Byi93KbnsYmCcKLExvQ': 'pro-ruso',            # Negocios TV\n",
    "    'UCgms7r9SaeYhuIBaPGOjnhw': 'pro-ruso',            # Miguel Ruiz Calvo\n",
    "    'UCNKomgId0-uTA-vVLM9v1pw': 'pro-ruso',            # IntereconomÃ­a\n",
    "    'UCGXbLrVe8vnkiFv7q2vYv3w': 'noticiero',           # El Mundo\n",
    "    'UCCJs5mITIqxqJGeFjt9N1Mg': 'noticiero',           # laSexta Noticias\n",
    "    'UCcgqSM4YEo5vVQpqwN-MaNw': 'pro-ruso',            # teleSUR\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f2e5b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- UTILS ----------------\n",
    "def yt() :\n",
    "    if not API_KEY:\n",
    "        raise RuntimeError(\"Falta YOUTUBE_API_KEY en el entorno.\")\n",
    "    return build(\"youtube\", \"v3\", developerKey=API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78ceab84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iso8601_duration_to_seconds(d: str) -> float:\n",
    "    # Formato tÃ­pico: PT1H2M3S / PT15M / PT45S\n",
    "    if not d or not d.startswith(\"P\"):\n",
    "        return float(\"nan\")\n",
    "    hours = minutes = seconds = 0\n",
    "    # Simple parser\n",
    "    t = d.split(\"T\")\n",
    "    date_part = t[0]\n",
    "    time_part = t[1] if len(t) > 1 else \"\"\n",
    "    num = \"\"\n",
    "    for ch in time_part:\n",
    "        if ch.isdigit() or ch == \".\":\n",
    "            num += ch\n",
    "        else:\n",
    "            if ch == \"H\":\n",
    "                hours = float(num or 0)\n",
    "            elif ch == \"M\":\n",
    "                minutes = float(num or 0)\n",
    "            elif ch == \"S\":\n",
    "                seconds = float(num or 0)\n",
    "            num = \"\"\n",
    "    return hours*3600 + minutes*60 + seconds\n",
    "\n",
    "def safe_int(x, default=0):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except:\n",
    "        return default\n",
    "\n",
    "def append_csv(path: str, rows: List[dict]):\n",
    "    if not rows:\n",
    "        return\n",
    "    df = pd.DataFrame(rows)\n",
    "    header = not os.path.exists(path)\n",
    "    df.to_csv(path, index=False, mode=\"a\", header=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c49b68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- DISCOVERY ----------------\n",
    "def get_channel_meta(y, channel_id: str) -> dict:\n",
    "    r = y.channels().list(\n",
    "        part=\"snippet,statistics,contentDetails\",\n",
    "        id=channel_id\n",
    "    ).execute()\n",
    "    items = r.get(\"items\", [])\n",
    "    if not items:\n",
    "        return {}\n",
    "    it = items[0]\n",
    "    uploads_pl = it[\"contentDetails\"][\"relatedPlaylists\"][\"uploads\"]\n",
    "    return {\n",
    "        \"channel_id\": channel_id,\n",
    "        \"channel_title\": it[\"snippet\"][\"title\"],\n",
    "        \"subscriber_count\": safe_int(it.get(\"statistics\", {}).get(\"subscriberCount\")),\n",
    "        \"uploads_playlist\": uploads_pl\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2f92511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_videos_from_uploads(y, uploads_playlist: str, max_items: Optional[int] = None) -> List[dict]:\n",
    "    out = []\n",
    "    page = None\n",
    "    while True:\n",
    "        r = y.playlistItems().list(\n",
    "            part=\"snippet,contentDetails\",\n",
    "            playlistId=uploads_playlist,\n",
    "            maxResults=50,\n",
    "            pageToken=page\n",
    "        ).execute()\n",
    "        for it in r.get(\"items\", []):\n",
    "            sn = it[\"snippet\"]\n",
    "            cd = it[\"contentDetails\"]\n",
    "            out.append({\n",
    "                \"video_id\": cd.get(\"videoId\"),\n",
    "                \"video_title_guess\": sn.get(\"title\"),\n",
    "                \"video_published_at_guess\": cd.get(\"videoPublishedAt\", sn.get(\"publishedAt\")),\n",
    "            })\n",
    "            if max_items and len(out) >= max_items:\n",
    "                return out\n",
    "        page = r.get(\"nextPageToken\")\n",
    "        if not page:\n",
    "            break\n",
    "        time.sleep(0.1)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0190aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_video_meta(y, video_ids: List[str]) -> pd.DataFrame:\n",
    "    # videos().list permite 50 ids por llamada\n",
    "    rows = []\n",
    "    for i in range(0, len(video_ids), 50):\n",
    "        chunk = video_ids[i:i+50]\n",
    "        r = y.videos().list(\n",
    "            part=\"snippet,statistics,contentDetails\",\n",
    "            id=\",\".join(chunk)\n",
    "        ).execute()\n",
    "        for it in r.get(\"items\", []):\n",
    "            sn = it[\"snippet\"]; st = it.get(\"statistics\", {}); ct = it.get(\"contentDetails\", {})\n",
    "            rows.append({\n",
    "                \"video_id\": it[\"id\"],\n",
    "                \"video_title\": sn.get(\"title\"),\n",
    "                \"channel_title\": sn.get(\"channelTitle\"),\n",
    "                \"video_published_at\": sn.get(\"publishedAt\"),\n",
    "                \"video_views\": safe_int(st.get(\"viewCount\")),\n",
    "                \"video_likes\": safe_int(st.get(\"likeCount\")),\n",
    "                \"video_duration\": iso8601_duration_to_seconds(ct.get(\"duration\", \"\")),\n",
    "                \"video_tags\": \"|\".join(sn.get(\"tags\", [])) if sn.get(\"tags\") else \"\",\n",
    "                \"video_category_id\": safe_int(sn.get(\"categoryId\"), default=0),\n",
    "            })\n",
    "        time.sleep(0.2)\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2850440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_videos_for_channels(canales: Dict[str, str], max_per_channel: Optional[int] = None) -> pd.DataFrame:\n",
    "    y = yt()\n",
    "    all_rows = []\n",
    "    for channel_id, bloque in canales.items():\n",
    "        try:\n",
    "            meta = get_channel_meta(y, channel_id)\n",
    "            if not meta:\n",
    "                print(f\"[WARN] Channel sin meta: {channel_id}\")\n",
    "                continue\n",
    "            vids = list_videos_from_uploads(y, meta[\"uploads_playlist\"], max_items=max_per_channel)\n",
    "            if not vids:\n",
    "                print(f\"[INFO] Sin videos en uploads: {channel_id}\")\n",
    "                continue\n",
    "            dfm = enrich_video_meta(y, [v[\"video_id\"] for v in vids])\n",
    "            if dfm.empty:\n",
    "                continue\n",
    "            dfm.insert(0, \"channel_id\", meta[\"channel_id\"])\n",
    "            dfm.insert(1, \"channel_title_resolved\", meta[\"channel_title\"])\n",
    "            dfm[\"subscriber_count\"] = meta[\"subscriber_count\"]\n",
    "            dfm[\"bloque\"] = bloque\n",
    "            all_rows.append(dfm)\n",
    "        except HttpError as e:\n",
    "            print(f\"[ERROR] {channel_id}: {e}\")\n",
    "            time.sleep(1.0)\n",
    "            continue\n",
    "    if not all_rows:\n",
    "        return pd.DataFrame()\n",
    "    df = pd.concat(all_rows, ignore_index=True).drop_duplicates(subset=[\"video_id\"])\n",
    "    df.to_csv(F_VIDEOS, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb78b870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- COMMENTS ----------------\n",
    "def fetch_comments_for_video(y, video_id: str, video_meta: dict,\n",
    "                             subscriber_count: Optional[int],\n",
    "                             channel_id: str,\n",
    "                             bloque: str,\n",
    "                             max_comments: int = 800,\n",
    "                             sleep_s: float = 0.3) -> List[dict]:\n",
    "    out = []\n",
    "    page = None\n",
    "    got = 0\n",
    "    while True:\n",
    "        try:\n",
    "            r = y.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                textFormat=\"plainText\",\n",
    "                pageToken=page\n",
    "            ).execute()\n",
    "        except HttpError as e:\n",
    "            # Backoff suave ante 403/429\n",
    "            time.sleep(1.5)\n",
    "            break\n",
    "        for it in r.get(\"items\", []):\n",
    "            sn = it[\"snippet\"]\n",
    "            top = sn[\"topLevelComment\"][\"snippet\"]\n",
    "            row = {\n",
    "                # --- esquema solicitado ---\n",
    "                \"comment_id\": sn[\"topLevelComment\"][\"id\"],\n",
    "                \"comment\": top.get(\"textDisplay\", \"\"),\n",
    "                \"comment_text_length\": len(top.get(\"textDisplay\", \"\")),\n",
    "                \"user_id\": (top.get(\"authorChannelId\") or {}).get(\"value\"),\n",
    "                \"user_name\": top.get(\"authorDisplayName\"),\n",
    "                \"comment_time\": top.get(\"publishedAt\"),\n",
    "                \"comment_likes\": safe_int(top.get(\"likeCount\")),\n",
    "                \"total_reply_count\": safe_int(sn.get(\"totalReplyCount\")),\n",
    "                \"is_top_level_comment\": True,\n",
    "                \"video_title\": video_meta.get(\"video_title\"),\n",
    "                \"channel_title\": video_meta.get(\"channel_title\"),\n",
    "                \"video_published_at\": video_meta.get(\"video_published_at\"),\n",
    "                \"video_views\": video_meta.get(\"video_views\"),\n",
    "                \"video_likes\": video_meta.get(\"video_likes\"),\n",
    "                \"video_duration\": video_meta.get(\"video_duration\"),\n",
    "                \"video_tags\": video_meta.get(\"video_tags\"),\n",
    "                \"video_category_id\": video_meta.get(\"video_category_id\"),\n",
    "                \"relacion_evento\": \"\",\n",
    "                \"evento\": \"\",\n",
    "                \"tipo_evento\": \"\",\n",
    "                \"condiciones_cuenta\": \"\",\n",
    "                \"account_created_at\": None,  # no disponible\n",
    "                \"channel_id\": channel_id,\n",
    "                \"subscriber_count\": subscriber_count,\n",
    "                # extras Ãºtiles\n",
    "                \"video_id\": video_id,\n",
    "                \"bloque\": bloque,\n",
    "            }\n",
    "            out.append(row)\n",
    "            got += 1\n",
    "            if got >= max_comments:\n",
    "                break\n",
    "        if got >= max_comments:\n",
    "            break\n",
    "        page = r.get(\"nextPageToken\")\n",
    "        if not page:\n",
    "            break\n",
    "        time.sleep(sleep_s)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1fda707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_video_meta_row(r) -> dict:\n",
    "    return {\n",
    "        \"video_title\": r[\"video_title\"],\n",
    "        \"channel_title\": r[\"channel_title\"],\n",
    "        \"video_published_at\": r[\"video_published_at\"],\n",
    "        \"video_views\": r[\"video_views\"],\n",
    "        \"video_likes\": r[\"video_likes\"],\n",
    "        \"video_duration\": r[\"video_duration\"],\n",
    "        \"video_tags\": r[\"video_tags\"],\n",
    "        \"video_category_id\": r[\"video_category_id\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a9c6a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_comments_incremental(\n",
    "    max_comments_per_video: int = 800,\n",
    "    save_every: int = 10000\n",
    "):\n",
    "    y = yt()\n",
    "    if not os.path.exists(F_VIDEOS):\n",
    "        raise RuntimeError(\"No existe videos_por_canal.csv. Ejecuta primero la fase de descubrimiento.\")\n",
    "    df_vids = pd.read_csv(F_VIDEOS)\n",
    "\n",
    "    # Progreso previo\n",
    "    seen = set()\n",
    "    if os.path.exists(F_COMMENTS):\n",
    "        try:\n",
    "            prev = pd.read_csv(F_COMMENTS, usecols=[\"comment_id\"])\n",
    "            seen = set(prev[\"comment_id\"].astype(str).tolist())\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    buffer = []\n",
    "    total_new = 0\n",
    "    for i, r in df_vids.iterrows():\n",
    "        video_id = str(r[\"video_id\"])\n",
    "        channel_id = str(r[\"channel_id\"])\n",
    "        bloque = r[\"bloque\"]\n",
    "        subs = int(r.get(\"subscriber_count\", 0) or 0)\n",
    "        video_meta = build_video_meta_row(r)\n",
    "\n",
    "        rows = fetch_comments_for_video(\n",
    "            y,\n",
    "            video_id=video_id,\n",
    "            video_meta=video_meta,\n",
    "            subscriber_count=subs,\n",
    "            channel_id=channel_id,\n",
    "            bloque=bloque,\n",
    "            max_comments=max_comments_per_video\n",
    "        )\n",
    "        # de-dupe por comment_id\n",
    "        new_rows = [x for x in rows if str(x[\"comment_id\"]) not in seen]\n",
    "        for x in new_rows:\n",
    "            seen.add(str(x[\"comment_id\"]))\n",
    "        buffer.extend(new_rows)\n",
    "        total_new += len(new_rows)\n",
    "\n",
    "        if len(buffer) >= save_every:\n",
    "            append_csv(F_COMMENTS, buffer)\n",
    "            buffer = []\n",
    "            print(f\"ðŸ’¾ Guardado incremental. Nuevos: {total_new}\")\n",
    "\n",
    "        time.sleep(0.5)  # cortesÃ­a de cuota\n",
    "\n",
    "    if buffer:\n",
    "        append_csv(F_COMMENTS, buffer)\n",
    "\n",
    "    print(f\"âœ… Listo. Comentarios totales nuevos guardados: {total_new}\")\n",
    "    print(f\"Archivo: {F_COMMENTS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4167075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- RUN HELPERS ----------------\n",
    "def fase_1_descubrimiento(max_per_channel: Optional[int] = None):\n",
    "    df = discover_videos_for_channels(canales_interes, max_per_channel=max_per_channel)\n",
    "    print(f\"Descubiertos {len(df)} videos. CSV: {F_VIDEOS}\")\n",
    "\n",
    "def fase_2_comentarios(max_comments_per_video: int = 800, save_every: int = 10000):\n",
    "    collect_comments_incremental(\n",
    "        max_comments_per_video=max_comments_per_video,\n",
    "        save_every=save_every\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43ffd660",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Ejemplos:\n",
    "    # 1) Solo descubrimiento (sin tocar comments). Limita por canal si quieres pruebas rÃ¡pidas.\n",
    "    # fase_1_descubrimiento(max_per_channel=100)\n",
    "\n",
    "    # 2) ExtracciÃ³n de comentarios (requiere haber corrido fase 1)\n",
    "    # fase_2_comentarios(max_comments_per_video=800, save_every=10000)\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
